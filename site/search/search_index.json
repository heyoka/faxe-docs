{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"General","text":"<p>Flow based data-collector and time-centric data-processor.</p> <p>Faxe's inner core is based on a dataflow computing engine and it's components also called <code>nodes</code> can be freely combined into an acyclic graph.</p> <p>Unlike other flowbased frameworks (node_red, ...) in Faxe computing graphs are built with a DSL called <code>dfs</code>. </p>"},{"location":"index.html#rest-api","title":"Rest Api","text":"<p>FAXE can be managed via its rest api.</p>"},{"location":"index.html#general_1","title":"General","text":""},{"location":"index.html#data-in-faxe","title":"Data in faxe","text":"<p>As you can read here, in faxe we deal with <code>data_points</code> and <code>data_batches</code> as basic data types.  These <code>data-items</code> as we call them, are emitted by nodes that are freely combined into a computing graph to achieve the desired processing.</p> <p>Every <code>data_point</code> consists of a <code>ts</code> field, <code>fields</code> and <code>tags</code>. The value of the ts field is always: unix-timestamp in millisecond precision without a timezone.</p> <p><code>fields</code> and <code>tags</code> are essentially <code>key-value maps</code>.</p> <p>Valid data-types for field and tag values are: <code>string, integer, float, key-value map (also deeply nested) and lists</code>.  The only valid data-type for field and tag keys is <code>string</code>.</p> <p>A data_batch consists of a list of data_points ordered by timestamp.</p> <p>Most faxe nodes can deal with both points and batches. </p>"},{"location":"index.html#value-referencing","title":"Value referencing","text":"<p>As field and tag values can be deeply nested maps and lists, it is possible to use a <code>JSON-path</code> like syntax to declare and reference these values:</p> <p>Valid examples:</p> <pre><code>averages\naxis.z.cur\nvalue.sub[2].data\naverages.emitted[5]\n</code></pre> <p>For more information, you can read the introduction.</p>"},{"location":"configuration.html","title":"Configuration","text":"<p>FAXE supports a sysctl like configuration syntax. Here are the simple rules of the syntax:</p> <ul> <li>Everything you need to know about a single setting is on one line</li> <li>Lines are structured Key = Value</li> <li>Any line starting with # is a comment, and will be ignored.</li> </ul> <p>Every config item can be overwritten with OS Environment variables (see 'ENV-Key'). </p> <pre><code>## Name of the Erlang node\n## \n## Default: faxe@127.0.0.1\n## \n## ENV-Key: FAXE_NODENAME\n## \n## Acceptable values:\n##   - text\nnodename = faxe@127.0.0.1\n\n## Cookie for distributed node communication.  All nodes in the\n## same cluster should use the same cookie or they will not be able to\n## communicate.\n## \n## Default: distrepl_proc_cookie\n## \n## ENV-Key: FAXE_DISTRIBUTED_COOKIE\n## \n## Acceptable values:\n##   - text\ndistributed_cookie = distrepl_proc_cookie\n\n## Base directory for mnesia files\n## \n## Default: ./mnesia_data\n## \n## ENV-Key: FAXE_MNESIA_DIR\n## \n## Acceptable values:\n##   - the path to a directory\nmnesia_dir = ./mnesia_data\n\n## Sets the number of threads in async thread pool, valid range\n## is 0-1024. If thread support is available, the default is 64.\n## More information at: http://erlang.org/doc/man/erl.html\n## \n## Default: 64\n## \n## ENV-Key: FAXE_ERLANG_ASYNC_THREADS\n## \n## Acceptable values:\n##   - an integer\nerlang.async_threads = 64\n\n## The number of concurrent ports/sockets\n## Valid range is 1024-134217727\n## \n## Default: 262144\n## \n## ENV-Key: FAXE_ERLANG_MAX_PORTS\n## \n## Acceptable values:\n##   - an integer\nerlang.max_ports = 262144\n\n## Raise the default erlang process limit\n## \n## Default: 512000\n## \n## ENV-Key: FAXE_ERLANG_PROCESS_LIMIT\n## \n## Acceptable values:\n##   - an integer\nerlang.process_limit = 512000\n\n## --------------------------------------------------------------------------\n## Erlangs timewarp and time-correction behaviour\n## More info: https://www.erlang.org/doc/apps/erts/time_correction.html#Multi_Time_Warp_Mode\n## ------------------------------------------------------------------------------\n## What time wrap mode to use for the erlang runtime system\n## \n## Default: multi_time_warp\n## \n## ENV-Key: FAXE_ERLANG_TIME_WARP_MODE\n## \n## Acceptable values:\n##   - one of: no_time_warp, single_time_warp, multi_time_warp\n## erlang.time.warp_mode = multi_time_warp\n\n## Whether to use time correction\n## \n## Default: false\n## \n## ENV-Key: FAXE_ERLANG_TIME_CORRECTION\n## \n## Acceptable values:\n##   - one of: true, false\n## erlang.time.correction = false\n\n## --------------------------------------------------------------------------\n## Erlangs scheduler busy wait threshold\n## More info: https://www.erlang.org/doc/man/erl.html\n## ------------------------------------------------------------------------------\n## scheduler busy wait threshold\n## \n## Default: medium\n## \n## ENV-Key: FAXE_ERLANG_BUSY_WAIT_CPU_SCHEDULER\n## \n## Acceptable values:\n##   - one of: none, very_short, short, medium, long, very_long\n## erlang.busy_wait.cpu_scheduler = medium\n\n## \n## Default: short\n## \n## ENV-Key: FAXE_ERLANG_BUSY_WAIT_DIRTY_CPU_SCHEDULER\n## \n## Acceptable values:\n##   - one of: none, very_short, short, medium, long, very_long\n## erlang.busy_wait.dirty_cpu_scheduler = short\n\n## \n## Default: short\n## \n## ENV-Key: FAXE_ERLANG_BUSY_WAIT_DIRTY_IO_SCHEDULER\n## \n## Acceptable values:\n##   - one of: none, very_short, short, medium, long, very_long\n## erlang.busy_wait.dirty_io_scheduler = short\n\n## --------------------------------------------------------------\n## LOGGING\n## --------------------------------------------------------------\n## set the logging level for console\n## \n## Default: info\n## \n## ENV-Key: FAXE_LOG_CONSOLE_LEVEL\n## \n## Acceptable values:\n##   - one of: debug, info, notice, warning, error, alert\n## log.console_level = info\n\n## set the log level for the emit backend\n## \n## Default: warning\n## \n## ENV-Key: FAXE_LOG_EMIT_LEVEL\n## \n## Acceptable values:\n##   - one of: debug, info, notice, warning, error, alert\n## log.emit_level = warning\n\n## whether to send logs to logstash\n## logs will be sent via a udp or tcp socket to the configured logstash host\n## \n## Default: off\n## \n## ENV-Key: FAXE_LOG_LOGSTASH_BACKEND_ENABLE\n## \n## Acceptable values:\n##   - on or off\nlog.logstash_backend_enable = off\n\n## whether to send logs to logstash using the udp or tcp protocol\n## \n## Default: udp\n## \n## ENV-Key: FAXE_LOG_LOGSTASH_BACKEND_PROTOCOL\n## \n## Acceptable values:\n##   - one of: udp, tcp\n## log.logstash_backend_protocol = udp\n\n## enable/disable tls\n## enable the use of tls for the logstash handler, if tcp is used\n## \n## Default: off\n## \n## ENV-Key: FAXE_LOG_LOGSTASH_BACKEND_SSL_ENABLE\n## \n## Acceptable values:\n##   - on or off\n## log.logstash_backend.ssl_enable = off\n\n## logstash host name or address\n## \n## Default: 127.0.0.1\n## \n## ENV-Key: FAXE_LOG_LOGSTASH_HOST\n## \n## Acceptable values:\n##   - text\n## log.logstash_host = 127.0.0.1\n\n## logstash port\n## \n## Default: 9125\n## \n## ENV-Key: FAXE_LOG_LOGSTASH_PORT\n## \n## Acceptable values:\n##   - an integer\n## log.logstash_port = 9125\n\n## set the log level for the logstash backend\n## \n## Default: info\n## \n## ENV-Key: FAXE_LOG_LOGSTASH_LEVEL\n## \n## Acceptable values:\n##   - one of: debug, info, notice, warning, error, alert\n## log.logstash_level = info\n\n## --------------------------------------------------------------\n## AUTO START faxe flows (tasks)\n## --------------------------------------------------------------\n## whether to start tasks marked \"permanent\" automatically on node startup\n## \n## Default: off\n## \n## ENV-Key: FAXE_FLOW_AUTO_START\n## \n## Acceptable values:\n##   - on or off\n## flow_auto_start = off\n\n## --------------------------------------------------------------\n## AUTO RELOAD faxe flows (tasks)\n## --------------------------------------------------------------\n## whether to reload all tasks automatically on node startup\n## \n## Default: off\n## \n## ENV-Key: FAXE_FLOW_AUTO_RELOAD\n## \n## Acceptable values:\n##   - on or off\n## flow_auto_reload = off\n\n## --------------------------------------------------------------\n## DFS\n## --------------------------------------------------------------\n## path to folder where dfs scripts live\n## \n## Default: /home/heyoka/workspace/faxe/dfs/\n## \n## ENV-Key: FAXE_DFS_SCRIPT_PATH\n## \n## Acceptable values:\n##   - the path to a directory\n## dfs.script_path = /home/heyoka/workspace/faxe/dfs/\n\n## \n## Default: off\n## \n## ENV-Key: FAXE_DFS_DEBUG\n## \n## Acceptable values:\n##   - on or off\ndfs.debug = off\n\n## ----------------------------------------------------------------\n## API USER - default user, that will be created on first startup\n## ----------------------------------------------------------------\n## anonymous access to the api endpoint\n## set to false for production use\n## \n## Default: true\n## \n## ENV-Key: FAXE_ALLOW_ANONYMOUS\n## \n## Acceptable values:\n##   - true or false\nallow_anonymous = true\n\n## \n## Default: user\n## \n## ENV-Key: FAXE_DEFAULT_USERNAME\n## \n## Acceptable values:\n##   - text\n## default_username = user\n\n## \n## Default: pass\n## \n## ENV-Key: FAXE_DEFAULT_PASSWORD\n## \n## Acceptable values:\n##   - text\n## default_password = pass\n\n## \n## Default: false\n## \n## ENV-Key: FAXE_RESET_USER_ON_STARTUP\n## \n## Acceptable values:\n##   - true or false\n## reset_user_on_startup = false\n\n## ----------------------------------------------------------------\n## API AUTH with JWT\n## ----------------------------------------------------------------\n## \n## Default: /path/to/cacertfile.pem\n## \n## ENV-Key: FAXE_HTTP_API_JWT_PUBLIC_KEY_FILE\n## \n## Acceptable values:\n##   - the path to a file\n## http_api.jwt.public_key_file = /path/to/cacertfile.pem\n\n## ----------------------------------------------------------------\n## REST API\n## ----------------------------------------------------------------\n## http port for rest api endpoint\n## \n## Default: 8081\n## \n## ENV-Key: FAXE_HTTP_API_PORT\n## \n## Acceptable values:\n##   - an integer\nhttp_api_port = 8081\n\n## \n## Default: 3000000\n## \n## ENV-Key: FAXE_HTTP_API_MAX_UPLOAD_SIZE\n## \n## Acceptable values:\n##   - an integer\nhttp_api.max_upload_size = 3000000\n\n## http-api tls\n## enable the use of tls for the http-api\n## \n## Default: off\n## \n## ENV-Key: FAXE_HTTP_API_TLS_ENABLE\n## \n## Acceptable values:\n##   - on or off\n## http_api.tls.enable = on\n\n## http-api ssl certificate\n## \n## Default: /path/to/certfile.pem\n## \n## ENV-Key: FAXE_HTTP_API_SSL_CERTFILE\n## \n## Acceptable values:\n##   - the path to a file\n## http_api.ssl.certfile = /path/to/certfile.pem\n\n## http-api ssl ca certificate\n## \n## Default: /path/to/cacertfile.pem\n## \n## ENV-Key: FAXE_HTTP_API_SSL_CACERTFILE\n## \n## Acceptable values:\n##   - the path to a file\n## http_api.ssl.cacertfile = /path/to/cacertfile.pem\n\n## http-api ssl key file\n## \n## Default: /path/to/keyfile.key\n## \n## ENV-Key: FAXE_HTTP_API_SSL_KEYFILE\n## \n## Acceptable values:\n##   - the path to a file\n## http_api.ssl.keyfile = /path/to/cert.key\n\n## a list of ciphers to use for the http listener\n## \n## Default: ECDHE-RSA-AES256-GCM-SHA384, ECDHE-RSA-AES128-GCM-SHA256, DHE-RSA-AES256-GCM-SHA384, DHE-RSA-AES128-GCM-SHA256, TLS_AES_256_GCM_SHA384,ECDHE-RSA-AES256-GCM-SHA384\n## \n## ENV-Key: FAXE_HTTP_API_CIPHERS\n## \n## Acceptable values:\n##   - text\n## http_api.ciphers = ECDHE-RSA-AES256-GCM-SHA384, ECDHE-RSA-AES128-GCM-SHA256, DHE-RSA-AES256-GCM-SHA384, DHE-RSA-AES128-GCM-SHA256, TLS_AES_256_GCM_SHA384,ECDHE-RSA-AES256-GCM-SHA384\n\n## -----------------------------------------------------------\n## PYTHON\n## -----------------------------------------------------------------\n## python version\n## \n## Default: 3\n## \n## ENV-Key: FAXE_PYTHON_VERSION\n## \n## Acceptable values:\n##   - text\n## python.version = 3\n\n## path to custom python files\n## \n## Default: /home/heyoka/workspace/faxe/python/\n## \n## ENV-Key: FAXE_PYTHON_SCRIPT_PATH\n## \n## Acceptable values:\n##   - the path to a directory\npython.script_path = /home/heyoka/workspace/faxe/python/\n\n## -------------------------------------------------------------------\n## ESQ\n## -------------------------------------------------------------------\n## several faxe nodes use persistent queues for safe data-delivery and buffering in\n## case any upstream services are disconnected temporarily\n## These queues can be configured with the following settings.\n## base directory for persistent queues\n## \n## Default: /tmp\n## \n## ENV-Key: FAXE_QUEUE_BASE_DIR\n## \n## Acceptable values:\n##   - the path to a directory\nqueue_base_dir = /tmp\n\n## queue message time to live\n## expired messages are evicted from queue\n## \n## Default: 4h\n## \n## ENV-Key: FAXE_QUEUE_TTL\n## \n## Acceptable values:\n##   - a time duration with units, e.g. '10s' for 10 seconds\nqueue_ttl = 4h\n\n## queue sync to disc interval\n## queue time-to-sync (rotate) file segments.\n## Any enqueued message might remain invisible until sync is performed.\n## \n## Default: 300ms\n## \n## ENV-Key: FAXE_QUEUE_TTS\n## \n## Acceptable values:\n##   - a time duration with units, e.g. '10s' for 10 seconds\nqueue_tts = 300ms\n\n## queue time to flight\n## ttf message time-to-flight in milliseconds,\n## the time required to deliver message acknowledgment before it reappears to client(s) again.\n## \n## Default: 20000ms\n## \n## ENV-Key: FAXE_QUEUE_TTF\n## \n## Acceptable values:\n##   - a time duration with units, e.g. '10s' for 10 seconds\nqueue_ttf = 20000ms\n\n## capacity defines the size of in-memory queue.\n## The queue would not fetch anything from disk into memory buffer if capacity is 0.\n## \n## Default: 30\n## \n## ENV-Key: FAXE_QUEUE_CAPACITY\n## \n## Acceptable values:\n##   - an integer\nqueue_capacity = 30\n\n## dequeue interval.\n## Start interval at which the queue is asked for an element.\n## \n## Default: 15ms\n## \n## ENV-Key: FAXE_DEQUEUE_INTERVAL\n## \n## Acceptable values:\n##   - a time duration with units, e.g. '10s' for 10 seconds\ndequeue_interval = 15ms\n\n## dequeue min interval.\n## Min interval at which the queue is asked for an element.\n## \n## Default: 3ms\n## \n## ENV-Key: FAXE_DEQUEUE_MIN_INTERVAL\n## \n## Acceptable values:\n##   - a time duration with units, e.g. '10s' for 10 seconds\ndequeue.min_interval = 3ms\n\n## dequeue max interval.\n## Max interval at which the queue is asked for an element.\n## \n## Default: 200ms\n## \n## ENV-Key: FAXE_DEQUEUE_MAX_INTERVAL\n## \n## Acceptable values:\n##   - a time duration with units, e.g. '10s' for 10 seconds\ndequeue.max_interval = 200ms\n\n## interval change step size.\n## Step size for dequeue interval changes\n## \n## Default: 3\n## \n## ENV-Key: FAXE_DEQUEUE_STEP_SIZE\n## \n## Acceptable values:\n##   - an integer\ndequeue.step_size = 3\n\n## -------------------------------------------------------------------------\n## NODE SPECIFIC settings\n## -------------------------------------------------------------------------\n## \n## Default: off\n## \n## ENV-Key: FAXE_NODE_CRATE_QUERY_CONT_EXTENDED_LOG\n## \n## Acceptable values:\n##   - on or off\nnode.crate_query_cont.extended_log = off\n\n## -------------------------------------------------------------------------\n## S7 DEFAULTS\n## -------------------------------------------------------------------------\n## for every unique ip address used by s7_read nodes,\n## faxe will maintain a separate connection pool,\n## each pool will have at least 's7pool.min_size' connections\n## and a maximum of 's7pool.max_size' connections\n## s7 connection pool min size\n## \n## Default: 2\n## \n## ENV-Key: FAXE_S7POOL_MIN_SIZE\n## \n## Acceptable values:\n##   - an integer\ns7pool.min_size = 2\n\n## s7 connection pool max size\n## \n## Default: 16\n## \n## ENV-Key: FAXE_S7POOL_MAX_SIZE\n## \n## Acceptable values:\n##   - an integer\ns7pool.max_size = 16\n\n## whether to use the s7 pool (default for all s7read nodes)\n## \n## Default: off\n## \n## ENV-Key: FAXE_S7POOL_ENABLE\n## \n## Acceptable values:\n##   - on or off\ns7pool.enable = off\n\n## whether to use the optimized s7 reader\n## \n## Default: on\n## \n## ENV-Key: FAXE_S7READER_OPTIMIZED\n## \n## Acceptable values:\n##   - on or off\ns7reader.optimized = on\n\n## -------------------------------------------------------------------------------\n## MQTT defaults\n## -------------------------------------------------------------------------------\n## \n## Default: on\n## \n## ENV-Key: FAXE_MQTT_POOL_ENABLE\n## \n## Acceptable values:\n##   - on or off\nmqtt_pool.enable = on\n\n## max size (maximum number of connections) for the mqtt connection pool\n## {mapping, \"mqtt_pool.max_size\", \"faxe.mqtt_pool.max_size\",\n## [{default, 30}, {datatype, integer}]\n## }.\n## mqtt host\n## \n## Default: 10.14.204.20\n## \n## ENV-Key: FAXE_MQTT_HOST\n## \n## Acceptable values:\n##   - text\nmqtt.host = 10.14.204.20\n\n## mqtt port\n## \n## Default: 1883\n## \n## ENV-Key: FAXE_MQTT_PORT\n## \n## Acceptable values:\n##   - an integer\nmqtt.port = 1883\n\n## mqtt user\n## \n## Default: username\n## \n## ENV-Key: FAXE_MQTT_USER\n## \n## Acceptable values:\n##   - text\n## mqtt.user = username\n\n## mqtt pass\n## \n## Default: password\n## \n## ENV-Key: FAXE_MQTT_PASS\n## \n## Acceptable values:\n##   - text\n## mqtt.pass = password\n\n## mqtt ssl\n## enable the use of ssl for mqtt connections\n## \n## Default: off\n## \n## ENV-Key: FAXE_MQTT_SSL_ENABLE\n## \n## Acceptable values:\n##   - on or off\n## mqtt.ssl.enable = off\n\n## mqtt ssl peer verification\n## \n## Default: verify_none\n## \n## ENV-Key: FAXE_MQTT_SSL_VERIFY\n## \n## Acceptable values:\n##   - one of: verify_none, verify_peer\n## mqtt.ssl.verify = verify_none\n\n## mqtt ssl certificate\n## \n## Default: \n## \n## ENV-Key: FAXE_MQTT_SSL_CERTFILE\n## \n## Acceptable values:\n##   - the path to a file\n## mqtt.ssl.certfile = /path/to/certfile.pem\n\n## mqtt ssl ca certificate\n## \n## Default: \n## \n## ENV-Key: FAXE_MQTT_SSL_CACERTFILE\n## \n## Acceptable values:\n##   - the path to a file\n## mqtt.ssl.cacertfile = /path/to/cacertfile.pem\n\n## mqtt ssl key file\n## \n## Default: \n## \n## ENV-Key: FAXE_MQTT_SSL_KEYFILE\n## \n## Acceptable values:\n##   - the path to a file\n## mqtt.ssl.keyfile = /path/to/cert.key\n\n## \n## Default: 2\n## \n## ENV-Key: FAXE_MQTT_PUB_POOL_MIN_SIZE\n## \n## Acceptable values:\n##   - an integer\nmqtt_pub_pool.min_size = 2\n\n## mqtt publisher connection pool max size\n## \n## Default: 30\n## \n## ENV-Key: FAXE_MQTT_PUB_POOL_MAX_SIZE\n## \n## Acceptable values:\n##   - an integer\nmqtt_pub_pool.max_size = 30\n\n## mqtt publisher connection pool max worker rate (message throughput per second)\n## this is used for the elastic pool grow/shrink mechanism\n## \n## Default: 70\n## \n## ENV-Key: FAXE_MQTT_PUB_POOL_WORKER_MAX_RATE\n## \n## Acceptable values:\n##   - an integer\nmqtt_pub_pool.worker_max_rate = 70\n\n## whether to use the s7 pool (default for all s7read nodes)\n## \n## Default: on\n## \n## ENV-Key: FAXE_MQTT_PUB_POOL_ENABLE\n## \n## Acceptable values:\n##   - on or off\nmqtt_pub_pool.enable = on\n\n## -------------------------------------------------------------------------------\n## AMQP defaults\n## -------------------------------------------------------------------------------\n## amqp host\n## \n## Default: 10.14.204.28\n## \n## ENV-Key: FAXE_AMQP_HOST\n## \n## Acceptable values:\n##   - text\namqp.host = 10.14.204.28\n\n## amqp port\n## \n## Default: 5672\n## \n## ENV-Key: FAXE_AMQP_PORT\n## \n## Acceptable values:\n##   - an integer\namqp.port = 5672\n\n## amqp user\n## \n## Default: guest\n## \n## ENV-Key: FAXE_AMQP_USER\n## \n## Acceptable values:\n##   - text\n## amqp.user = username\n\n## amqp pass\n## \n## Default: guest\n## \n## ENV-Key: FAXE_AMQP_PASS\n## \n## Acceptable values:\n##   - text\n## amqp.pass = password\n\n## amqp heartbeat interval\n## \n## Default: 60s\n## \n## ENV-Key: FAXE_AMQP_HEARTBEAT\n## \n## Acceptable values:\n##   - a time duration with units, e.g. '10s' for 10 seconds\n## amqp.heartbeat = 60s\n\n## amqp ssl\n## enable the use of ssl for amqp connections\n## \n## Default: off\n## \n## ENV-Key: FAXE_AMQP_SSL_ENABLE\n## \n## Acceptable values:\n##   - on or off\n## amqp.ssl.enable = off\n\n## amqp ssl certificate\n## \n## Default: \n## \n## ENV-Key: FAXE_AMQP_SSL_CERTFILE\n## \n## Acceptable values:\n##   - the path to a file\n## amqp.ssl.certfile = /path/to/certfile.pem\n\n## amqp ssl ca certificate\n## \n## Default: \n## \n## ENV-Key: FAXE_AMQP_SSL_CACERTFILE\n## \n## Acceptable values:\n##   - the path to a file\n## amqp.ssl.cacertfile = /path/to/cacertfile.pem\n\n## amqp ssl key file\n## \n## Default: \n## \n## ENV-Key: FAXE_AMQP_SSL_KEYFILE\n## \n## Acceptable values:\n##   - the path to a file\n## amqp.ssl.keyfile = /path/to/cert.key\n\n## amqp ssl peer verification\n## \n## Default: verify_none\n## \n## ENV-Key: FAXE_AMQP_SSL_VERIFY\n## \n## Acceptable values:\n##   - one of: verify_none, verify_peer\n## amqp.ssl.verify = verify_none\n\n## special amqp-to-DB message ack mode\n## enable the use of flow-acknowledgement for amqp consume nodes\n## \n## Default: off\n## \n## ENV-Key: FAXE_AMQP_FLOW_ACK_ENABLE\n## \n## Acceptable values:\n##   - on or off\n## amqp.flow_ack.enable = off\n\n## -------------------------------------------------------------------------------\n## RabbitMQ defaults\n## -------------------------------------------------------------------------------\n## rabbitmq default exchange\n## the amqp_publish node will use this exchange as default\n## \n## Default: x_lm_fanout\n## \n## ENV-Key: FAXE_RABBITMQ_ROOT_EXCHANGE\n## \n## Acceptable values:\n##   - text\nrabbitmq.root_exchange = x_lm_fanout\n\n## \n## Default: q_\n## \n## ENV-Key: FAXE_RABBITMQ_QUEUE_PREFIX\n## \n## Acceptable values:\n##   - text\nrabbitmq.queue_prefix = q_\n\n## \n## Default: x_\n## \n## ENV-Key: FAXE_RABBITMQ_EXCHANGE_PREFIX\n## \n## Acceptable values:\n##   - text\nrabbitmq.exchange_prefix = x_\n\n## -------------------------------------------------------------------------------\n## CrateDB defaults (postgreSQL connect)\n## -------------------------------------------------------------------------------\n## CrateDB host\n## \n## Default: crate.example.com\n## \n## ENV-Key: FAXE_CRATE_HOST\n## \n## Acceptable values:\n##   - text\ncrate.host = crate.example.com\n\n## CrateDB port\n## \n## Default: 5432\n## \n## ENV-Key: FAXE_CRATE_PORT\n## \n## Acceptable values:\n##   - an integer\ncrate.port = 5432\n\n## CrateDB user\n## \n## Default: crate\n## \n## ENV-Key: FAXE_CRATE_USER\n## \n## Acceptable values:\n##   - text\ncrate.user = crate\n\n## CrateDB password\n## \n## Default: \n## \n## ENV-Key: FAXE_CRATE_PASS\n## \n## Acceptable values:\n##   - text\n## crate.pass = pass\n\n## CrateDB database\n## \n## Default: doc\n## \n## ENV-Key: FAXE_CRATE_DATABASE\n## \n## Acceptable values:\n##   - text\ncrate.database = doc\n\n## crate tls\n## enable the use of tls for crate postgre connections\n## \n## Default: off\n## \n## ENV-Key: FAXE_CRATE_TLS_ENABLE\n## \n## Acceptable values:\n##   - on or off\n## crate.tls.enable = off\n\n## CrateDB ignore rules\n## \n## Default: code=5000,message=example\n## \n## ENV-Key: FAXE_CRATE_IGNORE_RULES\n## \n## Acceptable values:\n##   - text\ncrate.ignore_rules = code=5000,message=example\n\n## -------------------------------------------------------------------------------\n## CrateDB defaults (http api)\n## -------------------------------------------------------------------------------\n## CrateDB host\n## \n## Default: 10.14.204.10\n## \n## ENV-Key: FAXE_CRATE_HTTP_HOST\n## \n## Acceptable values:\n##   - text\ncrate_http.host = 10.14.204.10\n\n## CrateDB port\n## \n## Default: 4200\n## \n## ENV-Key: FAXE_CRATE_HTTP_PORT\n## \n## Acceptable values:\n##   - an integer\ncrate_http.port = 4200\n\n## CrateDB user\n## \n## Default: crate\n## \n## ENV-Key: FAXE_CRATE_HTTP_USER\n## \n## Acceptable values:\n##   - text\ncrate_http.user = crate\n\n## CrateDB password\n## \n## Default: \n## \n## ENV-Key: FAXE_CRATE_HTTP_PASS\n## \n## Acceptable values:\n##   - text\n## crate_http.pass = pass\n\n## CrateDB database\n## \n## Default: doc\n## \n## ENV-Key: FAXE_CRATE_HTTP_DATABASE\n## \n## Acceptable values:\n##   - text\ncrate_http.database = doc\n\n## cratedb http connection timeout\n## \n## Default: 30s\n## \n## ENV-Key: FAXE_CRATE_HTTP_CONNECTION_TIMEOUT\n## \n## Acceptable values:\n##   - a time duration with units, e.g. '10s' for 10 seconds\ncrate_http.connection_timeout = 30s\n\n## cratedb http query timeout\n## \n## Default: 15s\n## \n## ENV-Key: FAXE_CRATE_HTTP_QUERY_TIMEOUT\n## \n## Acceptable values:\n##   - a time duration with units, e.g. '10s' for 10 seconds\ncrate_http.query_timeout = 15s\n\n## crate tls - enable the use of tls for crate http connections\n## \n## Default: off\n## \n## ENV-Key: FAXE_CRATE_HTTP_TLS_ENABLE\n## \n## Acceptable values:\n##   - on or off\n## crate_http.tls.enable = off\n\n## -------------------------------------------------------------------------------\n## InfluxDB defaults (http api)\n## -------------------------------------------------------------------------------\n## InfluxDB host\n## \n## Default: influx.example.com\n## \n## ENV-Key: FAXE_INFLUX_HTTP_HOST\n## \n## Acceptable values:\n##   - text\ninflux_http.host = influx.example.com\n\n## InfluxDB port\n## \n## Default: 8086\n## \n## ENV-Key: FAXE_INFLUX_HTTP_PORT\n## \n## Acceptable values:\n##   - an integer\ninflux_http.port = 8086\n\n## InfluxDB user\n## \n## Default: influx\n## \n## ENV-Key: FAXE_INFLUX_HTTP_USER\n## \n## Acceptable values:\n##   - text\ninflux_http.user = influx\n\n## InfluxDB pass\n## \n## Default: \n## \n## ENV-Key: FAXE_INFLUX_HTTP_PASS\n## \n## Acceptable values:\n##   - text\n## influx_http.pass = password\n\n## ----------------------------------------------------------------------------\n## EMAIL defaults\n## ----------------------------------------------------------------------------\n## email from address\n## \n## Default: noreply@example.com\n## \n## ENV-Key: FAXE_EMAIL_FROM\n## \n## Acceptable values:\n##   - text\nemail.from = noreply@example.com\n\n## email smtp relay\n## \n## Default: smtp.example.com\n## \n## ENV-Key: FAXE_EMAIL_SMTP\n## \n## Acceptable values:\n##   - text\nemail.smtp = smtp.example.com\n\n## email smtp port\n## \n## Default: 25\n## \n## ENV-Key: FAXE_EMAIL_PORT\n## \n## Acceptable values:\n##   - an integer\nemail.port = 25\n\n## email smtp tls, whether to use tls\n## \n## Default: off\n## \n## ENV-Key: FAXE_EMAIL_TLS\n## \n## Acceptable values:\n##   - on or off\nemail.tls = off\n\n## email smtp user\n## \n## Default: username\n## \n## ENV-Key: FAXE_EMAIL_USER\n## \n## Acceptable values:\n##   - text\n## email.user = username\n\n## email smtp pass\n## \n## Default: password\n## \n## ENV-Key: FAXE_EMAIL_PASS\n## \n## Acceptable values:\n##   - text\n## email.pass = password\n\n## email html template\n## \n## Default: /home/user/template.html\n## \n## ENV-Key: FAXE_EMAIL_TEMPLATE\n## \n## Acceptable values:\n##   - text\nemail.template = /home/user/template.html\n\n## --------------------------------------------------------------------------\n## AZURE BLOB\n## -------------------------------------------------------------------------------\n## account-url\n## \n## Default: https://someblob.blob.core.windows.net\n## \n## ENV-Key: FAXE_AZURE_BLOB_ACCOUNT_URL\n## \n## Acceptable values:\n##   - text\nazure_blob.account_url = https://someblob.blob.core.windows.net\n\n## \n## Default: azblob-secret\n## \n## ENV-Key: FAXE_AZURE_BLOB_ACCOUNT_SECRET\n## \n## Acceptable values:\n##   - text\nazure_blob.account_secret = azblob-secret\n\n## \n## --------------------------------------------------------------------\n## DEBUG, LOGS, METRICS, CONNECTION STATUS, FLOW_CHANGED\n## --------------------------------------------------------------------\n## There are mqtt handlers for debug, logs, metrics, connection-status and flow_changed events.\n## Note that the base options for these mqtt connections come from the 'mqtt' options above.\n## If needed you can override these default mqtt-options for every handler type.\n## \n## Default: \n## \n## ENV-Key: FAXE_REPORT_DEBUG_MQTT_HOST\n## \n## Acceptable values:\n##   - text\n## report_debug.mqtt_host = example.com\n\n## \n## ----------------------- METRICS ------------------------------\n## Metrics handler MQTT sends metric events to an mqtt broker\n## \n## Default: off\n## \n## ENV-Key: FAXE_METRICS_HANDLER_MQTT_ENABLE\n## \n## Acceptable values:\n##   - on or off\nmetrics.handler.mqtt.enable = off\n\n## metrics handler mqtt host\n## \n## Default: \n## \n## ENV-Key: FAXE_METRICS_HANDLER_MQTT_HOST\n## \n## Acceptable values:\n##   - text\n## metrics.handler.mqtt.host = example.com\n\n## metrics handler mqtt port\n## \n## ENV-Key: FAXE_METRICS_HANDLER_MQTT_PORT\n## \n## Acceptable values:\n##   - an integer\n## metrics.handler.mqtt.port = 1883\n\n## metrics handler mqtt base topic\n## The mqtt handler will prefix its topic with this value,\n## note that it must be a valid mqtt topic string.\n## \n## Default: sys/faxe\n## \n## ENV-Key: FAXE_METRICS_HANDLER_MQTT_BASE_TOPIC\n## \n## Acceptable values:\n##   - text\n## metrics.handler.mqtt.base_topic = sys/faxe\n\n## flow-metrics publish interval\n## Interval at which flow-metrics get publish to the handler\n## \n## Default: 30s\n## \n## ENV-Key: FAXE_METRICS_PUBLISH_INTERVAL\n## \n## Acceptable values:\n##   - text\nmetrics.publish_interval = 30s\n\n## ----------------------- CONNECTION STATUS ------------------------\n## Conn_status handler MQTT sends connection status events to an mqtt broker.\n## \n## Default: on\n## \n## ENV-Key: FAXE_CONN_STATUS_HANDLER_MQTT_ENABLE\n## \n## Acceptable values:\n##   - on or off\nconn_status.handler.mqtt.enable = on\n\n## \n## Default: \n## \n## ENV-Key: FAXE_CONN_STATUS_HANDLER_MQTT_HOST\n## \n## Acceptable values:\n##   - text\n## conn_status.handler.mqtt.host = example.com\n\n## connection status handler mqtt port\n## \n## ENV-Key: FAXE_CONN_STATUS_HANDLER_MQTT_PORT\n## \n## Acceptable values:\n##   - an integer\n## conn_status.handler.mqtt.port = 1883\n\n## connection status handler mqtt base topic\n## \n## Default: sys/faxe\n## \n## ENV-Key: FAXE_CONN_STATUS_HANDLER_MQTT_BASE_TOPIC\n## \n## Acceptable values:\n##   - text\n## conn_status.handler.mqtt.base_topic = sys/faxe\n\n## ----------------------- DEBUG AND TRACE --------------------------\n## Debug trace handler MQTT\n## enable/disable debug_trace handler mqtt\n## \n## Default: off\n## \n## ENV-Key: FAXE_DEBUG_HANDLER_MQTT_ENABLE\n## \n## Acceptable values:\n##   - on or off\ndebug.handler.mqtt.enable = off\n\n## debug_trace handler mqtt host\n## \n## Default: \n## \n## ENV-Key: FAXE_DEBUG_HANDLER_MQTT_HOST\n## \n## Acceptable values:\n##   - text\n## debug.handler.mqtt.host = example.com\n\n## debug_trace handler mqtt port\n## \n## ENV-Key: FAXE_DEBUG_HANDLER_MQTT_PORT\n## \n## Acceptable values:\n##   - an integer\n## debug.handler.mqtt.port = 1883\n\n## debug_trace handler mqtt base topic\n## \n## Default: sys/faxe\n## \n## ENV-Key: FAXE_DEBUG_HANDLER_MQTT_BASE_TOPIC\n## \n## Acceptable values:\n##   - text\n## debug.handler.mqtt.base_topic = sys/faxe\n\n## time debug and node-metric messages will be published to the configured endpoints\n## \n## Default: 3m\n## \n## ENV-Key: FAXE_DEBUG_TIME\n## \n## Acceptable values:\n##   - a time duration with units, e.g. '10s' for 10 seconds\ndebug.time = 3m\n\n## ----------------------- FLOW_CHANGED --------------------------\n## \n## Default: off\n## \n## ENV-Key: FAXE_FLOW_CHANGED_HANDLER_MQTT_ENABLE\n## \n## Acceptable values:\n##   - on or off\nflow_changed.handler.mqtt.enable = off\n\n## flow_changed handler mqtt host\n## \n## Default: \n## \n## ENV-Key: FAXE_FLOW_CHANGED_HANDLER_MQTT_HOST\n## \n## Acceptable values:\n##   - text\n## flow_changed.handler.mqtt.host = example.com\n\n## flow_changed handler mqtt port\n## \n## ENV-Key: FAXE_FLOW_CHANGED_HANDLER_MQTT_PORT\n## \n## Acceptable values:\n##   - an integer\n## flow_changed.handler.mqtt.port = 1883\n\n## flow_changed handler mqtt base topic\n## \n## Default: sys/faxe\n## \n## ENV-Key: FAXE_FLOW_CHANGED_HANDLER_MQTT_BASE_TOPIC\n## \n## Acceptable values:\n##   - text\n## flow_changed.handler.mqtt.base_topic = sys/faxe\n\n## ----------------------- FLOW HEALTH STATUS (observer) --------------------------\n## enable/disable flow health observer process\n## \n## Default: on\n## \n## ENV-Key: FAXE_FLOW_HEALTH_OBSERVER_ENABLE\n## \n## Acceptable values:\n##   - on or off\nflow_health.observer.enable = on\n\n##\n## Interval at which the observer reports the status of a flow, if there are not any other reports sent.\n## Default: 3m\n## \n## ENV-Key: FAXE_FLOW_HEALTH_OBSERVER_REPORT_INTERVAL\n## \n## Acceptable values:\n##   - a time duration with units, e.g. '10s' for 10 seconds\nflow_health.observer.report_interval = 3m\n\n##\n## Interval at which the stopped status of a flow is sent \n## Default: 60m\n## \n## ENV-Key: FAXE_FLOW_HEALTH_POST_MORTEM_REPORT_INTERVAL\n## \n## Acceptable values:\n##   - a time duration with units, e.g. '10s' for 10 seconds\nflow_health.post_mortem.report_interval = 60m\n\n## \n## Default: on\n## \n## ENV-Key: FAXE_FLOW_HEALTH_HANDLER_MQTT_ENABLE\n## \n## Acceptable values:\n##   - on or off\nflow_health.handler.mqtt.enable = on\n\n## flow_health handler mqtt host\n## \n## Default: \n## \n## ENV-Key: FAXE_FLOW_HEALTH_HANDLER_MQTT_HOST\n## \n## Acceptable values:\n##   - text\n## flow_health.handler.mqtt.host = example.com\n\n## flow_health handler mqtt port\n## \n## ENV-Key: FAXE_FLOW_HEALTH_HANDLER_MQTT_PORT\n## \n## Acceptable values:\n##   - an integer\n## flow_health.handler.mqtt.port = 1883\n\n## flow_health handler mqtt base topic\n## \n## Default: sys/faxe\n## \n## ENV-Key: FAXE_FLOW_HEALTH_HANDLER_MQTT_BASE_TOPIC\n## \n## Acceptable values:\n##   - text\n## flow_health.handler.mqtt.base_topic = sys/faxe\n</code></pre>"},{"location":"custom_nodes.html","title":"Custom nodes written in python","text":"<p>Faxe allows for custom nodes to be used in any flow just like any of the built-in nodes. Therefore, a dedicated interface can be used, which will be described here.</p> <p>Custom nodes are written in python &gt;= 3.6 in FAXE.</p> <p>The syntax for calling a custom node in DFS is exactly the same as for any built-in node, but instead of a  pipe symbol, we use the <code>@</code> symbol.</p> <pre><code>    |built_in_node()\n    .opt1('somestring')\n    .opt2(11)\n\n    @custom_node()\n    .option1('astring')\n    .option2(22)\n</code></pre>"},{"location":"custom_nodes.html#faxe-base-class","title":"Faxe base class","text":"<p>When writing a custom node in python, we have to create a python class, that inherits from the base class <code>Faxe</code>.</p> <pre><code>from faxe import Faxe\n\n\nclass Mynode(Faxe):\n\n    ...\n</code></pre> <p>In our class we can use a bunch of callbacks:</p>"},{"location":"custom_nodes.html#callbacks","title":"Callbacks","text":"<p>All callbacks are optional.</p>"},{"location":"custom_nodes.html#options-static","title":"options (static)","text":"<p>The <code>options</code> callback is used to tell FAXE, what node options you want to use for your python node. The return type for this callback is a <code>list of 2-or-3 tuples</code>.</p> <p><code>options</code> is the only static callback and the only one, that has to return a value. </p> <p>The first two elements of the tuples must be strings, the third, if given, depends on the <code>data_type</code>. Every option, that has no <code>default value</code> (3rd element in the tuple) is mandatory in DFS scripts.</p> <pre><code>    (\"name_of_the_option\", \"data_type\", {optional_default_value})\n</code></pre>"},{"location":"custom_nodes.html#example","title":"Example","text":"<pre><code>from faxe import Faxe\n\n\nclass Mynode(Faxe):\n\n    @staticmethod\n    def options():\n        opts = [\n            # mandatory\n            (\"field\", \"string\"),\n            # optional\n            (\"val\", \"integer\", 33),\n\n        ]\n        return opts\n\n    ...\n</code></pre> <p>The above example node can be used in DFS like so:</p> <pre><code>    @mynode()\n    % mandatory\n    .field('some_field_path')\n    % optional\n    .val(44)\n</code></pre>"},{"location":"custom_nodes.html#data-types-for-options","title":"Data types for options","text":"<p>The second element of an options tuple defines the data type, that must be given in DFS. A subset of the option types used for built-in nodes can be used.</p> type description DFS example <code>string</code> .option('string_value') <code>integer</code> .option(123) <code>float</code> .option(123.1564) <code>number</code> integer or float .option(456.1564) <code>double</code> same as float .option(13.98741755) <code>bool</code> .option(false) <code>string_list</code> 1 or more string values .option('string1', 'string2', 'string3') <code>integer_list</code> 1 or more integer values .option(1, 2, 3, 4445) <code>float_list</code> 1 or more float values .option(1.11, 2.456486, 3.0, 44.45) <code>number_list</code> 1 or more number values .option(1.11, 2, 3.0, 45) <code>list</code> list of possibly mixed data types .option('name', 11, 234.3, 'one', 'two')"},{"location":"custom_nodes.html#common-options-for-every-custom-node","title":"Common options for every custom node","text":"option description default <code>as</code>( string ) field path for the output data, used to give data a new root undefined <code>stop_on_exit</code>(boolean) if set to true, the whole corresponding flow will stop, when the python runtime exits because of an error (exception) true"},{"location":"custom_nodes.html#init","title":"init","text":"<p>The <code>init</code> callback is called on class instatiation, it gets injected a dictionary with the option values given in the DFS script.</p> <p>Do not overwrite the <code>__init__</code> method. The callback will not work in this case.</p> <pre><code>from faxe import Faxe\n\n\nclass Mynode(Faxe):\n\n    ...\n\n    def init(self, args=None):\n        # store the option values for later usage\n        self.fieldname = args[\"field\"]\n        self.value = args[\"val\"] \n\n    ...\n</code></pre>"},{"location":"custom_nodes.html#handle_point","title":"handle_point","text":"<p><code>handle_point</code> is called every time the custom node receives a data-point structure from upstream nodes in a FAXE flow. For details on the point structure see FAXE Data items - data_point below.</p> <pre><code>from faxe import Faxe\n\n\nclass Mynode(Faxe):\n\n    ...\n\n    def handle_point(self, point_data):  \n        # use the inherited emit method to emit data to downstream nodes in the flow\n        self.emit(point_data)\n\n    ...\n</code></pre>"},{"location":"custom_nodes.html#handle_batch","title":"handle_batch","text":"<p><code>handle_batch</code> is called every time the custom node receives a data-batch structure from upstream nodes in a FAXE flow. For details on the batch structure see FAXE Data items - data_batch below.</p> <pre><code>from faxe import Faxe\n\n\nclass Mynode(Faxe):\n\n    ...\n\n    def handle_batch(self, batch_data):  \n        # use the inherited emit method to emit data to downstream nodes in the flow\n        self.emit(batch_data)\n\n    ...\n</code></pre>"},{"location":"custom_nodes.html#inherited-methods-from-the-faxe-class","title":"Inherited methods from the Faxe class","text":""},{"location":"custom_nodes.html#emit","title":"emit","text":"<pre><code>def emit(self, emit_data: dict):\n</code></pre> <p>The <code>emit</code> method inherited from the base class (Faxe), is the only way send data to downstream nodes in a FAXE flow. It can take both point and batch data structures.</p> <pre><code>from faxe import Faxe\n\n\nclass Mynode(Faxe):\n\n    ...\n\n    def my_method(self):\n        batch_data = self.my_batch_data\n        self.emit(batch_data)\n\n    ...\n</code></pre>"},{"location":"custom_nodes.html#ack","title":"ack","text":"<pre><code>def ack(self, ack_data, multi=True):\n</code></pre> <p>Where ack_data is either a <code>dtag</code> value (integer) from a data_item or a data_item itself (point or batch).</p> <p>The <code>ack</code> method inherited from the base class (Faxe) is normally used to acknowledge data back to a message broker. </p> <p><code>multi</code> defines if the acknowledgement is done just for the given dtag (multi=False) or for all dtags up to the given one (multi=True).  If <code>ack_data</code> is a data_batch item, <code>multi</code> is set to True and the highest dtag value, that is found in the list of data_points, will be used.</p> <p>Normally other built-in faxe nodes take care of acknowledging data, when they are consumed from a broker, but under some circumstances it is necessary to do this in a custom node. For example: A flow that consumes from RabbitMQ and does not write to a database, but has a custom python node as a sink node.</p> <pre><code>from faxe import Faxe\n\n\nclass Mynode(Faxe):\n\n    ...\n\n    def handle_point(self, batch_data):\n\"\"\"do something with batch_data\"\"\"\n        self.emit(batch_data)\n        self.ack(batch_data)\n\n    ...\n</code></pre>"},{"location":"custom_nodes.html#log","title":"log","text":"<pre><code>def log(self, msg: str, level='notice': str):\n\"\"\"\n:param level: 'debug' | 'info' | 'notice' | 'warning' | 'error' | 'critical' | 'alert'\n\"\"\"\n</code></pre> <p>The <code>log</code> method inherited from the base class (Faxe), can be used for logging. Using this method makes sure, your log data will be injected into FAXE's logging infrastructure.</p> <pre><code>from faxe import Faxe\n\n\nclass Mynode(Faxe):\n\n    ...\n\n    def my_method(self, param):\n        self.log(f\"my_method is called with {param}\", 'info')\n        ...\n\n    ...\n</code></pre>"},{"location":"custom_nodes.html#now-static","title":"now (static)","text":"<pre><code>@staticmethod\ndef now():\n\"\"\"\n    unix timestamp in milliseconds (utc)\n    :return: int\n    \"\"\"\n</code></pre> <p>Used to retrieve the current timestamp in milliseconds.</p> <pre><code>from faxe import Faxe\n\n\nclass Mynode(Faxe):\n\n    ...\n\n    def my_method(self, _param):\n        now = Faxe.now()\n        ...\n\n    ...\n</code></pre>"},{"location":"custom_nodes.html#data-types","title":"Data types","text":"<p>Comparing data types on each side</p> FAXE python string string binary string integer integer floating point number floating point number map dictionary list list ---------------- ------------------ data-point dictionary, see below data-batch dictionary, see below"},{"location":"custom_nodes.html#faxe-data-items","title":"FAXE Data items","text":"<p>As you remember, in FAXE we know two types of data-items, data-point and data-batch.</p> <p>How do data-items look like in python ?</p>"},{"location":"custom_nodes.html#data-point","title":"data-point","text":"<pre><code>    ## data-point\n    {'fields': {'f1': 1, 'f2': {'f2_1': 'mode_on'}}, 'ts': 1669407781437, 'dtag': None, 'tags': {}}\n</code></pre> field data type meaning <code>ts</code> integer millisecond timestamp <code>fields</code> dictionary a dictionary of fields, a data-point carries, <code>tags</code> dictionary a dictionary of tags, a data-point carries <code>dtag</code> integer or None delivery tag, see description below"},{"location":"custom_nodes.html#fields","title":"fields","text":"<p>A key in the <code>fields</code> dict is called <code>fieldname</code> and is always a string.</p> <p>A dict value can be of any of the above listed data-types, including dictionary and list. </p> <p>Examples</p> <pre><code>    {'field1': 'string'}\n    {'field2': 1235468486}\n    {'field3': 12354.68486}\n    {'field4': {'field4_1': [1,43,4,67.7]}}\n</code></pre>"},{"location":"custom_nodes.html#tags","title":"tags","text":"<p>For <code>tags</code>, keys and values are strings only.</p>"},{"location":"custom_nodes.html#dtag","title":"dtag","text":"<p>A delivery tag is used to acknowledge a data-item to upstream nodes. Can be ignored for python nodes at the moment.</p>"},{"location":"custom_nodes.html#data-batch","title":"data-batch","text":"<pre><code>    ## data-batch\n    {'points': [\n        {'fields': {'f1': 1, 'f2': {'f2_1': 'mode_on'}}, 'ts': 1669407781437, 'dtag': None, 'tags': {}},\n        {'fields': {'f1': 2, 'f2': {'f2_1': 'mode_off'}}, 'ts': 1669407781438, 'dtag': None, 'tags': {}},\n        {'fields': {'f1': 3, 'f2': {'f2_1': 'mode_on'}}, 'ts': 1669407781439, 'dtag': None, 'tags': {}},\n    ], \n    'start_ts': 1669407781437, 'dtag': None}\n</code></pre> field data type meaning <code>start_ts</code> integer or None millisecond timestamp, not always set <code>points</code> list a list of data-points, sorted by their timestamps ascending <code>dtag</code> integer or None delivery tag, see description below"},{"location":"custom_nodes.html#points","title":"points","text":"<p>In a data-batch, <code>points</code> is a list of data-points, the list can be of any length (Note :there is no length check in place at the moment).</p>"},{"location":"custom_nodes.html#dtag_1","title":"dtag","text":"<p>See data-point for a description.</p>"},{"location":"custom_nodes.html#state-persistence","title":"State persistence","text":"<p>Faxe introduced the concept of state persistence for flows in version 1.2.0. With state persistence active for a flow, the faxe engine will persist state for every node in a flow to disc to be able to continue a flow where it left off, if for example a restart of the whole engine is necessary due to a version update.</p> <p>Read more about state persistence here.</p> <p>Custom python node can also utilize this feature. When on startup of a python node there is persisted state found for that node on the disc, this state data will be injected to the node's startup procedure.</p>"},{"location":"custom_nodes.html#activate-persistence","title":"Activate persistence","text":"<p>There are different ways, state gets persisted for custom python nodes:</p> <pre><code># defined in Faxe.py \n\nSTATE_MODE_HANDLE   = 'handle'\nSTATE_MODE_EMIT     = 'emit'\nSTATE_MODE_MANUAL   = 'manual'\n</code></pre> <ul> <li>STATE_MODE_HANDLE means, that state is automatically persisted, after every call to either <code>handle_batch</code> or <code>handle_point</code>.</li> <li>STATE_MODE_EMIT means, state is auto persisted after every call to self.emit() by the python node.</li> <li>STATE_MODE_MANUAL means, that it is completely up to the python node when to persist state.</li> </ul> <p>To choose which mode to use, simple implement the <code>state_mode</code> method:</p> <pre><code>from faxe import Faxe\n\ndef state_mode(self):\n        return Faxe.STATE_MODE_MANUAL\n</code></pre> <p>In the above example, the state mode is <code>manual</code>. In this mode the python node has to decide when to persist state and this is done with the <code>persist_state</code> method:</p> <pre><code>def persist_state(self, state=None):\n        \"\"\"\n        :param state: any|None\n        \"\"\"\n        ...\n</code></pre>"},{"location":"custom_nodes.html#manual","title":"manual","text":"<p>If state is None, then the state that gets persisted will be retreived either from the format_state method (see below) or, if format_state is not implemented, the state will be a dictionary with every member var of the object.</p> <p>Example:</p> <pre><code>def my_method(self):\n\n    ...\n\n    self.persist_state()\n\n    # OR provide state\n    self.persist_state(state={'mystate': 'dict'})\n\n    ...\n</code></pre>"},{"location":"custom_nodes.html#state-data","title":"State data","text":"<p>State data can be of any pickle-able type.</p> <p>If the python node does not overwrite the <code>format_state</code> method,  the Faxe base class will provide a dictionary with all the member vars of the python callback object using python's vars function.</p> <p>With the <code>format_state</code> method, the python node can provide state data as it wishes:</p> <pre><code>def format_state(self):\n   return {'counter': self.counter, 'items': self.items}\n</code></pre> <p>As shown above, state data can also be given, when calling the persist_state method.</p> <p>A python node can then get this state data with the <code>get_state</code> method:</p> <pre><code>def get_state(self):\n    \"\"\"\n    get the last persisted state data, that was given to this node\n    :return: any\n    \"\"\"\n    return self._pstate\n</code></pre> <p>Example:</p> <pre><code>from faxe import Faxe, Point, Batch\n\n\nclass Mynode(Faxe):\n\n    def init(self, args=None):\n\n        ...\n\n        # get the state\n        self.mystate = self.get_state()\n\n\n    ...\n</code></pre> <p>We can also use the <code>get_state_value</code> method (works when state data is a dictionary) and initialize member vars in an elegant way:</p> <pre><code>def get_state_value(self, key, default=None):\n        \"\"\"\n        get a specific entry from the state, if state is a dict, otherwise returns 'default'\n        :param key: string\n        :param default: any\n        :return: any\n        \"\"\"\n        if type(self._pstate) == dict:\n            if key in self._pstate:\n                return self._pstate[key]\n        return default\n</code></pre> <p>Example:</p> <pre><code>from faxe import Faxe, Point, Batch\n\n\nclass Mynode(Faxe):\n\n    def init(self, args=None):\n\n        ...\n\n        self.item_counter = self.get_state_value('item_counter', 0)\n        self.items = self.get_state_value('items', {})\n\n\n    ...\n</code></pre>"},{"location":"custom_nodes.html#helper-classes","title":"Helper classes","text":"<p>There are helper classes to make it easier to work with data coming from the faxe engine. All functions are static so you do not  instanciate them and nothing is stored inside an object, making it possible to mix the usage of the helpers with inline code.</p> <p>To use the helper classes, just import <code>faxe.Point</code> and/or <code>faxe.Batch</code>.</p> <pre><code>from faxe import Faxe, Point, Batch\n\n\nclass Mynode(Faxe):\n    ... \n</code></pre>"},{"location":"custom_nodes.html#helper-class-for-working-with-data-point-objects","title":"Helper class for working with data-point objects.","text":"<pre><code>class Point:\n\"\"\"\n    Completely static helper class for data-point structures (dicts)\n\n    point = dict()\n    point['ts'] = int millisecond timestamp\n    point['fields'] = dict()\n    point['tags'] = dict()\n    point['dtag'] = int\n\n    \"\"\"\n\n    @staticmethod\n    def new(ts=None):\n        p = dict()\n        p['fields'] = dict()\n        p['tags'] = dict()\n        p['dtag'] = None\n        p['ts'] = ts\n        return p\n\n    @staticmethod\n    def fields(point_data, newfields=None):\n\"\"\"\n        get or set all the fields (dict)\n        :param point_data: dict()\n        :param newfields: dict()\n        :return: dict()\n        \"\"\"\n        if newfields is not None:\n            point_data['fields'] = newfields\n            return point_data\n\n        return dict(point_data['fields'])\n\n    @staticmethod\n    def value(point_data, path, value=None):\n\"\"\"\n        get or set a specific field\n        :param point_data: dict()\n        :param path: string\n        :param value: any\n        :return: None, if field is not found /\n        \"\"\"\n        if value is not None:\n            Jsn.set(point_data, path, value)\n            return point_data\n\n        return Jsn.get(point_data, path)\n\n    @staticmethod\n    def values(point_data, paths, value=None):\n\"\"\"\n        get or set a specific field\n        :param point_data: dict\n        :param paths: list\n        :param value: any\n        :return: point_data|list\n        \"\"\"\n        if value is not None:\n            for path in paths:\n                Jsn.set(point_data, path, value)\n            return point_data\n\n        out = list()\n        for path in paths:\n            out.append(Jsn.get(point_data, path))\n        return out\n\n    @staticmethod\n    def default(point_data, path, value):\n\"\"\"\n\n        :param point_data:\n        :param path:\n        :param value:\n        :return:\n        \"\"\"\n        if Point.value(point_data, path) is None:\n            Point.value(point_data, path, value)\n\n        return point_data\n\n    @staticmethod\n    def tags(point_data, newtags=None):\n        if newtags is not None:\n            point_data['tags'] = newtags\n            return point_data\n\n        return point_data['tags']\n\n    @staticmethod\n    def ts(point_data, newts=None):\n\"\"\"\n        get or set the timestamp of this point\n        :param point_data: dict\n        :param newts: integer\n        :return: integer|dict\n        \"\"\"\n        if newts is not None:\n            point_data['ts'] = int(newts)\n            return point_data\n\n        return point_data['ts']\n\n    @staticmethod\n    def dtag(point_data, newdtag=None):\n        if newdtag is not None:\n            point_data['dtag'] = newdtag\n            return point_data\n\n        return point_data['dtag']\n</code></pre>"},{"location":"custom_nodes.html#helper-class-for-working-with-data-batch-objects","title":"Helper class for working with data-batch objects.","text":"<pre><code>class Batch:\n\"\"\"\n    Completely static helper class for data-batch structures (dicts)\n\n    batch = dict()\n    batch['points'] = list() of point dicts sorted by their timestamps\n    batch['start_ts'] = int millisecond unix timestamp denoting the start of this batch\n\n    batch['dtag'] = int\n    \"\"\"\n\n    @staticmethod\n    def new(start_ts=None):\n        b = dict()\n        b['points'] = list()\n        b['dtag'] = None\n        b['start_ts'] = start_ts\n        return b\n\n    @staticmethod\n    def empty(batch_data):\n\"\"\"\n        a Batch is empty, if it has no points\n        :param batch_data:\n        :return: True | False\n        \"\"\"\n        return ('points' not in batch_data) or (batch_data['points'] == [])\n\n    @staticmethod\n    def points(batch_data, points=None):\n\"\"\"\n\n        :param points: None | list()\n        :param batch_data: dict\n        :return: list\n        \"\"\"\n        if points is not None:\n            batch_data['points'] = points\n            Batch.sort_points(batch_data)\n            return batch_data\n\n        return list(batch_data['points'])\n\n    @staticmethod\n    def value(batch_data, path, value=None):\n\"\"\"\n        get or set path from/to every point in a batch\n        :param batch_data:\n        :param path:\n        :param value:\n        :return: list\n        \"\"\"\n        out = list()\n        points = batch_data['points']\n        for p in points:\n            out.append(Point.value(p, path, value))\n        if value is not None:\n            return batch_data\n\n        return out\n\n    @staticmethod\n    def values(batch_data, paths, value=None):\n\"\"\"\n        get or set path from/to every point in a batch\n        :param batch_data:\n        :param paths:\n        :param value:\n        :return: list\n        \"\"\"\n        if not isinstance(paths, list):\n            raise TypeError('Batch.values() - paths must be a list of strings')\n        if value is not None:\n            points = batch_data['points']\n            for p in points:\n                for path in paths:\n                    Point.value(p, path, value)\n            # batch_data['points'] = points\n            return batch_data\n        else:\n            out = list()\n            points = batch_data['points']\n            for p in points:\n                odict = dict()\n                for path in paths:\n                    odict[path] = Point.value(p, path, value)\n                out.append(odict)\n            return out\n\n    @staticmethod\n    def default(batch_data, path, value):\n\"\"\"\n\n        :param batch_data:\n        :param path:\n        :param value:\n        :return:\n        \"\"\"\n        points = batch_data['points']\n        for p in points:\n            Point.default(p, path, value)\n        return batch_data\n\n    @staticmethod\n    def dtag(batch_data):\n        return batch_data['dtag']\n\n    @staticmethod\n    def start_ts(batch_data, newts=None):\n        if newts is not None:\n            batch_data['start_ts'] = newts\n            return batch_data\n\n        return batch_data['start_ts']\n\n    @staticmethod\n    def add(batch_data, point):\n        if ('points' not in batch_data) or (type(batch_data['points']) != list):\n            batch_data['points'] = list()\n            batch_data['points'].append(point)\n        else:\n            batch_data['points'].append(point)\n            Batch.sort_points(batch_data)\n\n        return batch_data\n</code></pre>"},{"location":"datetime-parsing.html","title":"Timestamps and Datetime strings in Faxe","text":"<p>In Faxe we deal a lot with timestamps and datetime strings, in fact, every message that flows through all the nodes,  has a timestamp with it.</p> <p>Its inner representation of points in time is always an integer denoting the milliseconds that past since 1.1.1970. There is currently no timezone handling in Faxe.</p>"},{"location":"datetime-parsing.html#datetime-string-parsing","title":"Datetime string parsing","text":"<p>Faxe uses its own library to parse datetime strings (from the outside) into it's internal timestamp format. This is done in different nodes such as the mqtt_subscribe and the  amqp_consume node. Furthermore, datetime parsing can be done in lambda-expression with the dt_parse function.</p> <p>When parsing datetime strings with subsecond values smaller than milliseconds (microseconds and nanoseconds), if present,  will be rounded to millisecond values.</p>"},{"location":"datetime-parsing.html#parsing-examples","title":"Parsing examples","text":"datetime matching format string 'Mon, 15 Jun 2009 20:45:30 GMT' 'a, d b Y H:M:S Z' '19.08.01  17:33:44,867000 ' 'd.m.y  H:M:S,u ' '8/28/2033 8:03:45.576000 PM' 'n/d/Y l:M:S.u p'"},{"location":"datetime-parsing.html#conversion-specification","title":"Conversion Specification","text":"<p>The format used to parse and format dates closely resembles the one used in <code>strftime()</code>[1]. The most notable exception is that meaningful characters are not prefixed with a percentage sign (%) in datestring.</p> <p>Characters not matching a conversion specification will be copied to the output verbatim when formatting and matched against input when parsing. Meaningful characters can be escaped with a backslash (\\).</p> Character Sequence Parsing Formatting Description a Yes* Yes Abbreviated weekday name (\"Mon\", \"Tue\") A Yes Yes Weekday name (\"Monday\", \"Tuesday\") b Yes* Yes Abbreviated month name (\"Jan\", \"Feb\") B Yes* Yes Month name (\"January\", \"February\") d Yes Yes Day of month with leading zero (\"01\", \"31\") e Yes Yes Day of month without leading zero (\"1\", \"31\") F No Yes ISO 8601 date format (shortcut for \"Y-m-d\") H Yes Yes Hour (24 hours) with leading zero (\"01\", \"23\") I Yes Yes Hour (12 hours) with leading zero (\"01\", \"11\") k Yes Yes Hour (24 hours) without leading zero (\"1\", \"23\") l Yes Yes Hour (12 hours) without leading zero (\"1\", \"11\") m Yes Yes Month with leading zero (\"1\", \"12\") M Yes Yes Minute with leading zero (\"00\", \"59\") n Yes Yes Month without leading zero (\"1\", \"12\") o Yes Yes Ordinal number suffix abbreviation (st, nd, rd, th) p Yes* Yes AM/PM P Yes* Yes a.m./p.m. R No Yes The time as H:M (24 hour format) (\"23:59\") S Yes Yes Seconds with leading zero (\"00\", \"59\") T No Yes The time as H:M:S (24 hour format) (\"23:49:49\") c Yes No Milliseconds, 3 digits with leading zero (\"034\") u Yes Yes Microseconds, 6 digits with leading zero (\"000034\") f Yes No Nanoseconds, 9 digits with leading zero (\"000034000\") y Yes** Yes Year without century (\"02\", \"12\") Y Yes Yes Year including century (\"2002\", \"2012\") z Yes No UTC offset (+0100, +01:00, +01, -0100, -01:00, -01) Z Yes No Abbreviated timezone (UTC, GMT, CET etc) <p>* Case-insensitive when parsing</p> <p>** Falls back on current century of system when parsing years without century.</p>"},{"location":"datetime-parsing.html#special-formats","title":"Special formats","text":"dt_format description example 'millisecond' timestamp UTC in milliseconds 1565343079000 'microsecond' timestamp UTC in microseconds 1565343079000123 'nanosecond' timestamp UTC in nanoseconds 1565343079000123456 'second' timestamp UTC in seconds 1565343079 'float_nano' timestamp UTC float with nanosecond precision 1565343079.173588126 'float_micro' timestamp UTC float with microsecond precision 1565343079.173588 'float_millisecond' timestamp UTC float with millisecond precision 1565343079.173 'ISO8601' ISO8601 Datetime format string '2011-10-05T14:48:00.000Z' 'RFC3339' RFC3339 Datetime format string '2018-02-01 15:18:02.088Z'"},{"location":"flow_concurrency.html","title":"Flow concurrency","text":"<p>From every task (flow) registered with Faxe a number of copies (concurrent flows) can be started, this is called a task-group.</p> <p>Flow concurrency helps with building bridging functionality where it is useful to increase throughput by concurrency. Every member of a task-group will run exactly the same flow.</p>"},{"location":"flow_observer.html","title":"Flow observer","text":"<p>A flow observer process - one per flow - keeps track and reports connection problems and other errors that a flow may have. The only report backend at the moment is a configurable mqtt broker.</p> <p>mqtt topic : </p>"},{"location":"flow_observer.html#config","title":"config","text":"<pre><code>## ----------------------- FLOW HEALTH STATUS (observer) --------------------------\n## enable/disable flow health observer process\n## \n## Default: on\n## \n## ENV-Key: FAXE_FLOW_HEALTH_OBSERVER_ENABLE\n## \n## Acceptable values:\n##   - on or off\nflow_health.observer.enable = on\n\n## \n## Default: 3m\n## \n## ENV-Key: FAXE_FLOW_HEALTH_OBSERVER_REPORT_INTERVAL\n## \n## Acceptable values:\n##   - a time duration with units, e.g. '10s' for 10 seconds\nflow_health.observer.report_interval = 3m\n\n## \n## Default: on\n## \n## ENV-Key: FAXE_FLOW_HEALTH_HANDLER_MQTT_ENABLE\n## \n## Acceptable values:\n##   - on or off\nflow_health.handler.mqtt.enable = on\n\n## flow_health handler mqtt host\n## \n## Default: \n## \n## ENV-Key: FAXE_FLOW_HEALTH_HANDLER_MQTT_HOST\n## \n## Acceptable values:\n##   - text\n## flow_health.handler.mqtt.host = example.com\n\n## flow_health handler mqtt port\n## \n## ENV-Key: FAXE_FLOW_HEALTH_HANDLER_MQTT_PORT\n## \n## Acceptable values:\n##   - an integer\n## flow_health.handler.mqtt.port = 1883\n</code></pre>"},{"location":"introduction.html","title":"Intro","text":"<p>Faxe at it's core implements the architectural ideas of dataflow computing from the 1970s and 80s.</p> <p>The central idea was to replace the classic von Neumann architecture with something more powerful. In a von Neumann architecture, the processor follows explicit control flow, executing instructions one after another. In a dataflow processor, by contrast, an instruction is ready to execute as soon as all its inputs (typically referred to as \u201ctokens\u201d) are available, rather than when the control flow gets to it. This design promised efficient parallel execution in hardware when many \u201ctokens\u201d were ready to execute. -- The remarkable utility of dataflow computing</p> <p>Nowadays with the need to process large amounts of (flowing) data and with the rise of machine learning frameworks, dataflow computing has gained in popularity again.</p> <p>For example, machine learning frameworks like TensorFlow represent model training and inference as dataflow graphs, and the state transitions of actors (e.g., simulators used in reinforcement learning training) can be represented as dataflow edges, too.  Other research has extended the original dataflow graph abstraction for streaming computations. Instead of evaluating the dataflow graph once, with all inputs set at the beginning and all outputs produced at the end of evaluation, a streaming dataflow system continuously executes the dataflow in response to new inputs. This requires incremental processing and a stateful dataflow. In this setting, new inputs from a stream of input data combine with existing computation state inside the dataflow graph (e.g., an accumulator for a streaming sum). -- The remarkable utility of dataflow computing</p> <p>This is exactly where FAXE picks up the dataflow computing idea.</p> <p>To get started, we look at some of these concepts in Faxe.</p>"},{"location":"introduction.html#nodes","title":"Nodes","text":"<p>The components that make up the computing graph are called <code>nodes</code> in faxe. There are many <code>built-in nodes</code> for various different tasks, such as</p> <ul> <li>getting data from PLCs or Modbus devices</li> <li>reading and writing data from different databases</li> <li>publishing and consuming data from mqtt or other message brokers like RabbitMQ</li> <li>windowing</li> <li>statistics</li> <li>manipulating fields in data, that flows through the computing graph</li> <li>....</li> </ul> <p>Besides these nodes, FAXE users can also write <code>custom nodes implemented with python</code>, that can be used like the built-in ones in dataflows.</p> <p>FAXE is implemented in Erlang/OTP which makes it possible, that each of the nodes in a flow is running in its own seperate process. These processes share nothing in between them and only communicate with each other through <code>message passing</code>. This makes up for massive parallelism (concurrency) within a flow and also between all the flows running in a FAXE instance, matching exactly the architecture of the dataflow programming paradigm.  A FAXE instance can easily have thousands of processes runnning in parallel, which results in great throughput for a lot of data-streams.</p>"},{"location":"introduction.html#data","title":"Data","text":""},{"location":"introduction.html#data-point","title":"data-point","text":"<p>The smallest piece of data-item in FAXE is called a <code>data-point</code>. Since FAXE is mainly used for time series processes, these data-points always carry a unix timestamp in a field called <code>ts</code> with them. So a data-point holds data for a specific point in time and a series of such data-points then form this unbounded stream of data we call time series data.</p>"},{"location":"introduction.html#how-does-this-data-look-like","title":"How does this data look like ?","text":"<p>We can think of the before mentioned <code>data-point as a JSON object</code> (though internally it is not exactly JSON).</p> <pre><code>{\"ts\" : 1629812164152, \"value\" : 2.33}\n</code></pre> <p>The timestamp is always there and the field holding it is always called <code>ts</code>.</p> <p>Next to the timestamp a data-point can have any number of other <code>fields</code>, a field can be of the basic data-type like <code>int, string, float</code>, or it can be an object itself, possibly deeply nested. Basically everything that is allowed in the JSON format.</p> <pre><code>{\n\"ts\":1629812164152,\n\"values\":{\n\"value1\":12,\n\"value2\":\"a string\",\n\"value3\":{\n\"value3_1\":4.232341\n    }\n  }\n}\n</code></pre> <pre><code>{\n\"ts\":1629812164152,\n\"values\":[\n    {\n\"value1\":12\n    },\n    {\n\"value2\":\"a string\"\n    }\n  ]\n}\n</code></pre> <p>In order to deal with these data structures, we can use a basic form of JSON-Path.</p> <p>For example to reference the field value2 in the second example, we would use the following <code>path</code>:</p> <pre><code>values.value2\n</code></pre> <p>The path for the field value3_1 in the second example:</p> <pre><code>values.value3.value_3_1\n</code></pre> <p>In the third example we have a json-array, we can reference the field value2 like so (array indexes are 0 based):</p> <pre><code>values[1].value2\n</code></pre>"},{"location":"introduction.html#dots-in-fieldnames","title":"Dots in fieldnames","text":"<p>If you have to deal with dots in fieldnames, there is a syntax for this: you can use a <code>star</code> instead in faxe flow scripts:</p> <pre><code>{\n\"ts\":1629812164152,\n\"data\": {\n\"stats.speed\": 22.3,\n\"stats.freq\": 440.0,\n\"stats.cnt\": 12\n  }\n}\n</code></pre> <p>The path for the field stats.freq can be reached using a star character:</p> <pre><code>data.stats*freq\n</code></pre> <p>Note: You should absolutely not use such a notation for you data, since normally you would in- and  output json data from flows and such a notation is against the rules of JSON-Path, the '.' character is the child operator. Start would be the wildcard in JSON path, but since Faxe does not support wildcards explicitely, we use it for literal dots in field names.</p> <p>It is recommended to only use the star notation, if you are dealing with data from outside, that already has dots in fieldnames. You should immediately rename this kind of data, before any other processing in a flow.</p>"},{"location":"introduction.html#data-batch","title":"data-batch","text":"<p>There is a second type of data-item called <code>data-batch</code>, which is simply a <code>list of data-points</code>. The list of data-points that make up a data-batch is ordered by the points' timestamps.</p> <p>In JSON notation a data-batch will look like this:</p> <pre><code>[\n  {\n\"ts\":1629812164152,\n\"values\":{\n\"value1\":12,\n\"value2\":\"res-433\"\n    }\n  },\n  {\n\"ts\":1629812164154,\n\"values\":{\n\"value1\":13,\n\"value2\":\"res-124\"\n    }\n  },\n  {\n\"ts\":1629812164156,\n\"values\":{\n\"value1\":11,\n\"value2\":\"res-712\"\n    }\n  }\n]\n</code></pre>"},{"location":"introduction.html#strings-and-references","title":"Strings and References","text":"<p>Unlike what is possible in some programming languages, where you can use two different string notations: <code>'a string'</code> or <code>\"also a string\"</code>, in DFS these two notations have a completely different meaning.</p> <p>In DFS, single quotes are used for strings</p> <pre><code>'faxe is canned beer' \n\n'baseField.subField'\n</code></pre> <p>or for text</p> <pre><code>'\n    SELECT * \n    FROM table \n    ORDER BY timestamp\n    LIMIT 5;\n'   \n</code></pre> <p>Double quotes are used for references and are used only in lambda-expressions, to retrieve the value of the specified field from the current data-point.</p> <pre><code>lambda: \"data.value\" &gt; 3  \n</code></pre> <p>Return whether the value at <code>data.value</code> is greater than 3.</p>"},{"location":"introduction.html#lambda-expressions","title":"Lambda expressions","text":"<p>See lambda_expressions.</p>"},{"location":"introduction.html#rest-api","title":"Rest Api","text":"<p>FAXE can be managed via its rest api.</p>"},{"location":"introduction.html#how-nodes-are-connected","title":"How nodes are connected","text":"<p>See node_connection.</p>"},{"location":"metrics.html","title":"Metrics, Connection status, Debug events and Logs","text":"<p>For debugging and observability Faxe exposes internal metric as well as connection status events. Furthermore, every running flow can emit debugging and log events.</p> <p>All these events can be published to a mqtt broker.</p>"},{"location":"metrics.html#topics-and-routing-keys","title":"Topics and routing keys","text":"<p>Topics for the mqtt emitters can be prefixed with the config-value <code>base_topic</code> (see config). Note: MQTT topics should not start with a <code>/</code> character.</p> type topic base_topic default metrics per node {<code>base_topic</code>}/metrics/{flow_id}/{node_id}/{metric_name} sys/ metrics per flow {<code>base_topic</code>}/metrics/{flow_id} sys/ conn_status {<code>base_topic</code>}/conn_status/{flow_id}/{node_id} sys/ debug per node {<code>base_topic</code>}/debug/{flow_id}/{node_id}/{debug_type} sys/ logs per node {<code>base_topic</code>}/log/{flow_id}/{node_id} sys/"},{"location":"metrics.html#node-metrics","title":"Node Metrics","text":"<p>Faxe will collect and periodically emit various metrics to configurable endpoints. Metrics are collected for each individual node and a summery for whole tasks.</p> <p>These are the metrics that will be collected for every node running in a task:</p> metric name description metric fields <code>items_in</code> number of items a node received from other nodes or over the network <code>items_out</code> number of items a node emitted to other nodes or over some network connection <code>processing_errors</code> the number of errors that occurred during processing <code>mem_used</code> memory usage in bytes <code>msg_q_size</code> number of items currently in the node-process' message-queue <code>processing_time</code> time in milliseconds it took the node to process 1 item <p>Nodes that start a network connection have additional metrics  (such as the modbus, s7read, mqtt, ... - nodes): </p> metric name description metric fields <code>bytes_read</code> the number of bytes read from a network port see meter <code>bytes_read_size</code> the size of packets read or received from a network port in bytes see histogram <code>bytes_sent</code> the number of bytes send over the network see meter <code>bytes_sent_size</code> the size of packets sent over a network port in bytes see histogram"},{"location":"metrics.html#examples","title":"Examples","text":"<p>some metric example datapoints in json-format</p> <pre><code>{\"ts\":1592386096330,\"id\":\"00000\",\"df\":\"92.001\",\n\"data\":{\"type\":\"counter\",\"node_id\":\"default3\",\n\"metric_name\":\"processing_errors\",\n\"flow_id\":\"41ef642f-e5ee-4c48-8bd9-565de810f242\",\"counter\":0}}\n</code></pre> <pre><code>{\"ts\":1592386096330,\"id\":\"00000\",\"df\":\"92.001\",\n\"data\":{\"type\":\"histogram\",\"node_id\":\"default3\",\"n\":8,\"min\":0.011,\n\"metric_name\":\"processing_time\",\"mean\":0.016625,\"max\":0.028,\n\"flow_id\":\"41ef642f-e5ee-4c48-8bd9-565de810f242\"}}\n</code></pre> <pre><code>{\"ts\":1592386076289,\"id\":\"00000\",\"df\":\"92.001\",\n\"data\":{\"type\":\"gauge\",\"node_id\":\"default3\",\"metric_name\":\"msg_q_size\",\n\"gauge\":0,\"flow_id\":\"41ef642f-e5ee-4c48-8bd9-565de810f242\"}}\n</code></pre> <pre><code>{\"ts\":1592386076289,\"id\":\"00000\",\"df\":\"92.001\",\n\"data\":{\"type\":\"meter\",\"one\":0.2,\"node_id\":\"default3\",\n\"metric_name\":\"items_out\",\"instant\":0.2,\n\"five\":0.2,\"fifteen\":0.2,\"count\":2,\n\"flow_id\":\"41ef642f-e5ee-4c48-8bd9-565de810f242\"}}\n</code></pre>"},{"location":"metrics.html#common-fields","title":"Common fields","text":"field name meaning data.type the metric type, see table below data.node_id the nodes id data.flow_id id of the flow, the node belongs to"},{"location":"metrics.html#fields-by-metrics-type","title":"Fields by metrics-type","text":"metric-type field meaning counter counter total counted number meter instant number of occurrences in the last 5 sec meter one 1 min exponentially weighted moving average meter five 5 min exponentially weighted moving average meter fifteen 15 min exponentially weighted moving average meter count total number gauge gauge point-in-time single value histogram mean mean value histogram min minimum value histogram max maximum value histogram n number of values"},{"location":"metrics.html#flow-metrics","title":"Flow Metrics","text":"<p>For every task there is a summary of the node metrics:</p> <pre><code>{\"ts\":1592393302700,\"id\":\"00000\",\"df\":\"92.002\",\n\"data\":{\n\"ts\": 1631557230000,\n\"processing_time\": 0.088,\n\"processing_errors\": 0,\n\"msg_q_size\": 0,\n\"mem_used\": 17768,\n\"items_out\": 60,\n\"items_in\": 60,\n\"bytes_sent_avg\": 123,\n\"bytes_sent\": 24.6,\n\"bytes_read_avg\": 104,\n\"bytes_read\": 20.8,\n\"flow_id\": \"http_get_test\"\n  }\n}\n</code></pre>"},{"location":"metrics.html#fields-in-flow-metrics","title":"Fields in flow metrics","text":"metric name description note <code>processing_time</code> average processing time for the flow in milliseconds <code>processing_errors</code> sum of errors for all flow nodes <code>msg_q_size</code> total number of items currently in process-queues for all flow nodes <code>mem_used</code> total number of bytes the flow and all of its nodes are currently using <code>items_out</code> maximum of all nodes' <code>items_out</code> value <code>items_in</code> maximum of all nodes' <code>items_in</code> value <code>bytes_sent_avg</code> sum of bytes sent over the network for all nodes in the flow <code>bytes_sent</code> sum of the 5-minute exponential moving averages from all nodes' <code>bytes_sent</code> metrics <code>bytes_read_avg</code> sum of bytes received from the network for all nodes in the flow <code>bytes_read</code> sum of the 5-minute exponential moving averages from all nodes' <code>bytes_read</code> metrics"},{"location":"metrics.html#configuration","title":"Configuration","text":"<p>With the configuration setting <code>metrics.handler.mqtt.enable</code> you can turn on/off publishing of metrics for a faxe instance. If this setting is set to <code>on</code> flow metrics get published with the interval set with <code>metrics.publish_interval</code>.</p> <p>To additionally enable publishing of single node metrics for a faxe flow, use the RestAPI endpoint <code>/v1/task/start_metrics_trace/:task_id/[:duration_minutes]</code>.</p> <p><code>duration_minutes</code> defaults to the config setting <code>debug.time</code>.</p>"},{"location":"metrics.html#publish-metrics-events","title":"Publish metrics events","text":"<p>Faxe has 2 different metrics-handlers that can be configured. MQTT and AMQP metrics emitter. See config section for details.</p>"},{"location":"metrics.html#use-metrics-in-tasks","title":"Use metrics in tasks","text":"<p>Faxe's internal metrics can be used in tasks(flows) with the metrics node.</p>"},{"location":"metrics.html#connection-status","title":"Connection status","text":"<p>Faxe tracks the status of every external connection it opens and exposes events. These events can be used in tasks with the conn_status node.</p> <p>They can also be sent to a mqtt and/or amqp broker. </p>"},{"location":"metrics.html#examples_1","title":"Examples","text":"<p>connecting to an mqtt-broker on ip 10.14.204.3 and port 2883 ...</p> <pre><code>{\"ts\":1592386056299,\"id\":\"00000\",\"df\":\"92.003\",\n\"data\":{\"status\":2,\"port\":2883,\"peer\":\"10.14.204.3\",\"node_id\":\"sys\",\n\"flow_id\":\"sys\",\"connected\":false,\"conn_type\":\"mqtt\"}}\n</code></pre> <p>connected to the mqtt-broker </p> <pre><code>{\"ts\":1592386056319,\"id\":\"00000\",\"df\":\"92.003\",\n\"data\":{\"status\":1,\"port\":1883,\"peer\":\"10.14.204.3\",\"node_id\":\"sys\",\n\"flow_id\":\"sys\",\"connected\":true,\"conn_type\":\"mqtt\"}}\n</code></pre> <p>The connection status is represented by the boolean value <code>connected</code> and an enum <code>status</code>.</p> <code>connected</code> <code>status</code> meaning false 0 not connected false 2 connecting true 1 connected"},{"location":"metrics.html#configuration_1","title":"Configuration","text":""},{"location":"metrics.html#publish-connection-status-events","title":"Publish connection status events","text":"<p>As stated above, FAXE has 2 different conn_status-handlers that can be configured : See config section for details.</p>"},{"location":"metrics.html#debug-and-logs","title":"Debug and Logs","text":"<p>For debugging purposes faxe flows can expose events on items going in and out of every node in a flow. Like with metrics and conn_status events, these events can be published to a mqtt/amqp broker.  Debug and Log events must be started explicitly, and they will be published for a certain configurable amount of time.</p> <p>To enable publishing of debug events for a faxe flow, use the RestAPI endpoint <code>/v1/task/start_debug/:task_id/[:duration_minutes]</code>. <code>duration_minutes</code> defaults to the config setting <code>debug.time</code>.</p> <p>(This is for debugging purposes).</p> <p>See config section for details.</p> <p>Example debug data item:</p> <pre><code>{\"ts\":1594627419206,\"id\":\"00000\",\"df\":\"00.000\", \n\"data\":\n    {\"meta\":\n        {\"type\":\"item_in\",\"port\":1,\"node_id\":\"win_time6\",\"flow_id\":\"trace_test\"},\n\"data_item\":\"{\\\"ts\\\":1594627419205,\\\"id\\\":\\\"00000\\\",\\\"df\\\":\\\"00.000\\\",\\\"data\\\":{\\\"val\\\":4.770044683775623}}\"}}\n</code></pre> <p>Example log</p> <pre><code>{\"ts\":1595317825817,\"id\":\"00000\",\"df\":\"00.000\",\n\"data\":\n        {\"node_id\":\"eval26\",\"meta\":\n            {\"pid\":\"&lt;0.1750.0&gt;\",\"node\":\"faxe@ubuntu\",\n\"module\":\"df_component\",\"line\":316,\n\"function\":\"handle_info\",\"application\":\"faxe\"},\n\"message\":\"'error' in component esp_eval caught when processing item: \n            {1,{data_point,1595317823813,#{&lt;&lt;\\\"esp_avg\\\"&gt;&gt; =&gt; 6.0685635225505425,\n                &lt;&lt;\\\"factored\\\"&gt;&gt; =&gt; 3.0342817612752713},#{},&lt;&lt;&gt;&gt;}} -- \\\"\\\\n    \n                gen_server:try_dispatch/4 line 637\\\\n    df_component:handle_info/2 line 314\\\\n    \n                esp_eval:process/3 line 39\\\\n    esp_eval:eval/4 line 44\\\\n    lists:foldl/3 line 1263\\\\n    \n                esp_eval:'-eval/4-fun-0-'/4 line 47\\\\n    faxe_lambda:execute/3 line 34\\\\n    \n                erlang:'/'(undefined, 2)\\\\nEXIT:badarith\\\"\",\"level\":\"error\",\"flow_id\":\"script5\"}}\n</code></pre> <p>A data_point caused an error in an eval node.</p>"},{"location":"os_tuning.html","title":"Os tuning","text":""},{"location":"os_tuning.html#tune-os-limits","title":"Tune OS Limits","text":""},{"location":"os_tuning.html#number-of-file-desciptors","title":"Number of file desciptors","text":"<p>https://www.rabbitmq.com/install-debian.html#kernel-resource-limits</p> <p>https://github.com/basho/basho_docs/blob/master/content/riak/kv/2.2.3/using/performance/open-files-limit.md#debian--ubuntu</p> <p>https://docs.riak.com/riak/kv/2.2.3/using/performance/open-files-limit/#debian-ubuntu</p>"},{"location":"os_tuning.html#for-current-session","title":"For current session","text":"<pre><code>$ ulimit -n 100000\n</code></pre>"},{"location":"os_tuning.html#permanent-settings","title":"Permanent settings","text":"<p>Linux: In /etc/security/limit.conf set these two lines:</p> <pre><code>* soft nofile 65536\n* hard nofile 100000\n</code></pre>"},{"location":"os_tuning.html#docker","title":"Docker","text":"<p>https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-configuration-file</p>"},{"location":"os_tuning.html#erlang","title":"erlang","text":"<p>Q flag is used in vm.args</p> <p><code>+Q Number</code></p>"},{"location":"os_tuning.html#check-actual-number-used","title":"check actual number used","text":"<pre><code>erlang:system_info(port_limit).\n</code></pre>"},{"location":"state_persistence.html","title":"Flow state persistence","text":"<p>In version 1.2.0 faxe introduce the concept of state persistence for flows.</p>"},{"location":"state_persistence.html#what","title":"What ?","text":"<p>TBD</p>"},{"location":"stop_when_idle.html","title":"Auto stop feature","text":"<p>Since version 1.1.0</p> <p>Normally a flow in faxe is supposed to run 24/7/365 and forever, but sometimes it is desirable to stop a flow under special conditions.</p> <p>Faxe has a feature called <code>stop when idle</code>, that can be used in every flow and from every node.</p>"},{"location":"stop_when_idle.html#example","title":"Example:","text":"<p>Suppose we have the following simple flow, that consumes from a RabbitMQ Broker  and writes data into a CrateDB database:</p> <pre><code>|amqp_consume()\n.host('deves-amqp-cluster1.internal') \n.bindings('my.routing.key')\n.exchange('x_xchange')\n.queue('faxe_test')\n\n\n|batch(30)\n.timeout(5s)\n\n|crate_out() \n.table('customer1')\n.db_fields('stream', 'measurement')\n.faxe_fields('stream', 'measurement') \n</code></pre> <p>Say we know, that this flow will only get data for a limited amount of time and after that time, we want to stop the flow to save resources. What we do not know, is the exact time when we do not get any more data to write to the database.</p> <p>The <code>stop when idle</code> feature can help us here:</p> <pre><code>|amqp_consume()\n.host('deves-amqp-cluster1.internal') \n.bindings('my.routing.key')\n.exchange('x_xchange')\n.queue('faxe_test')\n\n\n|batch(30)\n.timeout(5s)\n\n|crate_out() \n.table('customer1')\n.db_fields('stream', 'measurement')\n.faxe_fields('stream', 'measurement') \n\n%% we use the stop when idle feature here\n._stop_idle(true)\n._idle_time(3m)\n</code></pre> <p>When the <code>_stop_idle</code> parameter is true for any node in a flow (could be used in more than on node in a flow),  the nodes base process will check periodically, if the node is idle already for the given amount of time (3 minutes in the above example).</p>"},{"location":"stop_when_idle.html#what-does-idle-exactly-mean","title":"What does idle exactly mean ?","text":"<p>A node is idle, if we do not see any data going in or out for a given amount of time.</p>"},{"location":"stop_when_idle.html#result","title":"Result","text":"<p>If a node, where this feature is used (<code>_stop_idle</code> is set to true), is idle for the amount of time, it will tell the dataflow system to stop the flow it is part of. The stop behavior is equivalent to this stop call (permanently stopping a flow) via faxe's REST API.</p>"},{"location":"stop_when_idle.html#conditional","title":"Conditional","text":"<p>Since version 1.1.6 a conditional lambda expression can be given with <code>_stop_when</code>. If this is present, the expression must evaluate to true once for any data-item, before the idle-time is measured and the feature becomes active.</p>"},{"location":"stop_when_idle.html#using-the-feature","title":"Using the feature","text":"<p>As mentioned above, this feature can be used on any node in a flow, including custom nodes written in python. If more than one node in a flow has <code>_stop_idle</code> set to true, the first node that detects it is idle, will cause the whole flow to stop.</p>"},{"location":"stop_when_idle.html#parameters","title":"Parameters","text":"Parameter Description Default _stop_idle(<code>boolean</code>) if set to true, the node will periodically check, if it was idle for at least the time given with <code>_idle_time</code> false _idle_time(<code>duration</code>) amount of time a node must be idle, before it initiates a stop procedure for the flow it is part of 5m _stop_when(<code>lambda</code>) if given, the lambda expression must evaluate to true (once!), before the idle time is measured undefined"},{"location":"dfs_examples/consume_data_rewrite_republish.html","title":"Consume data rewrite republish","text":""},{"location":"dfs_examples/consume_data_rewrite_republish.html#data-cleaning-and-publishing","title":"Data cleaning and publishing","text":"<p>Consume data from an MQTT-Broker and do some cleaning, at the end republish this data.</p> <pre><code>    def topic_in = 'my/topic/in'\n    def topic_out = 'my/topic/out'\n    def host = '192.168.1.2'\n\n    |mqtt_subscribe()\n    .host(host) \n    .topic(topic_in)\n    .dt_field('UTC-Time')\n    .dt_format('float_micro')\n\n    %% here is were we clean data\n\n    |eval(\n        lambda: int(\"u_length\" * 1000),\n        lambda: int(\"u_width\" * 1000),\n        lambda: int(\"lc_quantity\"),\n        lambda: int(\"pcs_lost\")\n    )\n    %% overwrite the original fields \n    .as(\n        'u_length',\n        'u_width',\n        'lc_quantity',\n        'pcs_lost'\n        )\n\n    |delete('UTC-Time')\n\n    % publish the resulting message \n\n    |mqtt_publish()\n        .host(host) \n        .qos(1)\n        .topic(topic_out)\n        .retained()\n</code></pre> <p><code>Note:</code> If topic_in = topic_out we create a loop using the mqtt broker, something we do not want normally.</p>"},{"location":"dfs_examples/fun_with_http.html","title":"Fun with http","text":""},{"location":"dfs_examples/fun_with_http.html#fun-with-http","title":"fun with http","text":"<p>Using faxe's http nodes. Http GET and POST from and to itself.</p> <pre><code>def path2 = '/faxe_stats'\n\n%% first set up a listen node to receive data via http\n|http_listen()\n.path(path2)\n.port(8899)\n.payload_type('json')\n.as('recv')\n\n|debug()\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\ndef path = '/v1/stats/faxe'\ndef host = '127.0.0.1'\ndef port = 8081\n\n%% call faxe's own rest api, and get some stats fields\n|http_get()\n.host(host)\n.port(port)\n.path(path)\n.every(3s)\n\n%% post this data to the http_listen node setup at the beginning of the script\n|http_post()\n.host(host)\n.port(8899)\n.path(path2)\n</code></pre>"},{"location":"dfs_examples/message_re-modeling.html","title":"Message re modeling","text":"<p>This example uses different faxe nodes to remodel and extend a json message that is received via mqtt.</p>"},{"location":"dfs_examples/message_re-modeling.html#input-message","title":"Input message","text":"<pre><code>{\n\"ts\": 1634657531710,\n\"data\": {\n\"availablitiy\": \"NRDY\",\n\"operatingState\": \"ON\",\n\"operatingMode\": \"AUTO\",\n\"alarmState\": \"NOAL\",\n\"errorStates\": [],\n\"Typ\": \"ATS\",\n\"Id\": 3179\n  }\n}\n</code></pre>"},{"location":"dfs_examples/message_re-modeling.html#desired-message","title":"Desired message","text":"<pre><code>{\n\"ts\": 1634657531710,\n\"data\": {\n\"robot_state\": \"BUSY\",\n\"order_state\": {\n\"name\": \"NRDY\",\n\"id\": 1\n    },\n\"operating_state\": {\n\"name\": \"ON\",\n\"id\": 0\n    },\n\"operating_mode\": {\n\"name\": \"AUTO\",\n\"id\": 1\n    },\n\"alarm_state\": {\n\"name\": \"NOAL\",\n\"id\": 0,\n\"errors\": []\n    },\n\"Typ\": \"ATS\",\n\"Id\": 3179\n  }\n}\n</code></pre>"},{"location":"dfs_examples/message_re-modeling.html#example","title":"Example","text":"<pre><code>%% enum mappings\ndef alarm_state_map = '{\"NOAL\":0,\"ERR\":1}'\ndef opmode_map = '{\"NOMO\":0,\"AUTO\":1,\"MANU\":2}'\ndef opstate_map = '{\"ON\":0,\"OFF\":1}'\ndef order_state_map = '{\"RDY\":0,\"NRDY\":1,\"OFF\":2}'\n\ndef topic_out = 'msm/r1/grp/condition/robot_state'\n\ndef republish_timeout = 15s\ndef topic_in = 'msm/r1/grp/wms/RMST/v1'\n\ndef out =\n|mqtt_subscribe()\n    .host('10.10.1.102')\n    .topic(topic_in)\n    .include_topic(false) \n\n|rename()\n.fields(\n    'data.alarmState',\n    'data.operatingMode',\n    'data.operatingState',\n    'data.errorStates',\n    'data.availablitiy'\n)\n.as_fields(\n    'data.alarm_state.name',\n    'data.operating_mode.name',\n    'data.operating_state.name',\n    'data.alarm_state.errors',\n    'data.order_state.name'\n)\n\n|eval(\n    lambda: map_get(\"data.alarm_state.name\", alarm_state_map),\n    lambda: map_get(\"data.operating_mode.name\", opmode_map),\n    lambda: map_get(\"data.operating_state.name\", opstate_map),\n    lambda: map_get(\"data.order_state.name\", order_state_map)\n)\n.as(\n    'data.alarm_state.id',\n    'data.operating_mode.id',\n    'data.operating_state.id',\n    'data.order_state.id'\n)\n\n|case(\n    lambda: \"data.alarm_state.name\" == 'ERR',\n    lambda: \"data.operating_state.name\" == 'ON' AND \"data.operating_mode.name\" == 'AUTO' AND \"data.order_state.name\" == 'NRDY',\n    lambda: \"data.operating_state.name\" == 'ON' AND \"data.operating_mode.name\" == 'AUTO' AND \"data.order_state.name\" == 'IDLE'\n)\n.values(\n    'ERROR',\n    'BUSY',\n    'IDLE'\n)\n.default('OFF')\n.as('data.robot_state')\n\nout\n|mqtt_publish()\n.topic(topic_out) \n</code></pre>"},{"location":"dfs_examples/python_double.html","title":"Python double","text":""},{"location":"dfs_examples/python_double.html#custom-python-node","title":"Custom python node","text":"<p>Using a custom python node called <code>double</code> to double values of a data_batch. Here data is produced every 2s, then accumulated to a data_batch of length 6, the result gets then forwarded to our custom python node, which doubles all values of the field <code>val</code>.</p>"},{"location":"dfs_examples/python_double.html#dfs","title":"dfs","text":"<pre><code>|value_emitter()\n.every(2s)\n.type(point)\n\n|win_event()\n.every(6) \n\n@double()\n.field('val')\n.as('double_val')\n\n|debug()\n</code></pre>"},{"location":"dfs_examples/python_double.html#python","title":"python","text":"<pre><code>from faxe import Faxe\n\n\nclass Double(Faxe):\n\n    @staticmethod\n    def options():\n        opts = [\n            (b'field', b'string'),\n            (b'as', b'string')\n        ]\n        return opts\n\n    def init(self, args):\n        self.fieldname = args[b'field']\n        self.asfieldname = args[b'as']\n\n    def handle_point(self, point_data):\n        self.emit(self.calc(point_data))\n\n    def handle_batch(self, batch_data):\n        out_list = list()\n        for point in batch_data:\n            out_list.append(self.calc(point))\n        self.emit(out_list)\n\n    def calc(self, point_dict):\n        point_dict[self.asfieldname] = point_dict[self.fieldname] * 2\n        return point_dict\n</code></pre>"},{"location":"dfs_examples/read_modbus_publish_mqtt.html","title":"Read modbus publish mqtt","text":""},{"location":"dfs_examples/read_modbus_publish_mqtt.html#modbus-to-mqtt","title":"Modbus to MQTT","text":"<p>In this example we write a simple DFS which reads energy data from a modbus-tcp device periodically and publishes to an mqtt broker.</p> <pre><code>def device_id = 255\ndef modbus_ip = '127.0.0.1'\ndef mqtt_broker = '10.14.204.3'\n\n|modbus()\n.ip('127.0.0.1')\n%% not the default port here\n.port(8899)\n.device(device_id)\n%% we read the values every second\n.every(1s)\n\n%% we read 3 values \n.function('coils', 'hregs', 'iregs')\n%% start addresses\n.from(2127, 3008, 104)\n%% amount of data for each value\n.count(1, 2, 2)\n%% we want these resulting fieldnames\n.as('ActiveEnergyConsumption', 'MaximalCurrentValue', 'BlindEnergyDelivered') \n\n%% add some default values to each message\n\n|default()\n.fields('id', 'vs', 'df')\n.field_values('my_id_string', 1, '01.010')\n\n%% publish to mqtt broker\n\n|mqtt_publish()\n.host(mqtt_broker)\n.port(1883)\n.qos(1)\n.topic('ttopic/energy')\n.retained()\n</code></pre>"},{"location":"dfs_examples/using_mem_node.html","title":"Using mem node","text":""},{"location":"dfs_examples/using_mem_node.html#mem-node","title":"mem node","text":"<p>Using the mem node standalone with predefined data as a lookup table.</p> <pre><code>def default_map = \n    '{\"key1\":\"topic/1/new\", \"key2\":\"topic/2/new\", \"key3\":\"topic/3/new\"}'\n\n%% setup the mem node with the json-map, we can reference this map later in lambda expressions with\n%% the key `topic_lookup`\n|mem() \n.key('topic_lookup')\n.default(default_map)\n.default_json()\n</code></pre>"},{"location":"dfs_script_language/index.html","title":"Introducing the DFS Script Language","text":"<p>Faxe  uses a Domain Specific Language(DSL) named dfs (Dataflow Scripting Language) to define dataflow tasks  involving the extraction, collection, transformation and loading and writing of data and involving, moreover,  the tracking of arbitrary changes and the detection of events within data. </p> <p>Dfs is heavily influenced by InfluxData's TICKScript.</p> <p>Dfs is used in .dfs files or via API to define pipelines and graphs for processing data.  The Dfs language is designed to chain together the invocation of data processing operations defined in nodes.</p> <p>At the heart of its's engine, faxe will run an acyclic graph of computing nodes.  Every node runs in its own erlang process.</p>"},{"location":"dfs_script_language/index.html#dfs-definitions","title":"DFS Definitions","text":""},{"location":"dfs_script_language/index.html#keywords","title":"Keywords","text":"Word Usage true boolean true false boolean false TRUE boolean true FALSE boolean false lambda: used to denote lambda expression e: used to denote script expressions def starts a variable declaration"},{"location":"dfs_script_language/index.html#operators","title":"Operators","text":"Operator Usage + addition operator - substraction operator / division operator * multiplication operator AND and OR or &lt; less than &gt; greater than =&lt; less than or equal &lt;= less than or equal =&gt; greater or equal &gt;= greater or equal == equal != Not equal /= Not equal ! Logical Not rem remainder div integer division <p>These operators are mainly used in Lambda expressions.</p>"},{"location":"dfs_script_language/index.html#chaining-operators","title":"Chaining operators","text":"Operator Usage Example <code>|</code> Used to declare a new node instance and chains it to the node above it (if any) <code>|some_node() |debug()</code> <code>||</code> Used to reference a macro script <code>||some_macro().some_param(3)</code> <code>.</code> Declares a property (or parameter) call, setting or changing an internal param in the node to which it belongs <code>|log() .file('log1.txt')</code> <code>@</code> Declares a user defined node written in python. Same as <code>|</code>, but for user defined nodes <code>|some_node() ... @mynode()</code>"},{"location":"dfs_script_language/index.html#variables-and-literals","title":"Variables and literals","text":"<p>Variables are declared using the keyword <code>def</code> at the start of a declaration.  Variables are immutable and cannot be reassigned new values later on in the script,  though they can be used in other declarations and can be passed into functions, property calls and text-templates.</p>"},{"location":"dfs_script_language/index.html#variable-declarations","title":"Variable declarations","text":"<pre><code>    def string = 'this is a string !'\n    def text = ' this is a text with some weird chars :// %&amp; '\n    %% escape single quotes in strings with a second single quote:\n    def string_with_single quotes = 'my string has ''single quotes'' in it'\n    def func = lambda: \"value\" / 3\n    def expr = e: str_replace(string, ' ', '_')\n    def meas = 4.44\n    % A lambda expression as literal\n    def func2 = lambda: int(meas / 13)\n    def an_int = 32342\n    def a_float = 2131.342\n\n    % a chain can also be bound to a declaration\n    def in1 =\n        |mqtt_subscribe()\n        .host('127.0.0.1')\n        .topic('some/topic')\n    % it can then be used like so\n    in1\n        |debug()\n</code></pre>"},{"location":"dfs_script_language/index.html#datatypes","title":"Datatypes","text":"<p>DFS recognizes a view basic data types, the type of the literal will be interpreted from its declaration.</p> Type name Description Examples string String/text type. Single quotes are used for strings, strings can also be multiline.To use single quotes in your string, simple use 2 single quotes (since 0.19.0) 'this_is_a_string'  since 0.19.0: 'SELECT MEAN(obj[''current'']) FROM mytable' integer Integer type. Arbitrarily big ints are allowed 123456789987654321, 55 float Floating point number. May be arbitrarily big 12.343422023, 5.6 double Same as float 12.343422023, 5.6 duration A duration literal. See section below. 34s, 500ms, 2d lambda A lambda expression. See extra section in this documentation. lambda: str_downcase('BIG') list A list of above simple types. ['a', 'b'] [1, 456,  4536]"},{"location":"dfs_script_language/index.html#json-strings","title":"Json strings","text":"<p>A bunch of faxe's built-in lambda functions and some nodes (json_emitter for example) can directly work with json strings, in faxe, we call them <code>jsn</code>.</p>"},{"location":"dfs_script_language/index.html#duration-literals","title":"Duration literals","text":"<p>Duration literals define a span of time. </p> <p>A duration literal is comprised of two parts: an integer and a duration unit. It is essentially an integer terminated by one or a pair of reserved characters, which represent a unit of time.</p> <p>The following table presents the time units used in declaring duration types.</p> Unit Meaning ms millisecond s second m minute h hour d day w week <p>Internally all time and duration related values are converted to milliseconds.</p>"},{"location":"dfs_script_language/index.html#examples","title":"Examples","text":"<pre><code>def span = 10s\ndef frequency = 10m\ndef short = 50ms\n\n|win_time()\n.period(1h)\n.every(30m)\n</code></pre>"},{"location":"dfs_script_language/index.html#text-templates","title":"Text templates","text":"<p>Embedding literal values in a string, using double curly braces:</p> <pre><code>{{ variable_name }}\n</code></pre>"},{"location":"dfs_script_language/index.html#use","title":"Use","text":"<pre><code>def this_portion = 'it'\ndef text_template = 'Some string/text where {{this_portion}} will get replaced'\n</code></pre> <p>In the above example, after compilation of the dfs script the variable <code>text_template</code> will hold the following value:</p> <p><code>Some string/text where it will get replaced</code></p> <p>Text templates can be used in variable declarations like in the above example,  they can be used in node-parameter and option-parameter calls.</p> <p>When used in template scripts string/text templates can be very powerful. The variable <code>this_portion</code> could be overwritten with a new value for every instantiation of a template script.</p>"},{"location":"dfs_script_language/index.html#more-examples","title":"More examples","text":"<pre><code>def an_integer = 33\ndef a_float = 345.78\ndef a_string = 'Embedding an integer: {{an_integer}} and a floating point number: {{a_float}}.'\n% results in: 'Embedding an integer: 33 and a floating point number: 345.78.'\n\n%% list:\ndef fruits =\n'[\n  {\"color\":  \"orange\", \"name\": \"orange\", \"peel\": true}, \n  {\"color\":  \"orange\", \"name\": \"mandarin\", \"peel\": true}, \n  {\"color\":  \"orange\", \"name\": \"peach\", \"peel\": false},\n  {\"color\":  \"orange\", \"name\": \"navel-orange\", \"peel\": true},\n  {\"color\":  \"yellow\", \"name\": \"lemon\", \"peel\": true}\n]'\ndef selected_fruits = e: select('name', [{'peel', true}], fruits)\ndef citric_fruits = 'citrus fruits are: {{selected_fruits}}.'\n% results in: 'citrus fruits are: orange,mandarin,navel-orange,lemon.'\n</code></pre> <p>There is another version of text-templating which uses a value inside the current data_point, that can be used with some nodes in faxe:</p> <p>{% raw %}</p> <pre><code>{{\"field_name\"}}\n</code></pre> <pre><code>|email()\n.body('\n    No data since {{\"datetime\"}} on topic ''ttgw/energy'', last value was {{\"val\"}}. \n    ')\n</code></pre> <p>Note: We use double quotes to reference a field in the current data_item.</p> <p>Here the values for <code>datetime</code> and <code>val</code> will be taken from the current data_point in the email node.</p> <p>If a field used in a text_template is not present in the current data_point, the string 'undefined' will be used.</p>"},{"location":"dfs_script_language/built-in_functions.html","title":"Built in functions","text":"<p>A complete list of currently supported functions.</p> <p>These function can be used in lambda and script expressions</p>"},{"location":"dfs_script_language/built-in_functions.html#type-conversion","title":"Type conversion","text":"<p>With a few exceptions every type can be converted to every other type.</p> <p>Bool</p> <pre><code>bool(a_value) -&gt; true|false\n</code></pre> <p>Integer</p> <pre><code>int(value) -&gt; integer\n</code></pre> <p>If value is a float, then int(value) is equivalent to  <code>trunc(value)</code></p> <p>Float</p> <pre><code>float(value) -&gt; float\n</code></pre> <p>String</p> <pre><code>string(val) -&gt; string\n</code></pre> <p>If val is a list:</p> <p>String</p> <pre><code>string(list) -&gt; string\n</code></pre> <p>Converting a list into a string with <code>string</code> is equivalent to</p> <pre><code>list_join(',', list_of_strings(List))\n</code></pre> <p>Converts every entry of List to a string first and then joins the resulting list with a comma.</p> <pre><code>[1,2,3] =&gt; ['1', '2', '3'] =&gt; '1,2,3'\n</code></pre>"},{"location":"dfs_script_language/built-in_functions.html#type-checks","title":"Type checks","text":"Function <code>is_string(Data)</code> -&gt; bool <code>is_integer(Data)</code> -&gt; bool <code>is_int(Data)</code> -&gt; bool <code>is_float(Data)</code> -&gt; bool <code>is_number(Data)</code> -&gt; bool <code>is_list(Data)</code> -&gt; bool <code>is_duration(Data)</code> -&gt; bool <code>is_boolean(Data)</code> -&gt; bool <code>is_bool(Data)</code> -&gt; bool"},{"location":"dfs_script_language/built-in_functions.html#time-functions","title":"Time functions","text":"<p>Every data_point in faxe contains a field called <code>ts</code>, which holds a UTC timestamp in milliseconds.</p> Function Description <code>now()</code> -&gt; integer returns an utc timestamp in milliseconds <code>dt_parse(ts, formatstring)</code> -&gt; integer used to parse a datetime string to the internal format, see datetime-parsing for details <code>to_iso8601(ts)</code> -&gt; string converts the timestamp to an ISO8601 datetime string <code>to_rfc3339(ts)</code> -&gt; string converts the timestamp to an RFC3339 datetime string <code>from_duration(ts)</code> -&gt; integer converts a duration to it's millisecond equivalent <code>millisecond(ts)</code> -&gt; integer milliseconds within the second [0, 999] <code>second(ts)</code> -&gt; integer second within the minute [0, 59] <code>minute(ts)</code> -&gt; integer minute within the hour [0, 59] <code>hour(ts)</code> -&gt; integer hour within the day [0, 23] <code>day(ts)</code> -&gt; integer day within the month [1, 31] <code>day_of_week(ts)</code> -&gt; integer the weekday with week [1, 7] 1 is monday <code>week(ts)</code> -&gt; integer isoweek-number within year [1, 53] <code>month(ts)</code> -&gt; integer month within the year [1, 12] <code>time_align(ts, duration)</code> -&gt; integer align the given timestamp to duration <p>Examples:</p> <pre><code>lambda: hour(\"ts\") &gt;= 8 AND hour(\"ts\") &lt; 19\n</code></pre> <p>The above expression evaluates to true if the hour of the day for the data point falls between 08:00 and 19:00.</p> <pre><code>lambda: time_align(\"ts\", 3m)\n</code></pre> <p>Will align every timestamp (ts) to a multiple of the 3 minutes within an hour (ie: 00:00, 00:03, 00:06, ...)</p>"},{"location":"dfs_script_language/built-in_functions.html#math-functions","title":"Math functions","text":"Function Description <code>abs(x)</code> -&gt; number <code>acos(x)</code> -&gt; float <code>acosh(x)</code> -&gt; float <code>asin(x)</code> -&gt; float <code>asinh(x)</code> -&gt; float <code>atan(x)</code> -&gt; float <code>atan2(y, x)</code> -&gt; float <code>atanh(x)</code> -&gt; float <code>ceil(x)</code> -&gt; float <code>cos(x)</code> -&gt; float <code>cosh(x)</code> -&gt; float <code>exp(x)</code> -&gt; float <code>floor(x)</code> -&gt; float <code>fmod(x, y)</code> -&gt; float <code>log(x)</code> -&gt; float <code>log10(x)</code> -&gt; float <code>log2(x)</code> -&gt; float <code>max(x, y)</code> -&gt; number <code>max(list)</code> -&gt; number <code>min(x, y)</code> -&gt; number <code>min(list)</code> -&gt; number <code>pi() -&gt; float</code> gives pi <code>pow(x, y)</code> -&gt; float <code>round(x)</code> -&gt; integer round a number to an integer <code>round_float(x, precision)</code> -&gt; float round a float (x) with the given precision <code>trunc(x)</code> -&gt; integer Returns an integer by truncating a number <code>sin(x)</code> -&gt; float <code>sinh(x)</code> -&gt; float <code>sqrt(x)</code> -&gt; float <code>tan(x)</code> -&gt; float <code>tanh(x)</code> -&gt; float"},{"location":"dfs_script_language/built-in_functions.html#string-functions","title":"String functions","text":"<p>String positions start with index 0.</p> Function Description <code>str_at(x, pos)</code> -&gt; string/undefined Returns the grapheme in the position of the given utf8 string. If position is greater than string length, then it returns undefined. Negative offsets count back from the end of the string. <code>str_capitalize(x)</code> -&gt; string Converts the first character in the given string to uppercase and the remaining to lowercase <code>str_concat(string1, string2)</code> -&gt; string Concatenate 2 strings. <code>str_concat([string1, string2, ...])</code> -&gt; string Concatenate a list of strings. <code>str_contains(x, contents)</code> -&gt; bool Check if string contains any of the given contents <code>str_downcase(x)</code> -&gt; string Convert all characters on the given string to lowercase <code>str_enclose(Wrapper, StringOrList)</code> -&gt; string or list Prepend and append Wrapper to a string or each entry of a list of strings <code>str_ends_with(x, suffix)</code> -&gt; string Returns true if string ends with suffix, otherwise false. <code>str_ends_with_any(x, suffixes)</code> -&gt; string Returns true if string ends with any of the suffixes given, otherwise false. <code>str_eqi(x,y)</code> -&gt; bool Compares strings case insensitively <code>str_find(string, pattern)</code> -&gt; string/undefined Removes anything before <code>pattern</code> in <code>string</code> and returns the remainder of the string or undefined if pattern is not found. Same as str_find(string, pattern, 'leading'). <code>str_find(string, pattern, dir)</code> -&gt; string/undefined Like str_find/2, but <code>dir</code>, which can be 'leading' or 'trailing', indicates from which direction characters are to be searched. <code>str_first(x)</code> -&gt; string/undefined Returns the first grapheme from an utf8 string, undefined if the string is empty <code>str_last(x)</code> -&gt; string/undefined Returns the last grapheme from an utf8 string, undefined if the string is empty <code>str_length(x)</code> -&gt; int Returns the number of unicode graphemes in an utf8 string <code>str_lstrip(x)</code> -&gt; string Returns a string where leading Unicode whitespace has been removed <code>str_lstrip(string, char::int)</code> -&gt; string Returns a string where leading char have been removed ('char' number of leading chars) str_pad_leading/2 str_pad_leading/3 str_pad_trailing/2 str_pad_trailing/3 <code>str_quote(string)</code> -&gt; string Enclose a string with double quotes. <code>str_replace(x, patt, repl)</code> -&gt; string Returns a new string based on subject by replacing the parts matching pattern by replacement. Both <code>pattern</code> and <code>replace</code> may be of type string or list. str_replace_leading/3 Replaces all leading occurrences of match by replacement of match in string. str_replace_trailing/3 Replaces all trailing occurrences of match by replacement of match in string. <code>str_replace_prefix(x, match, repl)</code> -&gt; string Replaces prefix in string by replacement if it matches match. Returns the string untouched if there is no match. If match is an empty string (\"\"), replacement is just prepended to string. <code>str_replace_suffix(x, match, repl)</code> -&gt; string Replaces suffix in string by replacement if it matches match. Returns the string untouched if there is no match. If match is an empty string (\"\"), replacement is just appended to string. <code>str_reverse(x)</code> -&gt; string Reverses the given string. <code>str_rstrip(x)</code> -&gt; string Returns a string where trailing Unicode whitespace has been removed <code>str_rstrip(x, char)</code> -&gt; string Returns a string where trailing char have been removed <code>str_slice(x, start, len)</code> -&gt; string Returns a substring starting at the offset given by the first, and a length given by the second param, if offset is negative, count back from end of string. <code>str_slice(x, start)</code> -&gt; string Returns a substring starting at the offset given by start. str_split/1 str_split/2 str_split/3 str_split_at/2 str_split_by_any/2 str_split_by_any/3 str_split_by_re/2 str_split_by_re/3 <code>str_starts_with(x, pre)</code> -&gt; bool Returns true if string starts with Prefix <code>str_starts_with_any(x, prefixes)</code> -&gt; bool Returns true if string starts with any of the prefixes given, otherwise false. <code>str_strip(x)</code> -&gt; string Returns a string where leading/trailing Unicode whitespace has been removed <code>str_strip(x, char)</code> -&gt; string Returns a string where leading/trailing char have been removed <code>str_upcase(x)</code> -&gt; string Convert all characters on the given string to uppercase <code>str_repeat_replace(string, pattern, replaceList)</code> -&gt; list Given a string, a pattern and a list of strings, produces a new list of strings by <code>duplicating</code> string <code>length(replaceList)</code> times, while replacing every occurance of <code>pattern</code> with the corresponding entry from <code>replaceList</code>.  If <code>pattern</code> is not found in <code>string</code>, it will be left untouched and simply duplicated."},{"location":"dfs_script_language/built-in_functions.html#lists","title":"Lists","text":"Function Description Example <code>head(List)</code> -&gt; any Returns the head, or the first element, of list List. <code>head(\"data.lines\")</code> <code>nth(Integer, List)</code> -&gt; any Returns the nth element from list List, list indices start with 1. <code>nth(N, \"data.lines\")</code> <code>nthtail(Integer, List)</code> -&gt; any Returns the Nth tail of List, that is, the sublist of List starting at N+1 and continuing up to the end of the list. <code>nthtail(N, \"data.lines\")</code> <code>list_append(List1, List2)</code> -&gt; list Returns a new list, which is made from the elements of List1 followed by the elements of List2. <code>list_append(['a', 'b', 'c'], [1, 2, 4])</code> <code>list_delete(Elem, List)</code> -&gt; list Returns a copy of List, where the first element matching Elem is deleted, if there is such an element. <code>list_delete(2, [1, 2, 4]) -&gt; [1, 4]</code> <code>list_last(List)</code> -&gt; any Returns the last element from List. <code>list_max(List)</code> -&gt; any Returns the first element of List that compares greater than or equal to all other elements of List. <code>list_max([1, 2, 4]) -&gt; 4</code> <code>list_min(List)</code> -&gt; any Returns the first element of List that compares less than or equal to all other elements of List. <code>list_min([1, 2, 4]) -&gt; 1</code> <code>list_reverse(List)</code> -&gt; list Returns a list with the elements in List1 in reverse order. <code>list_reverse([1, 2, 4]) -&gt; [4, 2, 1]</code> <code>list_sort(List)</code> -&gt; list Returns a list containing the sorted elements of List1. <code>list_sort([4, 5, 1, 2, 4]) -&gt; [1, 2, 4, 4, 5]</code> <code>list_subtract(List1, List2)</code> -&gt; list Returns a new list List3 that is a copy of List1, subjected to the following procedure:for each element in List2, its first occurrence in List1 is deleted. <code>list_sum(List)</code> -&gt; number Returns the sum of the elements in List. <code>list_unique(List)</code> -&gt; list Returns a new list with only unique values from List. <code>list_unique(\"data.lines\")</code> <code>list_usort(List)</code> -&gt; list Returns a list containing the sorted elements of List1 where all except the first element of the elements comparing equal have been deleted. <code>list_join(List)</code> -&gt; string Equivalent to list_join(',', List). <code>list_join(\"data.lines\")</code> <code>list_join(Separator, List)</code> -&gt; string Join a list of strings with a separator string. <code>list_join('-', \"data.lines\")</code> <code>list_of_strings(List)</code> -&gt; list Convert every entry of a list to a string and return the list of strings. <code>list_of_strings(\"data.lines\")</code> <code>list_string_prefix(List, Prefix)</code> -&gt; list Prefix the elements of a list of strings with a given string. <code>list_string_prefix(['v1', 'v2'], 'mod_') -&gt; ['mod_v1', 'mod_v2']</code> <code>list_string_postfix(List, Postfix)</code> -&gt; list Postfix the elements of a list of strings with a given string. <code>list_string_postfix(['v1', 'v2'], '_dfe') -&gt; ['v1_dfe', 'v2_dfe']</code> <code>list_string_concat(List1, List2)</code> -&gt; list Concatenate the string representation of the elements of 2 lists, the resulting list is as long as the shorter of the two lists. <code>list_string_concat(['v1', 'v2'], ['k1', 'k2']) -&gt; ['v1k1', 'v2k2']</code> <code>list_string_concat(List1, List2, Sep)</code> -&gt; list Concatenate the string representation of the elements of 2 lists, the resulting list is as long as the shorter of the two lists. The given separator string will be place in between each of the pairs of strings. <code>sublist(List, Len))</code> -&gt; list Returns the sublist of List1 starting at position 1 and with (maximum) Len elements.  It is not an error for Len to exceed the length of the list, in that case the whole list is returned. <code>sublist(List, Start, Len))</code> -&gt; list Returns the sublist of List1 starting at Start and with (maximum) Len elements. It is not an error for Start+Len to exceed the length of the list."},{"location":"dfs_script_language/built-in_functions.html#lists-and-maps-jsn","title":"Lists and Maps (jsn)","text":"<p>Functions marked with a star (*), can work directly on json strings representing lists or objects (maps in faxe). For example, you can feed a json object string to <code>map_get</code> to get a value out of the json object: </p> <pre><code>def json_map = '{\"key1\": \"value1\", \"key2\": \"value2\"}'\n\ndef key1_val = e:map_get('key1', json_map)\n%% 'value1'\n</code></pre> <p>To use a json string in other functions listed here, you have to convert them to faxe's internal format first, using <code>from_json_string</code>:</p> <pre><code>def json_map = '{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}'\n%% first convert to faxe's internal format\ndef internal_map = e: from_json_string(json_map)\ndef has_key3 = e:member('key3', internal_map)\n%% true   \n</code></pre> Function Description Example *<code>from_json_string(String)</code> -&gt; faxe map or list Convert a valid json string into faxe's internal data-structure. from_json_string('{\"a\":\"b\"}') <code>to_json_string(String)</code> -&gt; string Convert faxe's internal data-structure into a json string. to_json_string(\"data.struct\") <code>size(ListOrMap)</code> -&gt; integer Get the number of entries in a list or map. size(\"data.lines\") <code>member(Ele, ListOrMap)</code> -&gt; bool Check for list/set membership of a value, or when used with a map, check if Ele is a key in the map. <code>not_member(Ele, ListOrMap)</code> -&gt; bool *<code>map_get(Key, Map)</code> -&gt; any Get a value from a map, equivalent to <code>map_get(Key, Map, 'undefined')</code>. map_get(\"topic\", '{\"data/temp/val1\": \"temp\"}') *<code>map_get(Key, Map, Default)</code> -&gt; any Get a value from a map, <code>Default</code> is returned, if the key is not present in map. map_get(\"topic\", '{\"data/temp/val1\": \"temp\"}', 'no_match') *<code>select(Key, Jsn)</code> -&gt; list Select every value with the path <code>Key</code> from a json-array. See description below. *<code>select(Key, Where, Jsn)</code> -&gt; list Select every value with the path <code>Key</code> and conditions <code>Where</code> from a json-array. Elements that do not meet the given conditions are filtered out. See description below. *<code>select_all(Key, Where, Jsn)</code> -&gt; list Select every value with the path <code>Key</code> and conditions <code>Where</code> from a json-array. The list of <code>Where</code> conditions are <code>OR</code>ed in this case, so there is a select operation for every entry in <code>Where</code> See description below. *<code>select_first(Key, Jsn)</code> -&gt; any Select the first value with the path <code>Key</code> from a json-array. See description below. *<code>select_first(Key, Where, Jsn)</code> -&gt; any Select the first value with the path <code>Key</code> and conditions <code>Where</code> from a json-array. See description below. *<code>select_any(Key, Where, Jsn)</code> -&gt; any Select the first value with the path <code>Key</code> that is found with any of the <code>Where</code> conditions from a json-array. See description below."},{"location":"dfs_script_language/built-in_functions.html#select-select_first-select_any","title":"select, select_first, select_any","text":"<p>With these functions we can select values from a json-array like datastructure, meaning an array of - possibly nested - json objects.</p> <pre><code>def fruits =\n'[\n  {\"color\":  \"orange\", \"name\": \"orange\", \"peel\":  true}, \n  {\"color\":  \"orange\", \"name\": \"mandarin\", \"peel\":  true}, \n  {\"color\":  \"orange\", \"name\": \"peach\", \"peel\":  false},\n  {\"color\":  \"orange\", \"name\": \"navel-orange\", \"peel\":  true},\n  {\"color\":  \"yellow\", \"name\": \"lemon\", \"peel\":  true}\n]'\n</code></pre> <p>The Where parameter in select functions is a list of tuples.</p> <pre><code>e: select('name', [{'peel', true}], fruits)\n\ne: select_first('name', [{'color', 'orange'}, {'peel', false}], fruits)\n</code></pre> <p>The first example above would select the names of all fruits, that have a 'peel' value of true:  <code>['orange', 'mandarin', 'navel-orange', 'lemon']</code>. The second example selects the first name with 'color' <code>orange</code> and the 'peel' value <code>false</code>: <code>'peach'</code>.</p> <p>Besides an exact match in the Where parameters, we can also use a regular expression:</p> <pre><code>e: select('name', [{'regex', 'name', 'orange$'}], fruits)\n</code></pre> <p>Results in <code>['orange', 'navel-orange']</code>.</p>"},{"location":"dfs_script_language/built-in_functions.html#misc","title":"Misc","text":"Function Description Example <code>defined(Key::string)</code> -&gt; bool whether the given Key is defined in the current data-item <code>undefined(Key::string)</code> -&gt; bool whether the given Key is NOT defined in the current data-item <code>empty(Key::string)</code> -&gt; bool whether the given Key points to an empty or not defined value, empty means an empty string or an empty list or tuple empty(\"data.topic\") -&gt; false <code>topic_part(TopicString, PartIndex, [Seperator])</code> -&gt; string extract a part from a topic string, Separator defaults to '/', the index of the first part is <code>1</code> not 0 topic_part('this/is/mytopic', 2) -&gt; 'is' <code>random(N::int)</code> -&gt; integer generate a random integer uniformly distributed between 1 and N, N &gt; 0 <code>random(N::int, M::int)</code> -&gt; integer generate a random integer uniformly distributed between N and M, N &gt; 0 and M &gt; N <code>random_real(N::int)</code> -&gt; float generate a random float between 0.0 and 1.0, that gets multiplied by N, N &gt; 0 <code>random_normal()</code> -&gt; float generate a standard normal deviate float (that is, the mean is 0 and the standard deviation is 1) <code>random_latin_string(Length)</code> -&gt; string Returns a random string of size Length, consisting of latins <code>[a-zA-Z]</code> and digits <code>[0-9]</code>. <code>base64_encode(String)</code> -&gt; string Encode a string to base64 format. <code>base64_decode(String)</code> -&gt; string Decode a base64 encoded string. <code>crc32(String)</code> -&gt; string Computes the crc32 (IEEE 802.3 style) checksum for the given string. <code>phash(Any)</code> -&gt; integer Portable hash function, that outputs an integer in the range 0..2^27-1 <code>uuidv4()</code> -&gt; string Get a strong v4 UUID string (using crypto/openssl) <code>env(Key::string)</code> -&gt; string | false Get the value from an OS-ENV variable, if not found, returns <code>false</code>. Note that the value returned (except false) is always as string. env('FAXE_MQTT_HOST') -&gt; '127.0.0.1' <code>env(Key::string, Default)</code> -&gt; string | any Get the value from an OS-ENV variable, if not found, returns <code>Default</code>. Note that the value returned (except Default) is always as string. env('FAXE_MQTT_HOS', '127.0.0.1') -&gt; '127.0.0.1' <code>envs()</code> -&gt; map Get a map of all OS-ENV variables. All keys and values are strings. <code>mem</code> values are set with the mem node <code>ls_mem(Key)</code> -&gt; any get the single value associated with Key from the flow-memory <code>ls_mem_list(Key)</code> -&gt; any get the list value associated with Key from the flow-memory <code>ls_mem_set(Key)</code> -&gt; any get the set value associated with Key from the flow-memory <code>build_plc_addresses(Jsn)</code> -&gt; list Given jsn list with PLC address information, builds address strings, that can be used for the s7read node <code>build_plc_addresses(Jsn, FieldName::string)</code> -&gt; list Injects the results of building the addresses back into the given JsnList and returns the updated List"},{"location":"dfs_script_language/built-in_functions.html#build_plc_addressesjsn-list","title":"build_plc_addresses(Jsn) -&gt; list","text":"<p>Jsn is a list of objects containing plc address information in one of two formats:</p> <ul> <li><code>{\"bit\": 2, \"byte\": 7, \"db\": 2077.0, \"offset\": 324}</code> results in a BOOL address: <code>DB2077.DBX342.2</code> (byte-1 + 12) <li><code>{\"offset\": 36.0, \"db\": 11245.0, \"var_type\": \"DINT\" [, \"var_len\":1]}</code> results in <code>DB11245.DBDINT36</code> (<code>var_len</code> is only used, when <code>var_type</code> is \"STRING\")</li> var_type BOOL UINT UDINT WORD DWORD TIME CHAR BYTE REAL DATETIME STRING needs <code>var_len</code> <p>If the function sees any other format or a field missing, it will fail.</p>"},{"location":"dfs_script_language/built-in_functions.html#conditional-functions","title":"Conditional functions","text":""},{"location":"dfs_script_language/built-in_functions.html#special-if-function","title":"Special if function","text":"<p>If</p> <p>In DFS <code>if</code> is not a language construct, but a function with 3 parameters. The if function\u2019s return type is the same type as its second and third arguments.</p> <pre><code>if(condition, true expression, false expression)\n</code></pre> <p>Returns the result of its operands depending on the value of the first argument.</p> <p>Examples:</p> <pre><code>|eval(lambda: if(\"field.val1\" &gt; threshold AND \"field.val1\" != 0, 'true', 'false'))\n.as('value')\n</code></pre> <p>The value of the field <code>value</code> in the above example will be the string <code>true</code> or <code>false</code>, depending on the condition passed as the first argument.</p> <pre><code>|eval(lambda: if(is_float(\"data.duration_ms\"), trunc(\"data.duration_ms\" * 1000), \"data.duration_ms\"))\n.as('value')\n</code></pre> <p>Both expressions (2nd and 3rd parameter) may also be arbitrarily complex. In this example, if the condition returns true, <code>data.duration_ms</code> will be mulitplied be 1000 and then truncated to an integer. If the condition returns false, just the value of <code>data.duration_ms</code> will be returned.</p>"},{"location":"dfs_script_language/lambda_expressions.html","title":"Lambda expressions","text":"<p>In version 1.2.0, lambda expressions have been completely rewritten and are now fully compiled erlang code and thus much faster than before.</p>"},{"location":"dfs_script_language/lambda_expressions.html#overview","title":"Overview","text":"<p>DFS uses lambda expressions to define transformations on data points as well as define Boolean conditions that act as filters.  Lambda expressions wrap mathematical operations, Boolean operations, internal function calls or a combination of all three.  </p> <p>All lambda expressions in DFS begin with the <code>lambda:</code> keyword.</p> <pre><code>|where(lambda: \"topic\" == 'ttop/grap/prec')\n</code></pre> <p>In the above example <code>\"topic\"</code> is used to access the value of a field called <code>topic</code>  from the current data_point and compared against the string <code>'ttop/grap/prec'</code>.</p> <p>Note here that literal string values are declared using single quotes,  while double quotes are used to access the values of tags and fields within the current data_item.</p>"},{"location":"dfs_script_language/lambda_expressions.html#field-paths","title":"Field paths","text":"<p>As field and tag values can be deeply nested maps and lists, it is possible to use a <code>JSON-path</code> like syntax to reference them:</p> <p>Valid examples:</p> <pre><code>\"averages\"\n\n\"axis.z.cur\"\n\n\"value.sub[2].data\"\n\n\"averages.emitted[5]\"\n</code></pre>"},{"location":"dfs_script_language/lambda_expressions.html#built-in-functions","title":"Built-in functions","text":"<p>Faxe features a lot of built-in functions, that can be used in lambda and script expressions.</p> <p>See here for a full list of them.</p>"},{"location":"dfs_script_language/lambda_expressions.html#special-if-function","title":"Special if function","text":"<p>If</p> <p>In DFS <code>if</code> is not a language construct, but a function with 3 parameters. The if function\u2019s return type is the same type as its second and third arguments.</p> <pre><code>if(condition, true expression, false expression)\n</code></pre> <p>Returns the result of its operands depending on the value of the first argument.  </p> <p>Examples:</p> <pre><code>|eval(lambda: if(\"field.val1\" &gt; threshold AND \"field.val1\" != 0, 'true', 'false'))\n.as('value')\n</code></pre> <p>The value of the field <code>value</code> in the above example will be the string <code>true</code> or <code>false</code>,  depending on the condition passed as the first argument.</p> <pre><code>|eval(lambda: if(is_float(\"data.duration_ms\"), trunc(\"data.duration_ms\" * 1000), \"data.duration_ms\"))\n.as('value')\n</code></pre> <p>Both expressions (2nd and 3rd parameter) may also be arbitrarily complex. In this example, if the condition returns true, <code>data.duration_ms</code> will be mulitplied be 1000 and then truncated to an integer. If the condition returns false, just the value of <code>data.duration_ms</code> will be returned.</p>"},{"location":"dfs_script_language/macros.html","title":"Macros","text":"<p>Macros are a way to reuse often used parts and to build more complex scripts more easy.</p> <p>Every task that is registered with <code>faxe</code> can be used as a macro-script.</p> <p>Note: Macros are implemented on the script-level, so faxe's internal engine does not know anything about macros.</p> <pre><code>%%% MACRO called 'multiply_above_threshold'\ndef threshold = 30\ndef factor = 2\n|where(lambda: \"value\" &gt; threshold)\n|eval(lambda: \"value\" * factor)\n.as('multiple')\n</code></pre> <p>Once the above dfs script is registered with faxe, we can use it as a macro:</p> <pre><code>|value_emitter()\n.every(500ms) \n\n||multiply_above_threshold()\n.threshold(2.7)\n</code></pre> <p>A macro is referenced with a double pipe symbol <code>||</code>, followed by the name of the task which we want to use as macro.</p> <p>The resulting script will look like this:</p> <pre><code>|value_emitter()\n.every(500ms) \n\ndef threshold = 2.7\ndef factor = 2\n|where(lambda: \"value\" &gt; threshold)\n|eval(lambda: \"value\" * factor)\n.as('multiple')\n</code></pre> <p>We can override every literal declaration within the macro-script by simply using them as node-parameters. Ie: here the declaration 'threshold' is overriden, we could also override 'factor'.</p> <p>Theoretically any number of macro-references <code>||dfs_script_name</code> can be used in a single dfs-script. Furthermore every macro used in a script can itself reference any number of macros, as every macro-script is an ordinary dfs-script registered in <code>faxe</code>.</p>"},{"location":"dfs_script_language/node_connection.html","title":"Node connection","text":"<p>Nodes are normally connected by their occurence in the dfs script.</p> <pre><code>|node1()\n|node2()\n</code></pre> <p>Node1 is connected to node2.</p> <p>The above could also be written as:</p> <pre><code>def n1 = |node1()\n\nn1\n|node2()\n</code></pre> <p>... which results in the same computing flow.</p> <p>Here we see that we can actively manipulate the connections in a flow by binding a node to a declaration with the <code>def</code> keyword. Whole chains of nodes ie: sub-graphs can be bound to a variable. This is called a <code>chain-declaration</code>.</p> <pre><code>def chain1 = \n|node1()\n|node1_1()\n|node1_2\n\nchain1\n|node2()\n</code></pre> <p>Now node2 is connected to node1_2</p> <p>With the above example we can connect another node to chain1:</p> <pre><code>def chain1 = \n|node1()\n|node1_1()\n|node1_2\n\nchain1\n|node2()\n\nchain1\n|node3()\n</code></pre> <p>Here both nodes node2 and node3 are connected to node1_2.</p>"},{"location":"dfs_script_language/node_connection.html#note","title":"Note:","text":"<p>Every use of the <code>def</code> keyword interrupts the auto chaining of nodes, ie:</p> <pre><code>|node1()\n|node2()\ndef n3 = |node3()\n|node4()\n</code></pre> <p>In the above example, node3 and node4 are not connected to node2, as a consequence of using the <code>def</code> keyword. Instead we have 2 chains in this flow: 1. Node1 connected to node2 and 2. node3 connected node4.</p> <p>If we'd like to union these 2 node chains:</p> <pre><code>def in1 =\n|node1()\n|node2()\n\ndef in2 = \n|node3()\n|node4()\n\nin1\n|union(in2)\n</code></pre> <p>There are several node-types in faxe that deal with more than one input node, for example the combine node. Here the use of <code>chain-declarations</code> is necessary:</p> <pre><code>def s1 =\n|node1()\n|node1_1\n\ndef s2 =\n|node2()\n|node2_1()\n\ns1|combine(s2)\n</code></pre>"},{"location":"dfs_script_language/script_expressions.html","title":"Script expressions","text":"<p>Script expressions look the same as lambda expressions, but start with <code>e:</code> instead of <code>lambda:</code>.</p> <p>There is a main difference between script expressions and lambda expressions:</p> <p>Script expressions are evaluated during dfs compilation.</p> <p>Therefore, script expressions can not use \"references\" to access fields in data-items.</p>"},{"location":"dfs_script_language/script_expressions.html#example","title":"Example","text":"<pre><code>def topic = 'this/is/my/topic'\n%% script expression, which uses the topic definition\ndef routing_key = e: str_replace(topic, '/', '.')\n</code></pre> <p>The above example will resolve to the following equivalent dfs script:</p> <pre><code>def topic = 'this/is/my/topic'\n%% script expression, which uses the topic definition\ndef routing_key = 'this.is.my.topic'\n</code></pre>"},{"location":"dfs_script_language/script_expressions.html#references","title":"References","text":"<p>References are used in lambda expression to access fields in data-items (data_points and data_batches). They cannot be used in script expressions, so the next example will throw an error during script compilation:</p> <pre><code>def base_topic = 'this/is/my/base/'\ndef topic = e: str_concat(base_topic, \"data.postfix\")\n</code></pre> <p>The above example will fail with the message: <code>\"Reference(s) used in inline-expression: data.postfix\"</code></p> <p>Note: We use double quotes to reference a field in the current data_item.</p>"},{"location":"dfs_script_language/script_expressions.html#more-examples","title":"More examples","text":"<pre><code>def new_id = 3\ndef def_inline = e: string(3 + 5 + sqrt(new_id))\ndef i_string = 'this is my string'\ndef def_inline_string = e: str_replace(i_string, 'i', 'a')\n</code></pre> <pre><code>def batch_size = 50 \n\n|amqp_consume()\n.bindings('this.is.my.binding.#')\n%% use of script expression for node option, \n%% we have to round to an integer with the `round` function\n.prefetch(e: round(batch_size * 1.25) )\n.exchange('x_xchange')\n.queue('q_test')\n\n|batch(batch_size)\n.timeout(10s)\n</code></pre> <p>Script expression can be used for node parameters (except where a lambda expression is required of course),  as long as the resulting value is of the required data-type.</p>"},{"location":"dfs_script_language/tips_and_best_pratices.html","title":"DFS Script Language tips and best pratices","text":""},{"location":"dfs_script_language/tips_and_best_pratices.html#variables","title":"Variables","text":"<p>Ok, they are unmutable really, so we call them declarations.</p> <pre><code>def stream_id = 'ee29e1f8552e4a2561329fec59688e60'\n</code></pre> <p>When using declarations in DFS script you should stick to one type of naming schema. Be it CamelCase naming or all lowercase with underscores like in the example above.</p>"},{"location":"dfs_script_language/tips_and_best_pratices.html#lambda-expressions","title":"Lambda expressions","text":""},{"location":"dfs_script_language/tips_and_best_pratices.html#_1","title":"DFS Script Language tips and best pratices","text":""},{"location":"nodes/index.html","title":"Faxe nodes","text":""},{"location":"nodes/index.html#parameters","title":"Parameters","text":"<p>Faxe nodes can have <code>2 types of parameters</code>:</p> <ol> <li>Node parameters provided to the node declaration function</li> </ol> <pre><code>% the level parameter is given to the node declaration function\n|debug('notice')\n</code></pre> <ol> <li>Option parameters provided to an option call</li> </ol> <pre><code>% the level parameter is given as an extra option function\n|debug()\n.level('notice')\n</code></pre> <p>Some parameters are required and others are optional. In the nodes documentation parameters/options are listed in a table like this:</p> Parameter Description Default parameter1( <code>string</code> ) description for parameter1 'default string' parameter2( <code>integer</code> ) description for parameter2 config: <code>example.host</code>/<code>FAXE_EXAMPLE_HOST</code> parameter3( <code>string</code> ) description for parameter3 <p><code>parameter1</code> has a default value and is optional in a DFS script. </p> <p><code>parameter2</code> also has a default value, but this value comes from faxe's configuration.</p> <p>There are 2 ways to configure this parameter (which is global for all nodes using it) :</p> <ol> <li>in a config file</li> <li>with environment variables</li> </ol> <p>Therefor the pattern in the <code>Default</code> column of the list is: <code>config: {config-file setting}/{ENV variable name}</code></p> <p><code>parameter3</code> has no default value and therefor must be given in a DFS script.</p> <p>Every parameter with no default value is mandatory !</p> <p>The following is a list of all possible parameter types faxe supports based on the basic data-types:</p> Name Description Example is_set Special parameter type that evaluates to true if called (even with no value) .use_ssl() number Integer or float value 324 or 4.3424325 integer Integer value float Floating point value double Floating point value string String value .topic('home/alex/garage') binary atom used internally only list any kind of list lambda a lambda expression bool number_list a list of numbers .values(3, 44, 34.5) integer_list a list of integers .ints(2, 3, 4, 5) float_list a list of floats .floats(43.4, 12.2, 545.009832) string_list a list of strings .strings('alex1', 'alex2', 'flo', 'markus') binary_list atom_list internally only lambda_list a list of lambda expressions .functions(lambda: \"val\" * 2, lambda: \"val\" * 3, lambda: \"val\" / 4) <p>What is important to note: If a node requires a <code>_list</code> type for any parameter,  we just provide 1 or more of the same data-type separated be commas.</p> <p>For example the eval node requires the lambdas parameter to be of type <code>lambda_list</code>, the following calls would be valid:</p> <pre><code>|eval(lambda: str_concat(\"strval\", postfix))\n\n|eval(lambda: str_starts_with(\"strval\", 'pre'), lambda: 3 * (\"val1\" + \"val2\"))\n\n|eval(\n    lambda: sqrt(\"base\") + const,\n    lambda: if(hour(\"ts\") &gt; 18 AND day_of_week(\"ts\") &lt; 6, 'late_for_work', 'ok'),\n    lambda: abs(\"ts\" - \"ts_previous\")\n)\n</code></pre>"},{"location":"nodes/aggregate.html","title":"The aggregate node","text":"<p>The aggregate node lets you compute statistical functions on data_batches. This node can compute multiple statistic functions on multiple fields.</p> <p>See nodes under /nodes/statistics for details (Not all of these are available here).</p> <p>This node can only take <code>data_batch</code> items,  so in most cases you will need some kind of batch or window node to collect batches.</p> <p>The aggregate node produces a new stream of data consisting of <code>data_point</code> items.</p>"},{"location":"nodes/aggregate.html#example","title":"Example","text":"<pre><code>|value_emitter()\n.every(1s)\n.type('point') \n\n|win_time()\n.every(15s)\n\n|aggregate()\n.fields('val', 'val', 'val', 'val')\n.functions('variance', 'sum', 'avg', 'count_distinct')\n.as('variance', 'sum', 'average', 'count_distinct')\n\n|debug()\n</code></pre>"},{"location":"nodes/aggregate.html#parameters","title":"Parameters","text":"Parameter Description Default fields( <code>string_list</code> ) names of the fields used for each computation functions( <code>string_list</code> ) list of function names (see 'Functions' table below for possible values) as( <code>string_list</code> ) names for the fields for output values defaults to the name of the compute function keep( <code>string_list</code> ) fields that should be kept from the input data-items [] keep_tail( <code>boolean</code> ) keep the last data-point from every incoming batch, so it can be used for <code>count_change</code> as inital value true"},{"location":"nodes/aggregate.html#functions","title":"Functions","text":"Name Description Note <code>sum</code> computes the sum of all values numerical values only <code>avg</code> computes the average of all values numerical values only <code>min</code> computes the minimum value <code>max</code> computes the maximum value <code>mean</code> computes the mean value numerical values only <code>median</code> computes the median value numerical values only <code>first</code> outputs the first (oldest) value <code>last</code> outputs the last (newest) value <code>count</code> outputs the total number of values <code>count_distinct</code> outputs the number of unique values <code>count_change</code> outputs the number of value changes for the given field with keep_tail set to true, the last item will serve as initial value <code>range</code> computes the range between minimum and maximum <code>variance</code> computes the variance numerical values only <code>stddev</code> computes the standard deviation of the values numerical values only <code>skew</code> computes the skew of all values numerical values only"},{"location":"nodes/array_explode.html","title":"The array_explode node","text":"<p>Given a data_point with one or more array fields, create and emit one new data_point for every entry in the array(s).</p> <p>If more than 1 array field is used, then the arrays must have to be of the same size, otherwise the mapping will fail. Any field not found in an incoming data-point, will just be ignored.</p> <p>For every point created out of the array(s) a time offset will be added to the previous point timestamp, starting with the timestamp of the original data_point for the first resulting data-point.</p>"},{"location":"nodes/array_explode.html#example","title":"Example","text":"<pre><code>|json_emitter(\n    '{\"motor\": {\"drive\": [1,2,3,4,5,6,7,8,9]}, \n    \"torque\": [6,7,8,9,1,2,3,4,5], \n    \"zip\": [4,5,6,7,8,9,1,2,3]}'\n    )\n\n|array_explode()\n.fields('motor.drive', 'zip')\n.as('data.ex_drive', 'data.ex_zip')\n</code></pre> <p>On every emit of the json_emitter node, the <code>array_explode</code> node will produce 10 (length of the arrays) new data-items with 2 values: <code>data.ex_drive</code> (with one value from motor.drive) and <code>data.ex_zip</code> (with a value from the zip field).</p>"},{"location":"nodes/array_explode.html#parameters","title":"Parameters","text":"Parameter Description Default fields( <code>string_list</code> ) names of the fields used for each computation as( <code>string_list</code> ) list of output field names according to <code>fields</code>, defaults to the values of <code>fields</code> undefined time_offset( <code>duration</code> ) 1s keep( <code>string_list</code> ) list of field-names, that should be kept from the original data-point undefined"},{"location":"nodes/collect.html","title":"The collect node","text":"<p>Since 0.15.2</p> <p>The collect node will maintain a <code>set</code> of data-points based on some criteria given as lambda-expressions. It will normally <code>output a data_batch</code> item regularily (when <code>emit_every</code> is given) or based on every incoming item.</p> <p>The internal collection is a key-value set with unqiue values for the keys, taken from the <code>key_fields</code>. [{Key, DataPoint}]</p> <p>Although it should be straight forward, the behaviour of this node can get quite complex and a bit hard to understand, depending on the options given.  If you just want to collect and hold a bunch of values for certain fields, take a look at  the collect_fields node, it may be better suited for your needs.</p>"},{"location":"nodes/collect.html#adding-updating-and-removing-a-value","title":"Adding, updating and removing a value","text":"<p>With every incoming data-item the node will first check, if there is already an item with the same key-field value in the collection. If this is not the case, the node will evaluate the <code>add</code> function, if given. Data_points that do not have (any of) the key-field(s) present, will be ignored.</p> <p>An item will be added to the collection if the key is new to the collection and if the <code>add</code>-function</p> <ul> <li>returns true</li> <li>is not given</li> </ul> <p>If there is already an item with the same value(s) for the key-field(s),  the node would check, if there is an <code>update</code> function and evaluate it and if no update happend,  it will try to evaluate the <code>remove</code> function.</p> <p>If an <code>update</code> happened, the node will skip evaluating the <code>remove</code> function.</p> <p>If no <code>remove</code> function is given, data-items will not be removed, but only evicted by the <code>max_age</code> and/or the <code>max_ts_age</code> option.</p>"},{"location":"nodes/collect.html#update-and-remove-expression","title":"Update and remove expression","text":"<p>In the <code>update</code> or <code>remove</code> lambda-expressions, the datapoint, that is already in the collection, can be used, for comparing for example. That is to say: When the update or remove function is evaluated, it gets injected the data-point for the same key-field value, that is found in the collection. There is a root-object used for the fields in this datapoint: <code>__state</code>.</p> <p>What this means is, that you can do something like this in the <code>update</code> expression (see also Example 3 below)</p> <pre><code>|collect()\n.key_fields('data.id') \n\n%% comparing the current value of 'val' to the value from the data-point currently in the collection\n%% here we use the value from the internal buffer that is found in the '__state' object.\n.update(lambda: \"data.val\" &gt; \"__state.data.val\") \n</code></pre>"},{"location":"nodes/collect.html#output","title":"Output","text":"<p>When no <code>emit_every</code> is given, the node will output data with every incoming data-item.</p> <p>With <code>emit_unchanged</code> set to false, output will only happen after processing a data-item that has changed the internal set.</p> <p><code>data_batch</code> items are processed as a whole first and then may trigger an emit operation.</p> <p><code>Since 0.19.4</code> : The output batch can be conflated into a single data_point item, when the <code>merge</code> option is set to true.</p> <p>Note: Produced data may become very large, if the value of <code>key_fields</code> is ever-changing, so that     the node will cache a lot of data and therefore may use a lot of memory, be aware of that !</p>"},{"location":"nodes/collect.html#example-1","title":"Example 1","text":"<pre><code>|collect()\n.key_field('data.code')\n.max_age(2m) \n</code></pre> <p>Collect by the field <code>data.code</code> keeping every data-item for 2 minutes. No <code>update</code> or <code>remove</code> will occur otherwise.</p>"},{"location":"nodes/collect.html#example-2","title":"Example 2","text":"<pre><code>|collect()\n.key_field('data.code')\n.update(lambda: \"data.mode\" == 1) \n.remove(lambda: str_length(\"data.message\") &gt; 7)\n</code></pre> <p>Collect by the field <code>data.code</code>, update an item when the <code>data.mode</code> field is 1.</p> <p>Items get removed, if they have a value for <code>data.message</code>, that is more than 7 chars long.</p>"},{"location":"nodes/collect.html#example-3","title":"Example 3","text":"<pre><code>%% input data -&gt;\n\n|json_emitter()\n .every(500ms)\n .json(\n    '{\"id\": 224, \"name\" : \"twotwofour\", \"val\": 12, \"mode\": 1}',\n    '{\"id\": 112, \"name\" : \"oneonetow\", \"val\": 11, \"mode\": 1}',\n    '{\"id\": 358, \"name\" : \"threefiveeigth\", \"val\": 7, \"mode\": 1}',\n    '{\"id\": 102, \"name\" : \"onezerotwo\", \"val\": 12, \"mode\": 1}',\n    '{\"id\": 224, \"name\" : \"twotwofour\", \"val\": 13, \"mode\": 1}',\n    '{\"id\": 112, \"name\" : \"oneonetow\", \"val\": 9, \"mode\": 1}',\n    '{\"id\": 358, \"name\" : \"threefiveeigth\", \"val\": 4, \"mode\": 1}',\n    '{\"id\": 102, \"name\" : \"onezerotow\", \"val\": 2, \"mode\": 1}',\n\n        '{\"id\": 224, \"name\" : \"twotwofour\", \"val\": 3, \"mode\": 1}',\n        '{\"id\": 112, \"name\" : \"oneonetow\", \"val\": 15, \"mode\": 1}',\n        '{\"id\": 358, \"name\" : \"threefiveeigth\", \"val\": 22, \"mode\": 1}',\n        '{\"id\": 102, \"name\" : \"onezerotwo\", \"val\": 9, \"mode\": 1}',\n\n\n        '{\"id\": 224, \"name\" : \"twotwofour\", \"val\": 0, \"mode\": 0}',\n        '{\"id\": 112, \"name\" : \"oneonetow\", \"val\": 0, \"mode\": 0}',\n        '{\"id\": 358, \"name\" : \"threefiveeigth\", \"val\": 0, \"mode\": 0}',\n        '{\"id\": 102, \"name\" : \"onezerotow\", \"val\": 0, \"mode\": 0}'\n    )\n\n .as('data')\n .select('rand')\n\n|collect()\n.key_fields('data.id')\n.keep('data.id', 'data.val')\n.update(lambda: \"data.val\" &gt; \"__state.data.val\")\n.remove(lambda: \"data.mode\" == 0)\n</code></pre> <p>Collect the max value of <code>data.val</code> for every different <code>data.id</code> value. Note the use of the <code>'__state'</code> prefix for previous value, that can be used in the <code>update</code> expression.</p>"},{"location":"nodes/collect.html#example-4","title":"Example 4","text":"<pre><code>%% input data -&gt;\n\n|json_emitter()\n.every(500ms)\n.json(\n'{\"code\" : {\"id\": 224, \"name\" : \"224\"}, \"message\": \" a test\", \"mode\": 1}',\n'{\"code\" : {\"id\": 334, \"name\" : \"334\"}, \"message\": \" another test\", \"mode\": 1}',\n'{\"code\" : {\"id\": 114, \"name\" : \"114\"}, \"message\": \" another test\", \"mode\": 2}',\n'{\"code\" : {\"id\": 443, \"name\" : \"443\"}, \"message\": \" another test\", \"mode\": 1}', \n'{\"code\" : {\"id\": 111, \"name\" : \"111\"}, \"message\": \" another test\", \"mode\": 1}',\n'{\"code\" : {\"id\": 443, \"name\" : \"443\"}, \"message\": \" another test\", \"mode\": 0}',\n'{\"code\" : {\"id\": 224, \"name\" : \"224\"}, \"message\": \" another test\", \"mode\": 0}',\n'{\"code\" : {\"id\": 111, \"name\" : \"111\"}, \"message\": \" another test\", \"mode\": 0}',\n'{\"code\" : {\"id\": 334, \"name\" : \"334\"}, \"message\": \" another test\", \"mode\": 0}',\n'{\"code\" : {\"id\": 551, \"name\" : \"551\"}, \"message\": \" another test\", \"mode\": 2}',\n'{\"code\" : {\"id\": 551, \"name\" : \"551\"}, \"message\": \" another test\", \"mode\": 0}'\n)\n.as('data')\n\n%% collect 2 fields ('data.code' and 'data.message') \n%% with the key-field 'data.code'.\n%% this node will output a data_batch item with a list of data-points\n%% where the original timestamp and meta-data is preserved and\n%% containing the fields mentioned before every 10 secondes\n\n|collect()\n%% the collect node will build an internal buffer \n%% with the value of the 'key_field' as index\n.key_field('data.code')\n%% criterion for adding a data-point to the internal collection buffer\n.add(lambda: \"data.mode\" &gt; 0)\n%% criterion for removal of values\n.remove(lambda: \"data.mode\" == 0)\n%% we keep these fields in the resulting data_batch\n.keep('data.code', 'data.message') \n.emit_every(10s)\n\n%% make sense of the data-collection in counting the 'data.code' values\n|aggregate()\n.fields('data.code')\n.functions('count')\n.as('code_count')\n\n|debug()\n</code></pre>"},{"location":"nodes/collect.html#parameters","title":"Parameters","text":"Parameter Description Default key_fields(<code>string</code>) The value of the key-field will be used as an index for the collection. add(<code>lambda</code>) Criterion for adding an incoming point to the collection, must return a boolean value. Will always <code>add</code>, if not given. undefined remove(<code>lambda</code>) Criterion for removing a point from the collection, must return a boolen value. undefined update(<code>lambda</code>| <code>boolean</code>) Criterion for updating a data_point in the collection, must return a boolen value. If not given, no updating will occur. Reference the current value with <code>__state</code>. If <code>true</code>, will always update. false update_mode(<code>string</code>) <code>replace</code>, <code>merge</code>, <code>merge_reverse</code>. When updating, an existing point in the collection can be replaced by or merged with the new one. With <code>merge_reverse</code> data_point positions for the merge operation get flipped, so that the existing point is merged onto the new data_point. 'replace' tag_added(<code>boolean</code>) When set to true, emitted data_points that have been added since the last emit will have a field called <code>added</code> with the value of <code>tag_value</code> false tag_value(<code>any</code>) Value to be use for tag fields (<code>added</code>, <code>removed</code>) 1 include_removed(<code>boolean</code>) When set to true, data_points that would normally be removed from the collection will get a field called <code>removed</code> with the value of <code>tag_value</code> and are included in the next data-batch emit false keep(<code>string_list</code>) If given, these field will be kept from every data-point, if not given, the whole item will be kept. undefined keep_as(<code>string_list</code>) Rename fields that are kept (must have the same number of entries as <code>keep</code>). undefined emit_unchanged (<code>boolean</code>) When set to false, processing of a data-item that does not result in a change to the collection, will not trigger an output of data. true emit_every (<code>duration</code>) Interval at which to emit the current collection as a data_batch item. If not given, every data-item (point or batch) will trigger an output (based on the value of <code>emit_unchanged</code>). undefined max_age(<code>duration</code>) Maximum age for any data-item in the collection, before it gets removed. Reference time for item age is the time the item entered the collection. 3h max_ts_age(<code>duration</code>) Maximum age for any data-item in the collection, before it gets removed. Reference time for item age is the timestamp within the item. 3h merge(<code>boolean</code>) Whether to condense the output data_batch into a single data_point item by merging them. false"},{"location":"nodes/collect_fields.html","title":"The collect_fields node","text":"<p>For every field in <code>fields</code>, holds the latest value and outputs a  data-point item with these fields on every incoming data-point. If a field is not present in an input data-point, it will be ignored and the last known value for this field is used in output data.</p> <p>It is possible to set a <code>default</code> value for fields that have not been seen so far.</p> <p>With <code>emit_unchanged</code> set to false, no data is emitted for a data-point, that did not change any of the values.</p> <p>If you need more complex collecting behaviors, take a look at the collect node.</p>"},{"location":"nodes/collect_fields.html#example","title":"Example","text":"<pre><code>|json_emitter(\n'{\"hallo\":\"ho\"}',\n'{\"ballo\":\"du\", \"sim\": 3}',\n'{\"rallo\":\"su\"}',\n'{\"gallo\":\"mu\", \"rallo\": \"srrt\", \"dim\": 232.3}',\n'{\"mallo\":\"ffu\"}',\n'{\"hallo\":\"hott\"}',\n'{\"ballo\":\"dutt\", \"sim\": 3}',\n'{\"rallo\":\"sutt\", \"rallo\": \"srrt\", \"dim\": 232.3}',\n'{\"gallo\":\"mutt\"}',\n'{\"mallo\":\"ffutt\"}'\n)\n.select('rand')\n\n|collect_fields('hallo', 'mallo', 'gallo')\n.default(0.003)\n</code></pre> <p>This will output a data-point item with the fields <code>hallo</code>, <code>mallo</code> and <code>gallo</code>. Fields that have no value in the buffer yet, will get the default value.</p>"},{"location":"nodes/collect_fields.html#parameters","title":"Parameters","text":"Parameter Description Default [node]fields( <code>string_list</code> ) names of the fields used for each computation default( <code>any</code> ) Default value for field, that have not been collected so far. If this is not given, that output item will only contain fields, that have been seen already. undefined keep(<code>string_list</code>) If given, these fields will be included in output additionally to <code>fields</code>. The values for <code>keep</code> fields are taken from the current (incoming) data-point. [] keep_as(<code>string_list</code>) Rename fields that are kept (must have the same number of entries as <code>keep</code>). undefined emit_unchanged (<code>boolean</code>) When set to false, processing of a data-item that does not result in a change to the previous output, will not trigger an output of data. <code>keep</code> fields are excluded from this decision. true"},{"location":"nodes/email.html","title":"The email node","text":"<p>Send an email to one or more recipients</p> <p>Note: This node is a sink node and does not output any flow-data, therefore any node connected to this node will not get any data.</p>"},{"location":"nodes/email.html#example","title":"Example","text":"<pre><code>|email()\n.to('name@email.com')\n.subject('Alert #ex3 EnergyData {{\"building\"}}')\n.body('\n    No data since {{\"datetime\"}} on topic ''home/garage/energy'', \n    last value was {{\"val\"}}. \n    ')\n</code></pre> <p>Sends an email with the subject 'Alert #ex3 EnergyData'.</p> <p>The body will be rendered into an html template (see parameters). Body is a <code>text_template</code> parameter with two template-values: <code>datetime</code> and <code>val</code>, these two fields must be present in the data_point last received in the email node. If a field used in a text_template is not found in the current data_point, the string 'undefined' will be used.</p> <pre><code>|eval(lambda: str_concat('Module', \"mod_number\"))\n.as('email_subject')\n|email()\n.to('name@email.com','another@email.com')\n.subject_field('email_subject')\n.body('\n    No data since {{\"datetime\"}} on topic ''home/garage/energy'', \n    last value was {{\"val\"}}. \n    ')\n</code></pre> <p>Sends an email with the subject build with a lambda expression to 2 recipients.</p>"},{"location":"nodes/email.html#parameters","title":"Parameters","text":"Parameter Description Default to(<code>string_list</code>) the recipient email addresses subject(<code>text_template</code>) body(<code>text_template</code>) body_field(<code>string</code>) field_path used to get the body string subject_field(<code>string</code>) field_path used to get the subject string template (<code>string</code>) html email template to use from config file from_address (<code>string</code>) from config file smtp_relay(<code>string</code>) from config file smtp_user (<code>string</code>) from config file smtp_pass (<code>string</code>) from config file <p><code>body</code> or <code>body_field</code> must be provided. <code>subject</code> or <code>subject_field</code> must be provided.</p>"},{"location":"nodes/mem.html","title":"The mem node","text":"<p>Flow wide value storage. mem values are available to any other node (in lambda expressions) within a flow.</p> <p>There are 3 types of memories:</p> <ul> <li>'single' holds a single value</li> <li>'list' holds a list of values, value order is preserved within the list</li> <li>'set' holds a list of values, where values have no duplicates</li> </ul> <p>Here <code>value</code> can be any valid datatype that is supported in faxe, from a single scalar to a nested map and/or list.</p> <p>The values will be held in a non persistent ets term storage.</p>"},{"location":"nodes/mem.html#example-1","title":"Example 1","text":"<pre><code>|mem()\n.type('set')\n.field('topic')\n.key('topics_seen')\n</code></pre> <p>Holds a set of values from the field named <code>topic</code>. The set of values is available in lambda expression (within the same flow) with the key <code>topics_seen</code>.</p> <p>The above set can be used in lambda expressions with the functions: <code>ls_mem</code>, <code>ls_mem_list</code>, <code>ls_mem_set</code>.</p> <pre><code>|where(\n    lambda: member(\"topic\", ls_mem_set('topics_seen')) \n)\n</code></pre> <p>This will filter out all points that have a topic field, which has already been stored in the mem set. Thus the <code>where</code> node will only output points with a unique topic value.</p>"},{"location":"nodes/mem.html#-","title":"---------------------------------------------------------------------------------------------------","text":""},{"location":"nodes/mem.html#example-2","title":"Example 2","text":"<pre><code>def default_map = \n    '{\"key1\":\"topic/1/new\", \"key2\":\"topic/2/new\", \"key3\":\"topic/3/new\"}'\n\n|mem() \n.key('topic_lookup')\n.default(default_map)\n.default_json()\n</code></pre> <p>In the above example the <code>mem</code> node has no <code>field</code> parameter, but it is prepopulated with a json structure. The mem node is used  as a lookup table here. The default  value will stay in the storage as long as the node is running. With no field parameter given, a data_item coming in to the node will not overwrite the stored value.</p> <p>The internal representation of the given json object is a <code>map</code> in faxe.</p> <p>The stored map could then be use in a lambda expression:</p> <pre><code>|eval(lambda: map_get(\"some_field_name\", ls_mem('topic_lookup'))).as('topic')\n</code></pre> <p>So based on the value of the field \"some_field_name\", the field <code>topic</code> will get the value of the corresponding map-key stored in the mem node.</p> <p>For a list of lambda_library functions see lambda_functions.</p>"},{"location":"nodes/mem.html#parameters","title":"Parameters","text":"Parameter Description Default field( <code>string</code> ) field-path undefined key( <code>string</code> ) name of the value storage type( <code>string</code> ) Type of mem storage, one of 'single', 'list' or 'set' 'single' default( <code>string</code> | <code>number</code> ) Prefill the storage with this value undefined default_json( is_set) When set, the <code>default</code> value will be interpreted as a json string false (not set) <p>At least one of the parameters <code>field</code> and/or <code>default</code> has to be defined, otherwise this node will not be able to do anything useful.</p>"},{"location":"nodes/python.html","title":"The python node","text":"<p>deprecated since vsn 1.0.0, see custom_nodes.</p> <p>Rules for python callback classes:</p> <ul> <li>Callback class must be in a module with the lowercase name of the class ie: module: \"double\", class: \"Double\"</li> <li>python callback class must be a subclass of the class Faxe from module faxe</li> <li>'abstract' methods to implement are (note: they are all optional):<ul> <li><code>options()</code> -&gt; return a list of tuples // static</li> <li><code>init(self, args</code>) -&gt; gets the object and a dict with args from options()</li> <li><code>handle_point(self, point_data)</code> -&gt; point_data is a dict</li> <li><code>handle_batch(self, batch_data</code>) -&gt; batch_data is a list of dicts (points)</li> </ul> </li> <li>the callbacks need not return anything except for the options method</li> <li>to emit data the method <code>self.emit(data)</code> has to be used, where data is a dict or a list of dicts</li> <li>All fields of data-item going in and out of a custom python node are placed under the root-object <code>data</code>.</li> </ul> <p>A custom python node is used with an <code>@</code> as node sign instead of <code>|</code> in dfs!</p> <pre><code>@my_custom_python_node()\n</code></pre>"},{"location":"nodes/python.html#parameters","title":"Parameters","text":"<p>Parameters can be freely defined by the python callback class via the static <code>options()</code> method (See example below). Note that parameter definition must be in python's <code>bytes</code> type.</p>"},{"location":"nodes/python.html#example-callback","title":"Example Callback","text":"<p>The example python callback class below defined 2 Parameters:</p> <ul> <li><code>field</code> must be a string and has no default value (so it must be given)</li> <li><code>as</code> must be a string and has the default value 'double'</li> </ul> <pre><code>from faxe import Faxe\n\n\nclass Double(Faxe):\n\n    @staticmethod\n    def options():\n\"\"\"\n        overwrite this method to request options you would like to use\n\n        return value is a list of tuples: (option_name, option_type, (optional: default type))\n        a two tuple: (b\"foo\", b\"string\") with no default value is mandatory in the dfs script\n        a three tuple: (b\"foo\", b\"string\", b\"mystring\") may be overwritten in a dfs script\n\n        :return: list of tuples\n        \"\"\"\n        opts = [\n            (b'field', b'string'),\n            (b'as', b'string', b'double')\n        ]\n        return opts\n\n    def init(self, args):\n\"\"\"\n        will be called on startup with args requested with options()\n        :param args: dict\n        \"\"\"\n        self.fieldname = args[b'field']\n        self.asfieldname = args[b'as']\n        print(\"my args: \", args)\n\n    def handle_point(self, point_data):\n\"\"\"\n        called when a data_point comes in to this node\n        :param point_data: dict\n        \"\"\"\n        self.emit(self.calc(point_data[\"data\"]))\n\n    def handle_batch(self, batch_data):\n\"\"\"\n        called when a data_batch comes in\n        :param batch_data: list of dicts\n        \"\"\"\n        out_list = list()\n        for point in batch_data:\n            out_list.append(self.calc(point[\"data\"]))\n        self.emit(out_list)\n\n    def calc(self, point_dict):\n        point_dict[self.asfieldname] = point_dict[self.fieldname] * 2\n        return point_dict\n</code></pre> <p>Use in a dfs script:</p> <pre><code>@double()\n.field('val')\n.as('double_val')\n</code></pre>"},{"location":"nodes/stats.html","title":"The stats node","text":"<p>The stats node lets you compute statistical functions on data_points and data_batches.</p> <p>See nodes under <code>statistics</code> for details.</p> <p>Stats nodes produce a new stream, the incoming stream is not outputted.</p>"},{"location":"nodes/stats.html#parameters","title":"Parameters","text":"<p>All statistics nodes have the following parameters</p> Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node"},{"location":"nodes/tcp_send.html","title":"The tcp_send node","text":"<p>This node connects to a tcp endpoint and sends data with a defined packet size.</p> <p>Data to be sent can be: + a predefined string + a predefined json string + data that comes in to this node from downstream nodes</p> <p>Sending can be done either periodically (if <code>every</code> is given) or triggered by an incoming data-item. </p> <p>The node can also handle \"responses\" from the peer endpoint (timeout can be given). After the node has sent a tcp message, it starts the timeout to wait for the response.  Any data that comes in after this timeout will be ignored.</p> <p>But note: There is no guarantee, that a message that is received by this node on its tcp socket is in any way related to the message it just sent out. In other words: unless every message (and its response) is marked with some unique id, there is no means of ensuring that any incoming message is the \"response\" to a previous send operation done by this node. A tcp endpoint can send a message to another endpoint whenever is wants to do so. If you rely on a strict request-response paradigm, consider using http, as this is made for such operations.</p>"},{"location":"nodes/tcp_send.html#packet-data","title":"Packet Data","text":"<p><code>packet</code> can be: 1 | 2 | 4 and defaults to 2. Packets consist of a header specifying the number of bytes in the packet, followed by that number of bytes. The header length can be one, two, or four bytes, and containing an unsigned integer in big-endian byte order.</p> <pre><code>`Length_Header:16/integer, Data:{Length_Header}/binary`\n</code></pre> <p><code>Experimental</code> since <code>0.19.19</code>: packet now can also have the value 'line', if this is given, data from the socket will be treated as separated by newline characters (\\n, \\r\\n).</p> <p>The tcp listener is protected against flooding with the {active, once} inet option.</p>"},{"location":"nodes/tcp_send.html#example","title":"Example","text":"<pre><code>|tcp_send()\n.ip('127.0.0.1')\n.port(5555)\n.every(3s)\n.msg_text('hello tcp!') \n</code></pre> <p>tcp_send will send the string \"hello tcp!\" every 3 seconds ignoring any incoming tcp data. It will also send the same string, if any data-item is received from parent nodes.</p> <pre><code>|tcp_send()\n.ip('127.0.0.1')\n.port(5555) \n.response_as('data.tcp.response[1]')\n.response_timeout(2s)\n.response_json()\n</code></pre> <p>tcp_send will send all data coming in from its downstream nodes, after sending it will wait for data with a 2 second timeout. Data received will be interpreted as a json-string and injected into the current data-item with the path: data.tcp.response[1] (first array entry of the data-tcp-response subobject)</p>"},{"location":"nodes/tcp_send.html#parameters","title":"Parameters","text":"Parameter Description Default ip( <code>string</code> ) ip or hostname for the tcp peer port( <code>integer</code> ) port number packet( <code>integer</code> ) packet length 2 every( <code>string</code> ) send interval undefined msg_text( <code>string</code> ) predefined string to send to the peer endpoint undefined msg_json( <code>string</code> ) predefined json-string to send to the peer endpoint undefined response_as( <code>string</code> ) name of the field for parsed data undefined response_json( is_set ) interprets a response as a json-string false (not set) response_timeout( duration ) timeout for a \"response\" after a message has been sent 5s"},{"location":"nodes/udp_send.html","title":"The udp_send node","text":"<p>This node sets up an udp socket and sends data to a specified endpoint (host, port). Before sending, data-items will be converted to json format.</p> <p>Note: Broadcast messages are not possible with this node.</p> <p>Note: This node is a sink node and does not output any flow-data, therefore any node connected to this node will not get any data.</p>"},{"location":"nodes/udp_send.html#example","title":"Example","text":"<pre><code>|udp_send()\n.host('127.0.0.1')\n.port(5555)\n</code></pre> <p>Will send incoming data-items json-encoded to the specified endpoint.</p>"},{"location":"nodes/udp_send.html#parameters","title":"Parameters","text":"Parameter Description Default host( <code>string</code> ) ip or hostname for the udp peer port( <code>integer</code> ) port number"},{"location":"nodes/data_collection/modbus.html","title":"The modbus node","text":"<p>Deprecated since version 0.19.7, use the modbus_read instead.</p> <p>Read data via modbus tcp, supported read functions are : ['coils', 'hregs', 'iregs', 'inputs', 'memory']</p> <p>Reading can be done periodically (if <code>every</code> is given) and/or via a trigger (incoming value).</p> <p>Read multiple values with possibly different functions at once.</p> <p>The node will optimize reading by treating contiguous values as one reading var.  Normally the node will open one connection for every variable (after read optimization is applied).  The maximum number of connections can be set with the <code>max_connections</code> option. </p> <p>If the <code>align</code> property is set, the nodes's read times will be truncated to the every property (For example, if the node is started at 12:06 and the every property is 5m then the next read will  occur at 12:10, then the next at 12:15 and so on, instead of 12:06, 12:11 and so on).</p>"},{"location":"nodes/data_collection/modbus.html#examples","title":"Examples","text":"<pre><code>|modbus()\n.ip('127.0.0.1') \n.device(255)\n.every(1s)\n.function('coils', 'hregs', 'iregs', 'hregs')\n.from(2127, 3008, 104, 30306)\n.count(1, 2, 2, 4)\n.as('data.EnergyConsumption', 'data.CurrentValue', 'data.EnergyDelivered')\n.output('int16', 'float32', 'float32', 'double')\n.signed(true, true, false, false) \n</code></pre> <pre><code>|modbus()\n.ip('127.0.0.1')  \n.every(2m)\n.function('hregs', 'hregs', 'hregs')\n.from(2127, 2125, 104)\n.count(2, 2, 2)\n.as('data.EnergyConsumption', 'data.CurrentValue', 'data.EnergyDelivered')\n.output('float32', 'float32', 'float32') \n</code></pre> <p>The above modbus node will open 2 connections to the given modbus device, because the start values 2125 (2 bytes) and 2127 (2 bytes) can be treated as one single value.</p>"},{"location":"nodes/data_collection/modbus.html#parameters","title":"Parameters","text":"Parameter Description Default ip( <code>string</code> ) ip address of modbus device port( <code>integer</code> ) port of modbus device 502 every( <code>duration</code> ) time between reads undefined align( is_set ) align read intervals according to every false (not set) device( <code>integer</code> ) modbus device id (0-255) 255 function( <code>string_list</code> ) list of read functions, one of <code>['coils', 'hregs', 'iregs', 'inputs', 'memory']</code> from( <code>integer_list</code> ) list of start values count( <code>integer_list</code>) list of count values, how much data to read for every function given as( <code>string_list</code> ) output names for the read values output( <code>string_list</code> ) list of output formats one of <code>['int16', 'int32', 'float32', 'double', 'coils', 'ascii', 'binary']</code> undefined signed( <code>atom_list</code> true/false) list of values indicating if values are signed undefined round( <code>integer</code> ) Round all <code>float32</code> and <code>double</code> values with the given precision. If a value has less than the given decimal places, it will be left untouched undefined (no rounding) max_connections( <code>integer</code> ) number of connections to the modbus device <code>auto</code>, meaning 1 connection for every variable (after read optimization) <p>Note that, if given, all read parameters(<code>function, from, count, as, output, signed</code>) must have the same length, this means if you have two values you want to read :</p> <pre><code>.function('coils', 'hregs')` \n</code></pre> <p>all corresponding read params (if given) must have the same length:</p> <pre><code>.as('val1', 'val2')\n.output(int16, float32)\n.from(1,2) \n.count(2,4)\n.signed(true, true)\n</code></pre>"},{"location":"nodes/data_collection/modbus_read.html","title":"The modbus_read node","text":"<p>Read data from a modbus slave device via modbus tcp, supported read functions are : ['coils', 'hregs', 'iregs', 'inputs', 'memory']</p> <p>Reading can be done periodically (if <code>every</code> is given) and/or via a trigger (incoming value).</p> <p>Read multiple values with possibly different functions at once.</p> <p>The node will optimize reading by treating contiguous values as one reading var. </p> <p>If the <code>align</code> property is set to true (default), the nodes's read times will be truncated to the every property (For example, if the node is started at 12:06 and the every property is 5m then the next read will  occur at 12:10, then the next at 12:15 and so on, instead of 12:06, 12:11 and so on).</p>"},{"location":"nodes/data_collection/modbus_read.html#examples","title":"Examples","text":"<pre><code>|modbus_read()\n.ip('127.0.0.1') \n.device(255)\n.every(1s)\n.function('coils', 'hregs', 'iregs', 'hregs')\n.from(2127, 3008, 104, 30306)\n.count(1, 2, 2, 4)\n.as('data.EnergyConsumption', 'data.CurrentValue', 'data.EnergyDelivered')\n.output('int16', 'float32', 'float32', 'double')\n.signed(true, true, false, false) \n</code></pre> <pre><code>|modbus_read()\n.ip('127.0.0.1')  \n.every(2m)\n.function('hregs', 'hregs', 'hregs')\n.from(2127, 2125, 104)\n.count(2, 2, 2)\n.as('data.EnergyConsumption', 'data.CurrentValue', 'data.EnergyDelivered')\n.output('float32', 'float32', 'float32') \n</code></pre>"},{"location":"nodes/data_collection/modbus_read.html#parameters","title":"Parameters","text":"Parameter Description Default ip( <code>string</code> ) ip address of modbus device port( <code>integer</code> ) port of modbus device 502 every( <code>duration</code> ) time between reads undefined align( <code>boolean</code> ) align read intervals according to every true device( <code>integer</code> ) modbus device id (0-255) 255 function( <code>string_list</code> ) list of read functions, one of <code>['coils', 'hregs', 'iregs', 'inputs', 'memory']</code> from( <code>integer_list</code> ) list of start values count( <code>integer_list</code>) list of count values, how much data to read for every function given as( <code>string_list</code> ) output names for the read values output( <code>string_list</code> ) list of output formats one of <code>['int16', 'int32', 'float32', 'double', 'coils', 'ascii', 'binary']</code> undefined signed( <code>atom_list</code> true/false) list of values indicating if values are signed undefined round( <code>integer</code> ) Round all <code>float32</code> and <code>double</code> values with the given precision. If a value has less than the given decimal places, it will be left untouched undefined (no rounding) timeout( <code>duration</code> ) Read timeout for one request 5s <p>Note that, if given, all read parameters(<code>function, from, count, as, output, signed</code>) must have the same length, this means if you have two values you want to read :</p> <pre><code>.function('coils', 'hregs')` \n</code></pre> <p>all corresponding read params (if given) must have the same length:</p> <pre><code>.as('val1', 'val2')\n.output(int16, float32)\n.from(1,2) \n.count(2,4)\n.signed(true, true)\n</code></pre>"},{"location":"nodes/data_collection/s7read.html","title":"The s7read node","text":"<p>Read data from a siemens s7 plc via the snap7 library using the <code>iso on tcp protocol</code>. The node is tested to read data from S7 300 and S7 1500 CPUs, but should be able to read data from any S7 PLC that supports the S7 comm protocol.</p> <p>Reading can be done periodically and/or triggered via incoming data-items. If the <code>every</code> parameter is not given, reading will be done only with trigger data-items.</p> <ul> <li>Data <code>addressing</code> can be done in a <code>Step7</code> schema or with a slightly different schema used in node-red (although the step7 variant is preferred).  See table below for more information about addressing.</li> </ul>"},{"location":"nodes/data_collection/s7read.html#optimzation-when-reading-data","title":"Optimzation when reading data","text":"<ul> <li>The node will optimize reading by treating contiguous values as one reading var. Thus more data can be read in one go.</li> <li> <p>since 0.20.0: when <code>optimized</code> is set to true (default), then optimization and reading will be done outside of the node.   All variables from all s7read nodes, that target the same PLC are optimized as a whole, this makes it possible to take advantage of the maximum PDU size.  This will possibly lead to more than one request for a specific time slot, if there are many vars to read.  In optimized mode, it does not make a difference, if we use many s7read nodes, where every node just reads a view vars, or if we have  only one s7read node, that reads a lot of vars (or anything in between)</p> </li> <li> <p>Connections to a PLC are handled by a connection pool, if <code>use_pool</code> is set to true, which can grow and shrink as needed. (This is especially useful, when connecting to a PLC, that has limited connection resources) The number of  Min and Max connection count can be set in faxe configuration. Defaults to 2 and 16.</p> </li> </ul>"},{"location":"nodes/data_collection/s7read.html#strings","title":"Strings","text":"<p>A String is defined as a sequence of contiguous char (byte) addresses. For strings faxe uses a special syntax not found in step7 addressing:  <code>DB5.DBS7.4</code> would read 4 bytes starting at byte 7 of DB 5 and output a string value.</p> <p><code>Note</code>: Characters with an ascii value below 32 will be stripped from the char-sequence. Also for strings the above mentioned read-optimization will not be used.</p>"},{"location":"nodes/data_collection/s7read.html#examples","title":"Examples","text":"<pre><code>|s7read() \n.ip('10.1.1.5')\n.rack(0)\n.slot(2)\n.every(300ms) \n.vars(\n    'DB1140.DBX4.0', 'DB1140.DBX4.1',\n    'DB1140.DBX4.4', 'DB1140.DBX4.5'\n    )\n\n.as(\n    'data.tbo[1].ix_OcM1', 'data.tbo[1].ix_OcM2',\n    'data.tbo[1].ix_Lift_PosTop', 'data.tbo[1].ix_Lift_PosBo'\n    )\n</code></pre> <p>Read 4 values (BOOL in this case) from a plc every 300 milliseconds and name them with a deep json path.</p> <pre><code>def db_number = 1140\ndef db = 'DB{{db_number}}.DB'\n|s7read()  \n.ip('10.1.1.5')\n.as_prefix('data.tbo.') \n.vars_prefix(db)\n.vars(\n    'X4.0', 'X4.1',\n    'X4.4', 'X4.5'\n    )\n\n.as(\n    'ix_OcM1', 'ix_OcM2',\n    'ix_Lift_PosTop', 'ix_Lift_PosBo'\n    )\n</code></pre> <p>Use of <code>as_prefix</code> and <code>vars_prefix</code>. The node will not read data on its own, because it has no every parameter. instead reading is done on data input from another node.</p> <pre><code>|s7read()\n.ip('10.1.1.5')\n.rack(0)\n.slot(2)\n.every(300ms) \n.vars('DB12004.DBS36.30')\n.as('data.LcBc')\n</code></pre> <p>Read a sequence of 30 bytes as a string.</p> <pre><code>def db_number = 1140\ndef db = 'DB{{db_number}}.DB'\n|s7read()\n.ip('10.1.1.5')\n.as_prefix('data.tbo.') \n.vars_prefix(db)\n.byte_offset(4)\n.vars(\n    'X0.0', 'X0.1',\n    'X0.4', 'X0.5'\n    )\n\n.as(\n    'ix_OcM1', 'ix_OcM2',\n    'ix_Lift_PosTop', 'ix_Lift_PosBo'\n    )\n</code></pre> <p>In the last example <code>byte_offset</code> of 4 is used, so effectively the following addresses will be used: <code>X4.0, X4.1, X4.4, X4.5</code></p> <p><code>since v0.19.5</code> </p> <p>When <code>as</code> is not given, every second entry in <code>vars</code> is used as a field-name instead.</p> <pre><code>def db_number = 1140\ndef db = 'DB{{db_number}}.DB'\n|s7read()\n.ip('10.1.1.5')\n.as_prefix('data.tbo.') \n.vars_prefix(db)\n.byte_offset(4)\n.vars(\n    'X0.0', 'ix_OcM1',\n    'X0.1', 'ix_OcM2',\n    'X0.4', 'ix_Lift_PosTop',\n    'X0.5', 'ix_Lift_PosBo'\n    )\n</code></pre>"},{"location":"nodes/data_collection/s7read.html#parameters","title":"Parameters","text":"Parameter Description Default ip( <code>string</code> ) ip address of plc port( <code>integer</code> ) network port 102 (standard s7 port) every( <code>duration</code> ) time between reads undefined align( is_set ) align read intervals according to every false (not set) slot( <code>integer</code> ) plc slot number 1 rack( <code>integer</code> ) plc rack number 0 vars( <code>string_list</code> ) list of s7 addresses ie: 'DB3.DBX2.5' (see table below) as( <code>string_list</code> ) output names for the read values Since 0.19.5: if not given, every second entry in <code>vars</code> is used as a fieldname (prefixes can still be used) undefined vars_prefix( <code>string</code> ) vars  will be prefixed with this value undefined as_prefix( <code>string</code> ) as values will be prefixed with this value undefined byte_offset( <code>integer</code> ) offset for addressing, every address in vars gets this offset added 0 diff( is_set ) when given, only output values different to previous values false (not set) use_pool ( <code>bool</code> ) whether to use the built-in connection pool from config value optimized ( <code>bool</code> ) whether to use the collected read optimization or do standalone reading for this node from config value <p>Note that params <code>vars</code> and <code>as</code> must have the same length (if both are given).</p>"},{"location":"nodes/data_collection/s7read.html#data-addressing","title":"Data addressing","text":"<p><code>Note: Step7 style preferred and should be used !</code></p> Address Step7 equivalent Description DB5,X0.1 DB5.DBX0.1 Bit 1 of byte 0 of DB 5 DB23,B1 or DB23,BYTE1 DB23.DBB1 Byte 1 (0-255) of DB 23 DB100,C2 or DB100,CHAR2 DB100.DBC2 Byte 2 of DB 100 as a Char -- DB42.DBSINT36 or DB42.DBSI36 Signed 8-bit integer at byte 36 of DB 42 -- DB42.DBUSINT5 or DB42.DBUSI5 Unsigned 8-bit integer at byte 5 of DB 42 (equivalent to BYTE) DB42,I3 or DB42,INT3 DB42.DBI3 or DB42.DBINT3 Signed 16-bit integer at byte 3 of DB 42 DB57,WORD4 DB57.DBW4 Unsigned 16-bit integer at byte 4 of DB 57 DB13,DI5 or DB13,DINT5 DB13.DBDI5  or DB13.DBDINT5 Signed 32-bit integer at byte 5 of DB 13 DB19,DW6 or DB19,DWORD6 DB19.DBDW6 Unsigned 32-bit integer at byte 6 of DB 19 DB21,DR7 or DB21,REAL7 DB21.DBDR7 or DB21.DBR7 Floating point 32-bit number at byte 7 of DB 21 DB2,S7.10* <code>DB2.DBS7.10</code> (faxe only) String of length 10 starting at byte 7 of DB 2 DB1100,DT8 DB1100.DBDT8 or DB1100.DT8 8 byte S7 DATE_AND_TIME format (DT) (UTC is assumed) I1.0 or E1.0 I1.0 or E1.0 Bit 0 of byte 1 of input area Q2.1 or A2.1 Q2.1 or A2.1 Bit 1 of byte 2 of output area M3.2 QM3.2 Bit 2 of byte 3 of memory area IB4 or EB4 IB4 or EB4 Byte 4 (0 -255) of input area QB5 or AB5 QB5 or AB5 Byte 5 (0 -255) of output area MB6 MB6 Byte 6 (0 -255) of memory area IC7 or EC7 IB7 or EB7 Byte 7 of input area as a Char QC8 or AC8 QB8 or AB8 Byte 8 of output area as a Char MC9 MB9 Byte 9 of memory area as a Char II10 or EI10 IW10 or EW10 Signed 16-bit integer at byte 10 of input area QI12 or AI12 QW12 or AW12 Signed 16-bit integer at byte 12 of output area MI14 MW14 Signed 16-bit integer at byte 14 of memory area IW16 or EW16 IW16 or EW16 Unsigned 16-bit integer at byte 16 of input area QW18 or AW18 QW18 or AW18 Unsigned 16-bit integer at byte 18 of output area MW20 MW20 Unsigned 16-bit integer at byte 20 of memory area IDI22 or EDI22 ID22 or ED22 Signed 32-bit integer at byte 22 of input area QDI24 or ADI24 QD24 or AD24 Signed 32-bit integer at byte 24 of output area MDI26 MD26 Signed 32-bit integer at byte 26 of memory area ID28 or ED28 ID28 or ED28 Unsigned 32-bit integer at byte 28 of input area QD30 or AD30 QD30 or AD30 Unsigned 32-bit integer at byte 30 of output area MD32 MD32 Unsigned 32-bit integer at byte 32 of memory area IR34 or ER34 IR34 or ER34 Floating point 32-bit number at byte 34 of input area QR36 or AR36 QR36 or AR36 Floating point 32-bit number at byte 36 of output area MR38 MR38 Floating point 32-bit number at byte 38 of memory area"},{"location":"nodes/data_collection/tcp_receive.html","title":"The tcp_receive node","text":"<p>This node is used to receive data from a tcp socket. There are two modes it can operate in:</p> <ul> <li><code>Connect</code>: If <code>ip</code> is set to a valid hostname or ip address, the node will connect to that peer and then just waits for incoming data.</li> <li><code>Listen</code>: Without <code>ip</code>, the node will just open a tcp listen port and waits for a connection from outside to receive data.</li> </ul>"},{"location":"nodes/data_collection/tcp_receive.html#data-parser","title":"Data - Parser","text":"<p>When we define a parser module, incoming data can be parsed to faxe's internal data-format with it. </p> <p>If no <code>parser</code> is defined, incoming data will be set as is to the field given with the <code>as</code> paramenter.</p> <p>If both <code>parser</code> and <code>as</code> are not given, the node assumes, that incoming data has JSON format and will try to decode it into faxe's internal format.</p>"},{"location":"nodes/data_collection/tcp_receive.html#packet-size","title":"Packet size","text":"<p>Data is expected with a defined packet size.</p> <p><code>packet</code> can be: 1 | 2 | 4 and defaults to 2. Packets consist of a header specifying the number of bytes in the packet, followed by that number of bytes. The header length can be one, two, or four bytes, and containing an unsigned integer in big-endian byte order.</p> <pre><code>`Length_Header:16/integer, Data:{Length_Header}/binary`\n</code></pre> <p>Since <code>0.19.19</code>: packet now can also have the value 'line', if this is given, data from the socket will be treated as separated by newline characters (\\n, \\r\\n).</p> <p>If the <code>changed</code> option is given, the node will only emit on changed values (crc32 checksum comparison).</p> <p>The tcp listener is protected against flooding with the {active, once} inet option.</p>"},{"location":"nodes/data_collection/tcp_receive.html#examples","title":"Examples","text":"<pre><code>|tcp_recv() \n.port(9745) \n.packet(4) \n</code></pre> <p>Sets up a tcp listen socket on port 9745 and awaits an incoming connection. It uses a 4-byte length header to determine  packet boundaries and will try to json-decode incoming data.</p> <pre><code>def parser = 'parser_robot_plc_v1'\n\n|tcp_recv()\n.ip('212.14.149.8')\n.port(9715)\n.parser(parser)\n.as('data')\n</code></pre> <pre><code>|tcp_recv()\n.ip('212.14.149.8')\n.port(9715) \n.packet(4)\n.as('data.raw')\n</code></pre>"},{"location":"nodes/data_collection/tcp_receive.html#parameters","title":"Parameters","text":"Parameter Description Default ip( <code>string</code> ) ip or hostname for the tcp peer undefined port( <code>integer</code> ) port number packet( <code>integer</code>  or 'line' ) packet length/type 2 parser( <code>string</code> ) name of parser to use for data conversion undefined as( <code>string</code> ) name of the field for parsed data undefined changed( is_set ) whether to check for changed data false (not set)"},{"location":"nodes/data_collection/tcp_receive_line.html","title":"The tcp_receive_line node","text":"<p>deprecated since 1.0.0, use tcp_recv with <code>packet</code> type 'line' instead.</p> <p>This node connects to a tcp endpoint and awaits data in a <code>line separated</code> special format,  which is defined by the <code>parser</code> parameter. The parser will then try to convert the data to faxe's internal format and emit the result.</p> <p>At the moment the line separator is fixed to <code>\\n</code>. </p> <p>If the <code>changed</code> option is given, the node will only emit on changed values (crc32 checksum comparison).</p> <p>The tcp listener is protected against flooding with the {active, once} inet option.</p>"},{"location":"nodes/data_collection/tcp_receive_line.html#example","title":"Example","text":"<pre><code>def parser = 'parser_conv_tracking_v1'\n\n|tcp_recv_line()\n.ip('212.14.149.3')\n.port(2004)\n.parser(parser)\n.as('data')\n</code></pre>"},{"location":"nodes/data_collection/tcp_receive_line.html#parameters","title":"Parameters","text":"Parameter Description Default ip( <code>string</code> ) ip or hostname for the tcp peer port( <code>integer</code> ) port number parser( <code>string</code> ) name of parser to use for data conversion, see table below as( <code>string</code> ) name of the field for parsed data changed( is_set ) whether to check for changed data false (not set) min_length( integer) lines shorter than min_length bytes will be ignored 61"},{"location":"nodes/data_collection/tcp_receive_line.html#available-parsers","title":"Available Parsers","text":""},{"location":"nodes/data_collection/udp_receive.html","title":"The udp_receive node","text":"<p>This node listens on an <code>udp socket</code> and awaits data in a special format, which is defined by the <code>parser</code> parameter, the parser will then try to convert the data to faxe format and emit the result.</p> <p>If the <code>changed</code> option is given, the node will only emit on changed values (crc32 checksum comparison).</p> <p>The udp listener is protected against flooding with the {active, once} inet option.</p>"},{"location":"nodes/data_collection/udp_receive.html#example","title":"Example","text":"<pre><code>def parser = 'parser_robot_plc_v1'\n\n|udp_recv()\n.ip('212.14.149.8')\n.port(9715)\n.parser(parser)\n.as('data')\n</code></pre>"},{"location":"nodes/data_collection/udp_receive.html#parameters","title":"Parameters","text":"Parameter Description Default port( <code>integer</code> ) port number parser( <code>string</code> ) name of parser to use for data conversion, see table below as( <code>string</code> ) name of the field for parsed data changed( is_set ) whether to check for changed data false (not set)"},{"location":"nodes/data_collection/udp_receive.html#available-parsers","title":"Available Parsers","text":""},{"location":"nodes/database/crate_out.html","title":"The crate_out node","text":"<p>Write data to CrateDB.</p> <p>Sends data to a CRATE DB HTTP endpoint using Crate's HTTP Api. If any errors occur during the request, the node will attempt to retry sending.</p> <p>Note: This node is a sink node and does not output any flow-data, therefore any node connected to this node will not get any data.</p>"},{"location":"nodes/database/crate_out.html#since-vs-1013","title":"Since vs. 1.0.13:","text":"<p>If <code>db_fields</code> and <code>faxe_fields</code> are not given, the node now tries to find the table structure on its own, by querying the destination table. In this mode, the remaining_fields_as parameter will be ignored. For this feature a separate connection to CrateDB is used, which gets its default connection parameters from config settings regarding the <code>postgre protocol</code> of CrateDB.</p>"},{"location":"nodes/database/crate_out.html#example","title":"Example","text":"<pre><code>def db_table = 'grip_log_fulltext3'\ndef db_fields = ['id', 'df', 'vs', 'topic']\ndef faxe_fields = ['id', 'df', 'vs', 'topic']\n\n|crate_out() \n.table(db_table)\n.db_fields(db_fields)\n.faxe_fields(faxe_fields)\n.remaining_fields_as('data_obj')\n</code></pre> <p>Inserts the faxe-fields <code>id</code>, <code>df</code>, <code>vs</code>, <code>topic</code> into the db-fields with the same names and all remaining fields into the db-field named <code>data_obj</code> (which is of type 'OBJECT') in the table <code>grip_log_fulltext3</code> .</p> <p>Note: The timestamp field <code>ts</code> will always be written.</p> <pre><code>def db_table = 'grip_log_fulltext3' \n\n|crate_out() \n.database('test')\n.table(db_table) \n</code></pre> <p>Since vs. 1.0.13 The node derives the list of fields (faxe and db) by querying the schema for the given table. In this example schema (<code>database</code>) 'test' is used.</p>"},{"location":"nodes/database/crate_out.html#parameters","title":"Parameters","text":"Parameter Description Default host( <code>string</code> ) hostname or ip address of endpoint config: <code>crate_http.host</code>/<code>FAXE_CRATE_HTTP_HOST</code> port( <code>integer</code> ) port number config: <code>crate_http.port</code>/<code>FAXE_CRATE_HTTP_PORT</code> user( <code>string</code> ) username config: <code>crate_http.user</code>/<code>FAXE_CRATE_HTTP_USER</code> pass( <code>string</code> ) password config: <code>crate_http.pass</code>/<code>FAXE_CRATE_HTTP_PASS</code> tls( <code>is_set</code> ) whether to use tls ie. https config: <code>crate_http.tls.enable</code>/<code>FAXE_CRATE_TLS_ENABLE</code> database( <code>string</code> ) database/schema name 'doc' table( <code>string</code> ) database tablename db_fields( <code>string_list</code> ) db fieldnames (mapping for faxe fieldname to table field names), since 1.0.13: If not given, fields will be determined automatically undefined (since 1.0.13) faxe_fields( <code>string_list</code> ) faxe fieldnames (mapping for faxe fieldname to table field names), since 1.0.13: If not given, fields will be determined automatically undefined (since 1.0.13) remaining_fields_as( <code>string</code> ) if given inserts all fields not in faxe_fields into the given field, which must be of type 'object' undefined max_retries( <code>integer</code> ) max retry attempts after INSERT statement fails 3 use_flow_ack( <code>boolean</code> ) only in combination with a amqp_consume node, message will only be acknowledged to the amqp broker, when it is written to crate db false deduplicate( <code>boolean</code> ) whether to perform data deduplication true pg_port( <code>integer</code> ) since v 1.3 overwrite the port used for the separate postgre connection config: <code>crate.port</code>/<code>FAXE_CRATE_PORT</code> pg_user( <code>string</code> ) since v 1.3 overwrite the username used for the separate postgre connection config: <code>crate.user</code>/<code>FAXE_CRATE_USER</code> pg_pass( <code>string</code> ) since v 1.3 overwrite the password used for the separate postgre connection config: <code>crate.pass</code>/<code>FAXE_CRATE_PASS</code> pg_tls( <code>boolean</code> ) since v 1.3 overwrite the use of tls for the separate postgre connection, whether to use tls config: <code>crate.tls.enable</code>/<code>FAXE_CRATE_TLS_ENABLE</code>"},{"location":"nodes/database/crate_out.html#use_flow_ack","title":"use_flow_ack","text":"<p>This is a special mode that only has effect in combination with an amqp_consume node with the same setting (.use_flow_ack(true)). If this mode is enabled, messages consumed from an amqp broker will only be acknowledged after they are written to crate db. For normal errors (4xx) max_retries will be used, but for 5xx errors requests will be retried forever (until the problem on the CrateDB side is solved). There is an ENV setting and an API endpoint with which we can define rules to ignore such 5xx errors avoiding infinite retries for those.</p> <p>For a faxe-wide setting there is a special ENV variable used: FAXE_AMQP_FLOW_ACK_ENABLE=on, so you do not set this manually inside a flow script normally.</p>"},{"location":"nodes/database/crate_out.html#example_1","title":"Example:","text":"<p>FAXE_CRATE_IGNORE_RULES='message=MaxBytesLengthExceededException,code=5000'</p> <p>message rule: takes effect, if <code>message</code> is part of the response text from CrateDB</p> <p>code rule: takes effect, if the response error code matches <code>code</code></p> <p>Multiple message and code rules can be used</p>"},{"location":"nodes/database/crate_query.html","title":"The crate_query node","text":"<p>Query the CRATE database for <code>time series data</code>. This node is <code>experimental</code>.</p> <p>The select statement will be executed periodically according to the <code>every</code> parameter. Each time the database is queried, the timestamps will be set according to <code>period</code>.</p> <p>Queries must start with the keyword <code>select/SELECT</code> or <code>with/WITH</code> and must contain the keyword <code>from/FROM</code> to be valid. </p>"},{"location":"nodes/database/crate_query.html#example","title":"Example","text":"<pre><code> def host = '10.14.204.8'\n def port = 5433 \n\n %% to escape single quotes (') we use double single quotes ('')\n def query = '\n  SELECT\n avg(data_obj[''x''][''cur'']) AS x_cur, avg(data_obj[''y''][''cur'']) AS y_cur,\n avg(data_obj[''z''][''cur'']) AS z_cur, avg(data_obj[''yaw''][''cur'']) AS yaw_cur,\n avg(data_obj[''pitch''][''cur'']) AS pitch_cur\n  FROM robotplc_parted;\n '\n\n def s =\n  |crate_query()\n  .query(query)\n  .group_by_time(3m)\n  .every(15s)\n  .period(30m)\n  .align()\n</code></pre> <p>The above example will execute the query every 15 seconds. It gets data which is in the timerange <code>now -30 minutes</code> and <code>now</code>.</p>"},{"location":"nodes/database/crate_query.html#parameters","title":"Parameters","text":"Parameter Description Default host( <code>string</code> ) CrateDB host config: <code>crate.host</code>/<code>FAXE_CRATE_HOST</code> port( <code>integer</code> ) CrateDB port config: <code>crate.port</code>/<code>FAXE_CRATE_PORT</code> tls( <code>boolean</code> ) whether to use tls config: <code>crate.tls.enable</code>/<code>FAXE_CRATE_TLS_ENABLE</code> user( <code>string</code> ) username config: <code>crate.user</code>/<code>FAXE_CRATE_USER</code> pass( <code>string</code> ) password config: <code>crate.pass</code>/<code>FAXE_CRATE_PASS</code> database( <code>string</code> ) Database name config: <code>crate.database</code>/<code>FAXE_CRATE_DATABASE</code> query( <code>string</code> <code>text</code> ) 'SELECT-FROM' query clause time_field( <code>string</code> ) name of the timefield to use 'ts' every( <code>duration</code> ) time between query execution 5s period( <code>duration</code> ) time span of data to query 1h align( is_set ) whether to align <code>period</code> to full <code>every</code> durations false (not set) group_by_time( <code>duration</code> ) group the aggregations into time buckets 2m group_by( <code>string_list</code> ) additional group by [] limit( <code>string</code> ) LIMIT statement '30'"},{"location":"nodes/database/crate_query_cont.html","title":"The crate_query_cont node","text":"<p>since 0.19.0</p> <p>Query the CRATE database for <code>time series data</code>. </p> <p>This node is used for continous timeseries queries.</p>"},{"location":"nodes/database/crate_query_cont.html#query","title":"Query","text":"<p>A select statement will be executed periodically, on every iteration a <code>timefilter</code> gets adjusted according to the <code>period</code> parameter. For this to work, the query given must contain the <code>$__timefilter</code> placeholder in the query's where clause:</p> <p>Queries must start with the keyword <code>select/SELECT</code> or <code>with/WITH</code> and must contain the keyword <code>from/FROM</code> to be valid.</p> <pre><code> def query = \n '\n    SELECT ts, id, temp1 FROM doc.table \n    WHERE $__timefilter \n    AND stream_id = 'dd419f94834a'\n    ORDER BY ts ASC\n '\n</code></pre> <p>The timefilter placeholder gets replaced by this statement:</p> <pre><code>ts &gt;= $1 AND ts &lt; $2\n</code></pre>"},{"location":"nodes/database/crate_query_cont.html#start","title":"Start","text":"<p>The <code>start</code> parameter determines the query start time. It's value is a past point in time.</p> <p>There are two possible ways to provide this:</p> <ul> <li>provide an ISO8601 Datetimestamp, ie: '2021-11-16T17:15:00.000Z'</li> <li>provide a query that results in 1 row with exactly 1 column named 'ts' containing an ISO8601 Datetimestamp.</li> </ul> <pre><code>SELECT DATE_FORMAT(ts) FROM table WHERE worked_on = false ORDER BY ts LIMIT 1\n</code></pre> <p>or with a fallback start-time</p> <pre><code>SELECT COALESCE(\n    (SELECT DATE_FORMAT(ts) FROM table WHERE worked_on = false ORDER BY ts LIMIT 1),\n'2021-11-16T16:20:00.000000Z'\n    )\nAS ts\n</code></pre>"},{"location":"nodes/database/crate_query_cont.html#historic-and-up-to-date-data","title":"Historic and up-to-date data","text":"<p>While reading data from the past, <code>min_interval</code> will be used to schedule the operation.</p> <p>Once the <code>timefilter</code> reaches present wall-clock time, the <code>offset</code> parameter will determine an amount of time to add to the scheduled time, that is now <code>period</code>. This is to account for late incoming data to the database.</p>"},{"location":"nodes/database/crate_query_cont.html#example","title":"Example","text":"<pre><code>def period = 1m\ndef sql = \n'SELECT ts, id, temp1 FROM doc.table \n WHERE $__timefilter AND stream_id = 'dd419f94834a'\n ORDER BY ts ASC\n'\n\n|crate_query_cont()\n.query(sql)\n.period(period)  \n.start('2021-11-16T16:03:42.040000Z')\n</code></pre> <p>The above example will execute the query periodically, emitting data_batch items with data_points worth of 1 minute. </p> <p><code>start</code> will be aligned to <code>period</code>, so that the <code>timefilter</code> will look like this for the first query:</p> <pre><code>ts &gt;= '2021-11-16T16:03:00.000Z' AND ts &lt; '2021-11-16T16:04:00.000Z'\n</code></pre>"},{"location":"nodes/database/crate_query_cont.html#parameters","title":"Parameters","text":"Parameter Description Default host( <code>string</code> ) CrateDB host config: <code>crate.host</code>/<code>FAXE_CRATE_HOST</code> port( <code>integer</code> ) CrateDB port config: <code>crate.port</code>/<code>FAXE_CRATE_PORT</code> tls( <code>boolean</code> ) whether to use tls config: <code>crate.tls.enable</code>/<code>FAXE_CRATE_TLS_ENABLE</code> user( <code>string</code> ) username config: <code>crate.user</code>/<code>FAXE_CRATE_USER</code> pass( <code>string</code> ) password config: <code>crate.pass</code>/<code>FAXE_CRATE_PASS</code> database( <code>string</code> ) Database name config: <code>crate.database</code>/<code>FAXE_CRATE_DATABASE</code> query( <code>string</code> ) 'SELECT' query with <code>$__timefilter</code> placeholder start( <code>string</code> ) <code>timefilter</code> start point .ISO8601 datetime string or query that retrieves an ISO8601 datetime string from the database stop( <code>string</code> ) <code>timefilter</code> stop point .ISO8601 datetime string or query that retrieves an ISO8601 datetime string from the database undefined stop_flow( <code>boolean</code> ) Whether to stop the whole flow, this node runs in, when stop time is reached. If this is false, then the node will just stop querying the database. true filter_time_field( <code>string</code> ) name of timestamp db column, used for <code>timefiler</code> 'ts' result_time_field( <code>string</code> ) name of result column, used for retrieving timestamps defaults to filter_time_field period( <code>duration</code> ) timefilter timespan, query boundaries will be aligned to this value 1h offset( <code>duration</code> ) offset at which the database is queried when the timefilter reached 'now' time 20s min_interval( <code>duration</code> ) minimum query-interval when the timefilter is in the past 5s extended_log( <code>boolean</code> ) Whether to switch on extended logging for this node config: <code>node.crate_query_cont.extended_log</code>/<code>FAXE_NODE_CRATE_QUERY_CONT_EXTENDED_LOG</code>"},{"location":"nodes/database/http_post_crate.html","title":"The http_post_crate node","text":"<p>renamed to <code>crate_out</code> .</p>"},{"location":"nodes/database/influx_out.html","title":"The influx_out node","text":"<p>Write data to InfluxDB via it's HTTP API. This node supports InfluxDB up to version 1.8.</p> <p>If any errors occur during the request, the node will attempt to retry sending.</p> <p>Since FAXE and InfluxDB share the notion of <code>tags</code>, this node will write all fields to InfluxDB fields and all tags as Influx tags.</p> <p>If you want to control which fields and tags get written to the database, use one of the flowdata-nodes, ie. use delete to delete some fields and/or tags before writing data with this node.</p> <p>Note: it is recommended to batch single data-points.</p> <p>Note: This node is a sink node and does not output any flow-data, therefore any node connected to this node will not get any data.</p>"},{"location":"nodes/database/influx_out.html#example","title":"Example","text":"<p>Simple:</p> <pre><code>|influx_out()\n.host('127.0.0.1')\n.port(8086)\n.measurement('m1')\n.database('mydb') \n</code></pre> <p>Use delete and batch before writing to InfluxDB:</p> <pre><code>|delete()\n.fields('calc.avg_temp')\n.tags('is_on', 'color')\n\n|batch(25)\n.timeout(3s)\n\n|influx_out()\n.host('127.0.0.1')\n.port(8086)\n.measurement('m1')\n.database('mydb') \n</code></pre>"},{"location":"nodes/database/influx_out.html#parameters","title":"Parameters","text":"Parameter Description Default host( <code>string</code> ) hostname or ip address of endpoint from config file port( <code>integer</code> ) port number from config file user( <code>string</code> ) username from config file pass( <code>string</code> ) password from config file tls( <code>is_set</code> ) whether to use tls ie. https false (not set) database( <code>string</code> ) database name measurement( <code>string</code> ) measurement name retpol( <code>string</code> ) <code>retention policy</code> to write to default"},{"location":"nodes/database/mongo_query.html","title":"The mongo_query node","text":"<p><code>Experimental</code>. </p> <p>Since 0.15.2</p> <p>Read data from MongoDB.</p> <p><code>Database</code>, <code>collection</code> and selector (<code>query</code>) can be specified by the user.</p> <p>The mongo_query node will always output <code>data_batch</code> items at the moment, even if there is only 1 result document.</p> <p>Note: Faxe is not optimized for heavy batch processing, it is rather designed for massive concurrent stream processing.  If you want to process more than a few thousand rows at a time, maybe faxe is not the right tool for your job.</p>"},{"location":"nodes/database/mongo_query.html#example","title":"Example","text":"<pre><code>def host = 'localhost'\n\n|mongo_query()\n.host(host)\n.user('root')\n.pass('root')\n.database('test')\n.collection('inventory')\n.query(' {\"item\": \"canvas\"} ')\n.every(5s)\n.as('data')\n</code></pre> <p>Every 5 seconds query mongo db on database test and collection inventory for documents, that have \"canvas\" as their values for the \"item\" field. Sets root field to 'data'.</p> <pre><code>def host = 'localhost'\n\n|value_emitter()\n.every(7s)\n\n|mongo_query()\n.host(host)\n.user('root')\n.pass('root')\n.database('test')\n.collection('inventory')\n.query(' {\"size.h\": {\"$gt\": 16}} ') \n</code></pre> <p>Every time a dataitem arrives at this node, from outside, it will query mongodb for documents that have a nested <code>h</code> field inside a <code>size</code> object, that has a value greater than 16. The node will query mongodb every 7 seconds, triggered by data coming in from the value_emitter node.</p>"},{"location":"nodes/database/mongo_query.html#parameters","title":"Parameters","text":"Parameter Description Default host ( <code>string</code> ) host name or ip address port ( <code>integer</code> ) 27017 user ( <code>string</code>) username pass ( <code>string</code> ) the users password database ( <code>string</code> ) mongo database name collection ( <code>string</code> ) mongo collection name in database <code>database</code> query ( <code>string</code> ) a valid <code>mongo selector</code> as a json string {} every ( <code>duration</code> ) read interval 5s align ( is_set ) whether to align the <code>every</code> parameter false (not set) as set the root path for outputs undefined"},{"location":"nodes/database/oracle_query.html","title":"The oracle_query node","text":"<p>Read data from OracleDB.</p>"},{"location":"nodes/database/oracle_query.html#example","title":"Example","text":"<pre><code>def host = 'my.oracle.host'\ndef port = 1521\ndef user = 'MY_ORACLE_USER'\ndef password = 'MY_ORACLE_PASS'\ndef service_name = 'MY.service'\n\ndef query = '\n    select * from room order by room_number\n'\n\n|oracle_query()\n.host(host)\n.port(port)\n.user(user)\n.pass(password)\n.service_name(service_name)\n.query(query)\n.every(10s)\n.align()\n</code></pre>"},{"location":"nodes/database/oracle_query.html#parameters","title":"Parameters","text":"Parameter Description Default host ( <code>string</code> ) host name or ip address port ( <code>integer</code> ) 1521 user ( <code>string</code>) username pass ( <code>string</code> ) the users password service_name ( <code>string</code> ) query ( <code>string</code> ) a valid sql select statement result_type ( <code>string</code> ) eighter 'batch' or 'point' 'batch' time_field ( <code>string</code> ) name of the time field every ( <code>duration</code> ) read interval 5s align ( is_set ) whether to align every false (not set)"},{"location":"nodes/database/postgre_statement.html","title":"The postgre_statement node","text":"<p><code>Experimental</code> since vs. 1.0.13</p> <p>Execute a statement against a postreSQL or compatible database.</p> <p>2 different execution modes are available:</p> <ul> <li>one shot:     This mode is the default mode, when the <code>every</code> parameter is not set.     The node will execute the given statement only once, possibly retrying on failure.     After the statement has been executed sucessfully, the node will emit a data item and then will do nothing from this point on.</li> <li>periodically:     With the <code>every</code> parameter given, the node will execute the provided SQL statement periodically.</li> </ul> <p>The execution of the statement can also be triggered by a data item that enters the node (param <code>start_on_trigger</code> set to true).  But the execution mode will remain the same, in this case.</p> <p>Note: The statement can be anything from SELECT, INSERT, DELETE or even any schema manipulation. Handle with care !</p>"},{"location":"nodes/database/postgre_statement.html#example","title":"Example","text":"<pre><code>def source_schema = 'doc'\ndef source_table = '0x145f'\n%% the SHOW CREATE TABLE statement is supported by CrateDB, not by Postgre itself\ndef table_statement = 'SHOW CREATE TABLE \"{{ source_schema }}\".\"{{ source_table }}\"' \n\n%% statement to get the table create statement\n|postgre_statement()\n.host('127.0.0.1')\n.statement(table_statement) \n.retries(2)\n.result_type('point')\n</code></pre> <p>Executes the SHOW CREATE TABLE statement and emits a datapoint with a field that holds the result of the statement</p> <p>Because we can execute a statement from an incoming data item, we could use the result of the first example, to copy our table to a different database. So we extend our example by adding a second postgre_statement node that uses the result:</p> <pre><code>def source_schema = 'doc'\ndef source_table = '0x145f'\ndef table_statement = 'SHOW CREATE TABLE \"{{ source_schema }}\".\"{{ source_table }}\"' \ndef fieldname = 'SHOW_CREATE_TABLE_{{ source_schema }}_{{ source_table }}'\n\n%% statement to get the table create statement\n|postgre_statement()\n.host('127.0.0.1')\n.statement(table_statement)  \n.result_type('point')\n\n%% here we use the result of the SHOW CREATE TABLE statement\n|postgre_statement()\n%% as statement we use the value of the incoming data point\n.host('192.168.2.1')\n.statement_field(fieldname)   \n.start_on_trigger(true)\n</code></pre>"},{"location":"nodes/database/postgre_statement.html#parameters","title":"Parameters","text":"Parameter Description Default host( <code>string</code> ) hostname or ip address of endpoint config: <code>crate.host</code>/<code>FAXE_CRATE_HOST</code> port( <code>integer</code> ) port number config: <code>crate.port</code>/<code>FAXE_CRATE_PORT</code> tls( <code>boolean</code> ) whether to use tls for the connection config: <code>crate.tls.enable</code>/<code>FAXE_CRATE_TLS_ENABLE</code> user( <code>string</code> ) username config: <code>crate.user</code>/<code>FAXE_CRATE_USER</code> pass( <code>string</code> ) password config: <code>crate.pass</code>/<code>FAXE_CRATE_PASS</code> statement( <code>string</code> ) SQL statement, that should be executed. undefined statement_field( <code>string</code> ) Name of the field, that holds the SQL statement, that should be executed. <code>start_on_trigger</code> must be set to true in order to use this feature. undefined every( <code>duration</code> ) Interval at which to execute the statement (periodically), if not given, the node is in one-shot mode. undefined result_type( <code>string</code> ) Type of the resulting data item, 'batch' or 'point' 'batch' start_on_trigger( <code>boolean</code> ) if true, the node waits for an incoming data item before sending the statement to the database, this must be set to true, when <code>statement_field</code> is used false retries( <code>integer</code> ) max retry attempts, when a statement fails 2 <p>Either <code>statement</code> or <code>statement_field</code> must be given.</p>"},{"location":"nodes/debug/conn_status.html","title":"The conn_status node","text":"<p>Subscribe to internal connection status events of running tasks. For more information on these events make yourself familiar with faxe's metrics.</p>"},{"location":"nodes/debug/conn_status.html#example","title":"Example","text":"<pre><code>%% get connection status events for task \"flow1\" and node \"amqp_publish13\"\n|conn_status()\n.flow('flow1')\n.node('amqp_publish13') \n\n%% get connection status events for every node in task \"flow1\"  \n|conn_status()\n.flow('flow1') \n</code></pre>"},{"location":"nodes/debug/conn_status.html#parameters","title":"Parameters","text":"Parameter Description Default flow( <code>string</code> ) Id of task node( <code>string</code> ) Id of node undefined"},{"location":"nodes/debug/debug.html","title":"The debug node","text":"<p>The debug node logs all incoming data with erlang's <code>lager</code> framework and emits it, without touching it. Where the logs will be written, depends on the <code>lager</code> config.</p> <p>The debug message will include the current data-item converted to a string.</p> <p>See rest api for how to read the produced logs.</p>"},{"location":"nodes/debug/debug.html#example","title":"Example","text":"<pre><code>|debug()\n\n\n|debug('error')\n\n\n%% [since 0.19.13]\n|debug('warning')\n.where(lambda: empty(\"data.topic\") OR empty(\"data.stream_id\"))\n.message('Topic or StreamId is empty!')\n</code></pre>"},{"location":"nodes/debug/debug.html#parameters","title":"Parameters","text":"Parameter Description Default [node] level (<code>string</code>) log level (see below) 'notice' message (<code>string</code>) [since 0.19.13] custom message that is written to the log '' where (<code>lambda</code>) [since 0.19.13] lambda expression, if evaluates as true, then logging will be performed undefined <p>The level parameter must have one of the following values:</p> log_level debug info notice warning error critical alert"},{"location":"nodes/debug/json_emitter.html","title":"The json_emitter node","text":"<p>This node is used to produce data, that can be processed in a flow.</p> <p>It can be used for debugging and testing/mocking purposes as well as produce default values for different kind of applications.</p> <p>Starting with a given <code>list of JSON strings</code> (json structures in string format), this data can be emitted and/or modified on every  emit in different ways:</p> <ul> <li>determine, how data for output gets <code>selected</code> from the list of json inputs, see also select</li> <li>modify the selected data with <code>modifier</code> lambda functions</li> <li>output periodically with <code>every</code>, possibly adding a timing <code>jitter</code>, or have exactly 1 output with the <code>one-shot</code> parameter</li> </ul> <p>experimental since 0.19.9:     With the <code>modify</code> and <code>modify_with</code> parameters, certain fields in the json string(s) can be modified with every interval.</p>"},{"location":"nodes/debug/json_emitter.html#examples","title":"Examples","text":"<pre><code>|json_emitter(\n  ' {\"condition\": {\"id\": 0, \"name\": \"OK\"}, \"condition_reason\": \"\"} ',\n  ' {\"condition\": {\"id\": 1, \"name\": \"Warning\"}, \"condition_reason\": \"bad succer\"} ',\n  ' {\"condition\": {\"id\": 2, \"name\": \"Error\"}, \"condition_reason\": \"something went really wrong!\"} '\n  )\n</code></pre> <p>Emit one of the three given json strings(selected randomly) every 5 seconds (default).</p> <pre><code>|json_emitter(\n  ' {\"condition\": {\"id\": 0, \"name\": \"OK\"}, \"condition_reason\": \"\"} ',\n  ' {\"condition\": {\"id\": 1, \"name\": \"Warning\"}, \"condition_reason\": \"bad succer\"} ',\n  ' {\"condition\": {\"id\": 2, \"name\": \"Error\"}, \"condition_reason\": \"something went really wrong!\"} ' \n  )\n.every(3s)\n.select('seq')\n</code></pre> <p>Select one of the json strings in sequence every 3 seconds, starting with the first one. After the last json string has been selectect and outputted, start over with the first one again. Produces data-point items.</p>"},{"location":"nodes/debug/json_emitter.html#using-modifiers","title":"Using modifiers","text":"<pre><code>|json_emitter(\n  ' {\"Current_Ph_A\": 10.33, \"Current_Ph_B\": 10.53, \"Current_Ph_C\": 10.76} ' \n  )\n.every(200ms)\n.align(true)\n.modify(\n    'data.Current_Ph_A',\n    'data.Current_Ph_B', \n    'data.Current_Ph_C'\n     )\n.modify_with(\n    lambda: random_normal(), \n    lambda: \"data.Current_Ph_A\" + 0.45, \n    lambda: nth(random(4), [12.552, 44.2, 13.86, 16.22])\n    )\n.as('data') \n</code></pre> <p>Output the json data every 200ms, modifying it on every emit with the given lambda functions. Note the 'as' parameter here: It gives the output data a new root-object ('data') and the modifiers must target the 'rooted' fields.</p> <p>Sample output:</p> <pre><code>{\"ts\":  1675147412200, \"data\":  \n  {\"Current_Ph_A\": 1.6554763679823647, \"Current_Ph_B\": 2.105476367982365, \"Current_Ph_C\": 44.2}\n}\n</code></pre>"},{"location":"nodes/debug/json_emitter.html#using-__state-in-modifier","title":"Using __state in modifier","text":"<p><code>__state</code> contains the data_point, that was emitted previously, we can implement a simple counter with that for example:</p> <pre><code>|json_emitter('{\"field1\": 1}')\n.modify('field1')\n.modify_with(lambda: \"__state.field1\" + 1)\n</code></pre> <p>Output is a data_point with an ever increasing value for <code>field1</code>.</p>"},{"location":"nodes/debug/json_emitter.html#batch-output-and-one_shot","title":"Batch output and one_shot","text":"<pre><code>def start_time = '2021-11-16T16:21:00.000Z'\n|json_emitter(\n  ' {\"Current_Ph_A\": 10.33, \"Current_Ph_B\": 10.53, \"Current_Ph_C\": 10.76} ',\n  ' {\"Current_Ph_A\": 11.13, \"Current_Ph_B\": 10.74, \"Current_Ph_C\": 11.17} ',\n  ' {\"Current_Ph_A\": 13.78, \"Current_Ph_B\": 11.02, \"Current_Ph_C\": 11.12} ',\n  ' {\"Current_Ph_A\": 12.46, \"Current_Ph_B\": 10.98, \"Current_Ph_C\": 11.71} '\n  )\n.select('batch')\n.one_shot(true)\n.start_ts(start_time)\n</code></pre> <p>Output a data-batch item (using all the  json items at once) exactly one time (one-shot), using <code>start_time</code> as timestamp. After the first output, the node will go to sleep for the rest of its life.</p>"},{"location":"nodes/debug/json_emitter.html#parameters","title":"Parameters","text":"Parameter Description Default [node] json(<code>string_list</code>) list of json strings every(<code>duration</code>) emit interval 5s jitter(<code>duration</code>) max random value for time jitter added to every, adds time jitter to the values produced 0ms align(<code>bool</code>) align the time to the every param, does not have an effect, if <code>start_ts</code> is given. false select(<code>string</code>) entry select mode, possible values: <code>rand</code> or <code>seq</code>, <code>batch</code> 'rand' start_ts(<code>string</code> or <code>int</code>) Timestamp to start from, instead of wall clock time (default). ISO8601 datetime string or millisecond timestamp allowed here. '2021-11-16T16:21:12.505Z' modify(<code>string_list</code>) list of fields, that should be modified undefined modify_with(<code>lambda_list</code>) list of lambda expressions, that perform the modification on the fields given with <code>modify</code> undefined as(<code>string</code>) root object used for output items undefined one_shot(<code>bool</code>) when set to true, the node gives exactly 1 output, before it goes to sleep false <p>Note : <code>modify</code> and <code>modify_with</code> must have the same number of parameters, if given.</p>"},{"location":"nodes/debug/json_emitter.html#select","title":"select","text":"value Description Resulting data-item <code>rand</code> randomly selects one of the json structures data-point <code>seq</code> selects the json entries in sequence starting from top data-point <code>batch</code> selects all of the entries resulting in a batch of data-points data-batch"},{"location":"nodes/debug/log.html","title":"The log node","text":"<p>Log incoming data to a file in json format (line by line).</p> <p>That means, every incoming data-item will be converted to a json string and then appended as a new line to the given file.</p> <p>The node tries to ensure, that all parent directories for the specified file exist, trying to create them, if necessary.</p> <p>Note: Only single data_points will be written to the file. If the node is fed data_batch items, they will be unbatched to single data_points first.</p>"},{"location":"nodes/debug/log.html#example","title":"Example","text":"<pre><code>|log('topics.txt') \n</code></pre>"},{"location":"nodes/debug/log.html#parameters","title":"Parameters","text":"Parameter Description Default [node] file( <code>string</code> ) valid writeable or creatable filepath"},{"location":"nodes/debug/metrics.html","title":"The metrics node","text":"<p>Subscribe to internal metric events of running tasks. For more information on these events make yourself familiar with faxe's metrics.</p> <p>Note: It is not possible to subscribe to metrics for the task the metrics node belongs to.</p>"},{"location":"nodes/debug/metrics.html#example","title":"Example","text":"<pre><code>%% get all metrics for task \"flow1\" and node \"amqp_publish13\"\n|metrics()\n.flow('flow1')\n.node('amqp_publish13')\n\n\n%% get total number of bytes read and written for task \"flow32\"\n|metrics()\n.flow('flow32')\n.metrics('bytes_read', 'bytes_sent')\n</code></pre>"},{"location":"nodes/debug/metrics.html#parameters","title":"Parameters","text":"Parameter Description Default flow( <code>string</code> ) Id of task node( <code>string</code> ) Id of node undefined metrics( <code>string_list</code> ) List of metric_names undefined"},{"location":"nodes/debug/value_emitter.html","title":"The value_emitter node","text":"<p>This node is for debugging purposes.</p> <p>It periodically emits random values.</p>"},{"location":"nodes/debug/value_emitter.html#example","title":"Example","text":"<pre><code>|value_emitter()\n.every(1s)\n.type(point)\n</code></pre> <p>Emit a data_point with a random value in field val every second.</p>"},{"location":"nodes/debug/value_emitter.html#parameters","title":"Parameters","text":"Parameter Description Default every( <code>duration</code> ) emit interval 5s jitter( <code>duration</code> ) add time jitter to the values produced 0ms type( <code>atom</code> ) emit point or batch batch fields( <code>string_list</code> ) what fields to emit ['val'] format( <code>atom</code> ) the format of the fields emitted flat/ejson flat align( is_set ) align the time to the every param false (not set)"},{"location":"nodes/flow/collect_unique.html","title":"The collect_unique node","text":"<p><code>Experimental</code>.</p> <p>With this node we can collect a unique set of values from data_points based on a given field's value.</p> <p>For every different value of the <code>key-field</code>, the node will cache the last data_point with that value.</p> <p>This node is useful, if you have multiple data-streams - all share a field called the <code>key-field</code> -  that you want to condense into one data_point, according to that key-field's value; One can think of it as sort of an \"un-group\" function.</p> <p>Note: This node produces a completely new data_point.</p> <p>Given the name of a <code>key-field</code>, this node collects data_points using the value of this field to group and cache every data_point. Once the <code>min_vals</code> count of unique values is reached in internal buffer, it starts emitting every change within this set of values.</p> <p>New data_points, which have the same value for the key-field as seen before, will overwrite old values.</p> <p>Data_points that do not have the key-field present, will be ignored. On output, the node will condense the collected data_points into one, where all the data_points' fields are grouped by the value of the key-field.</p> <p>Note: The number of uniquely collected values will grow, but never shrink (at the moment).</p> <p>Also note: Produced data may become very large, if the value of the <code>key-field</code> is ever-changing, so that the node will cache a lot of data and therefore may use a lot of memory, be aware of that !</p>"},{"location":"nodes/flow/collect_unique.html#examples","title":"Examples","text":"<pre><code>|collect_unique('data.key_field')\n.min_vals(5)\n.keep('data.values.node_id', 'data.values.request_ref')\n.as('node_id', 'request_ref')\n</code></pre> <p>In the above example the node will collect the fields: \"data.values.node_id\" and \"data.values.request_ref\" from every data_point that has a field called \"data.key_field\". As soon as it has collected 5 different values for \"data.key_field\" it will emit those collected values as a new data_point with the two fields called \"node_id\" and \"request_ref\".</p>"},{"location":"nodes/flow/collect_unique.html#parameters","title":"Parameters","text":"Parameter Description Default [node] field( <code>string</code> ) path to the key-field min_vals( <code>integer</code> ) number of different items collected before first output starts 1 keep( <code>string_list</code> ) values to keep from every data_point as( <code>string_list</code> ) output names for the keep values [] max_age ( <code>duration</code> ) max age for every collected entry undefined"},{"location":"nodes/flow/combine.html","title":"The combine node","text":"<p>Combine the values from 2 nodes, used to enrich a stream of data with data from another stream, that usually has lower frequency.</p> <p>Port 1 is the trigger port and its the port where data to be enriched comes into the node.  Port 2 is the one where enrichment data come in. Every time a value is received on the trigger port, the node will emit a value, combined with whatever current value on port 2. The node will never emit on port 2 values.</p> <p>No output is given, as long as there has not arrived a value on port 2 to combine with.</p>"},{"location":"nodes/flow/combine.html#fields","title":"fields","text":"<p>The <code>fields</code> parameter defines the fields to inject into the combination for the stream on port 2. To rename these fields, parameter <code>prefix</code> or <code>aliases</code> can be used. With <code>prefix_delimiter</code> a delimiter can be given, defaults to: <code>'_'</code></p>"},{"location":"nodes/flow/combine.html#merge","title":"merge","text":"<p>When <code>merge_field</code> is given, the node will merge the values from the input port 2 with the values from port 1. Objects and lists and lists of objects will be merged.</p> <p>If a path exists in both streams, the value in the first stream is superseded by the value in the second stream (in2). Except for lists, which will be combined.</p> <p>Either fields(optionally with <code>prefix</code> or <code>aliases</code>) or merge_field must be given. </p> <p>If you want to join 2 or more streams consider using the join node.</p>"},{"location":"nodes/flow/combine.html#examples","title":"Examples","text":"<pre><code> def in1 =\n  |value_emitter()\n  .every(500ms)\n  .type(point)\n  .fields('val')\n\n def in2 =\n  |value_emitter()\n  .every(4s)\n  .type(point)\n  .fields('val2', 'val3') \n\n in1\n  |combine(in2)\n  .fields('val2', 'val3')\n  .prefix('comb')\n  .prefix_delimiter('_')\n</code></pre> <p>In this example values from the stream called <code>in1</code> will be enriched with values from <code>in2</code>. Outputfields will be called: <code>val</code>, <code>comb_val2</code> and <code>comb_val3</code> . The flow will emit every 500 milliseconds after 4 seconds have past initially.</p> <pre><code> def in1 =\n  |value_emitter()\n  .every(500ms)\n  .type(point)\n  .fields('data.val')\n\n def in2 =\n  |value_emitter()\n  .every(4s)\n  .type(point)\n  .fields('data.val2', 'data.val3') \n\n in1\n  |combine(in2)\n  .merge_field('data') \n</code></pre> <p>This example will merge data from <code>in2</code> into <code>in1</code>, such that the resulting data-point will have  the fields: <code>data.val</code>, <code>data.val2</code>, <code>data.val3</code></p> <pre><code>def v1 =\n|json_emitter()\n.every(1s)\n.json(' {\"condition\": {\"id\": 0, \"name\": \"OK\", \"sub_cond\":\n        [{\"value\": 33}]}, \"condition_reason\": \"\",\n        \"predicted_maintenance_time\": 1584246411783,\n        \"vac_on_without_contact\": [1.2, 2.5, 4.33]} ')\n\ndef v2 =\n|json_emitter()\n.every(5s)\n.json(' {\"condition\": {\"id1\": 0, \"name1\": \"OK\", \"sub_cond\":\n        [{\"number\": 44}]}, \"condition_reason\": \"\",\n        \"predicted_maintenance_time\": 1584246411785,\n        \"vac_on_without_contact\": [2.2, 2.5, 4.33],\n        \"vac_on_with_contact\": [5.6, 45.98, 7.012]} ')\n\nv1\n    |combine(v2)\n    .merge_field('data') \n</code></pre> <p>The output from the above example will be:</p> <pre><code>    {\"data\":\n        {\"condition\": {\"id1\":0,\"id\":0,\"name1\":\"OK\",\"name\":\"OK\",\n\"sub_cond\":[{\"number\":44},{\"value\":33}]},\n\"condition_reason\":\"\",\n\"predicted_maintenance_time\":1584246411785,\n\"vac_on_without_contact\":[1.2,2.2,2.5,2.5,4.33,4.33],\n\"vac_on_with_contact\":[5.6,45.98,7.012]\n        }\n</code></pre>"},{"location":"nodes/flow/combine.html#parameters","title":"Parameters","text":"Parameter Description Default [node] (<code>port</code>) input node for port 2 merge_field( <code>string</code> ) Base field for the merge operation [] fields( <code>string_list</code> ) List of fields to include [] tags( <code>string_list</code> ) List of tags to include [] aliases( <code>string_list</code> ) List of field aliases to use instead of the original field names [] prefix( <code>string</code> ) Prefix for the injected fields from stream 2 undefined prefix_delimiter( <code>string</code> ) Used to separate prefix and the original field name from stream 2 '_' nofill ( <code>isset</code> ) if set, dataoutput will happen regardless of a initial input on port 2 false (not set) <p>When <code>merge_field</code> is given the params fields, prefix and prefix_delimiter have no effect. Otherwise either <code>prefix</code> or <code>aliases</code> must be given these are mutually exclusive parameters.  If both are given, then <code>prefix</code> will win.</p>"},{"location":"nodes/flow/group_by.html","title":"The group_by node","text":"<p>The <code>group_by</code> node is used to group a stream of data by the values of one or more fields. Each group is then processed independently and concurrently to the other groups, for the rest of the chain (subgraph).</p> <p>Note: The behaviour of using more than 1 <code>group_by</code> node within a flow is not defined.</p> <p>See group_union for how to 'un-group' a dataflow.</p> <p>Be aware of high group cardinality, as for every group, a number of processes (depends on the size of the grouped sub-flow)  will be started in the dataflow engine. In other words, if you have a grouping that has high cardinality (many different values), more resources will be consumed.</p>"},{"location":"nodes/flow/group_by.html#examples","title":"Examples","text":"<pre><code>  |group_by('fieldname1', 'fieldname2') \n</code></pre> <p>Groups data along two dimensions: <code>fieldname1</code> and <code>fieldname2</code>.</p> <pre><code>def group_field = 'data.code'\n |json_emitter()\n .every(700ms)\n .json(\n     '{\"code\" : 224, \"message\": \"this is a test\", \"mode\": 1}',\n     '{\"code\" : 334, \"message\": \"this is another test\", \"mode\": 1}',\n     '{\"code\" : 114, \"message\": \"this is another test\", \"mode\": 2}',\n     '{\"code\" : 443, \"message\": \"this is another test\", \"mode\": 1}',\n     '{\"code\" : 224, \"message\": \"this is another test\", \"mode\": 2}',\n     '{\"code\" : 111, \"message\": \"this is another test\", \"mode\": 1}',\n     '{\"code\" : 551, \"message\": \"this is another test\", \"mode\": 2}'\n )\n .as('data')\n\n|group_by(group_field)\n|eval(\n    lambda: str_replace(\"data.message\", 'test', string(\"data.code\"))\n    )\n    .as('data.message')\n|debug()\n</code></pre> <p>In the above example data is grouped by the <code>code</code> field, so eventually the <code>group_by</code> node will start 6 computing sub-graphs to handle all groups. A computing sub-graph in this example will contain an <code>eval</code> node connected to a <code>debug</code> node.</p>"},{"location":"nodes/flow/group_by.html#parameters","title":"Parameters","text":"Parameter Description Default [node] fields( <code>string_list</code> ) fieldnames to group by lambda( <code>lambda</code> ) use a function to group the data-items (experimental) undefined"},{"location":"nodes/flow/group_union.html","title":"The group_union node","text":"<p>This node is used to terminate and 'un-group' a grouped dataflow. It must be placed <code>after a group_by</code> node.</p> <p>The group_union node acts as a union for grouped dataflows.</p> <p>If this node is used without a <code>group_by</code> node, it will have no effect at all on the data-flow.</p> <p>Note: The behaviour of using more than 1 <code>group_union</code> node within a flow is not defined.</p> <p>See group_by </p>"},{"location":"nodes/flow/group_union.html#examples","title":"Examples","text":"<pre><code>  |group_by('fieldname1')\n  %% 1 debug-node per group \n  |debug()\n  |group_union()\n  %% end of grouping\n  |debug()\n</code></pre> <p>An instance of the first debug node wil be started for every group, the second one will exist only once.</p>"},{"location":"nodes/flow/join.html","title":"The join node","text":"<p>Join data from two or more nodes.</p> <p>If the <code>merge_field</code> parameter is given, the node will merge the fields given from every in-node, instead of joining with prefixes. (See merge example below).</p> <p>If you want to enrich a stream of data with a second stream consider using the combine node.</p>"},{"location":"nodes/flow/join.html#example","title":"Example","text":"<pre><code>def v1 =\n|value_emitter()\n    .every(3s) \n    .align()\n\ndef v2 =\n|value_emitter()\n    .every(5s) \n    .align()\n\nv1\n|join(v2)\n    .prefix('v1.joined', 'v2.joined')\n    .tolerance(3s)\n    .missing_timeout(3s) \n</code></pre> <p>Joins the fields of <code>v1</code> and <code>v2</code> and produces a stream, that has the fields <code>v1.joined.val</code> and <code>v2.joined.val</code></p>"},{"location":"nodes/flow/join.html#node-parameters","title":"Node Parameters","text":"Parameter Description Default [node] nodes( <code>node_list</code> ) list of node (chains) to merge []"},{"location":"nodes/flow/join.html#parameters","title":"Parameters","text":"Parameter Description Default prefix( <code>string_list</code> ) list of prefixes (used in join mode) ['', ''] (no prefixes) merge_field( <code>string</code> ) when given, the join node will do a field merge operation undefined missing_timeout( <code>duration</code> ) values that do not arrive within this timeout will be treated as missing 20s tolerance( <code>duration</code> ) timestamp tolerance. Determines the maximum difference a data-item's timestamp can have to the current timeslot, for the item to be included in the join operation. 2s ~fill( 'none' 'null' <code>any</code> ) deprecated, use <code>full</code> instead, the nodes default behaviour stayed the same 'none' full( <code>boolean</code> ) whether to output full joins (no value missing) only, this would drop joins that are not complete true"},{"location":"nodes/flow/join.html#fill-value-join-behaviour-deprecated","title":"fill value - join behaviour (deprecated)","text":"<ul> <li><code>'none'</code> - (default) skip rows, where a point is missing, inner join.</li> <li><code>'null'</code> - fill missing points with null, full outer join.</li> <li><code>Any value</code> - fill fields with given value, full outer join.</li> </ul>"},{"location":"nodes/flow/join.html#merge-example","title":"Merge example","text":"<p>Let's look at an example where the streams coming out of two nodes are not joined with prefixes, but a merge operation is performed. </p> <pre><code>def v1 =\n|json_emitter()\n.every(3s)\n.json(' {\"condition\": {\"id\": 0, \"name\": \"OK\", \"sub_cond\":\n     [{\"value\": 33}]}, \"condition_reason\": \"\",\n     \"predicted_maintenance_time\": 1584246411783,\n     \"vac_on_without_contact\": [1.2, 2.5, 4.33]} ')\n.as('data')\n\ndef v2 =\n|json_emitter()\n.every(3s)\n.json(' {\"condition\": {\"id1\": 0, \"name1\": \"OK\", \"sub_cond\":\n     [{\"number\": 44}]}, \"condition_reason\": \"\",\n     \"predicted_maintenance_time\": 1584246411783,\n     \"vac_on_without_contact\": [2.2, 2.5, 4.33],\n     \"vac_on_with_contact\": [5.6, 45.98, 7.012]} ')\n.as('data')\n\n\nv1\n|join(v2)\n.merge_field('data')\n.tolerance(20ms)\n.missing_timeout(30ms)\n.full(false)\n\n|debug()\n</code></pre>"},{"location":"nodes/flow/join.html#v1-node-data-field-in-json-format-for-readability","title":"v1 node data-field (in json format for readability):","text":"<pre><code>{\"condition\": {\"id\": 0, \"name\": \"OK\", \"sub_cond\":\n            [{\"value\": 33}]}, \"condition_reason\": \"Reason\",\n\"predicted_maintenance_time\": 1584246411783,\n\"vac_on_without_contact\": [1.2, 2.5, 4.33]\n            }\n</code></pre>"},{"location":"nodes/flow/join.html#v2-node-data-field","title":"v2 node data-field:","text":"<pre><code>{\"condition\": {\"id1\": 0, \"name1\": \"OK\", \"sub_cond\":\n            [{\"number\": 44}]}, \"condition_reason\": \"\",\n\"predicted_maintenance_time\": 1584246411785,\n\"vac_on_without_contact\": [2.2, 2.5, 4.33],\n\"vac_on_with_contact\": [5.6, 45.98, 7.012]\n            }\n</code></pre> <p>The result data-field after merge (json format here):</p> <pre><code>{\"condition\":\n         {\"name1\":\"OK\",\"name\":\"OK\",\"id1\":0,\"id\":0,\n\"sub_cond\":[{\"number\":44}, {\"value\":33}]\n         },\n\"predicted_maintenance_time\":1584246411785,\n\"vac_on_without_contact\":[1.2,2.2,2.5,2.5,4.33,4.33],\n\"vac_on_with_contact\":[5.6,45.98,7.012],\n\"condition_reason\":\"\"\n         }\n</code></pre> <p>Objects and lists and lists of objects will be merged.</p> <p>If a path exists in several streams, the value in the first stream is superseded by the value in a following stream (\"condition_reason\" and \"predicted_maintenance_time\" in this example). Except for lists, which will be merged (\"vac_on_without_contact\").</p>"},{"location":"nodes/flow/union.html","title":"The union node","text":"<p>Union of multiple streams.</p> <p>The union node takes the union of all of its parents as a simple pass through.</p> <p>Data items received from each parent are passed onto child nodes without modification.</p>"},{"location":"nodes/flow/union.html#example","title":"Example","text":"<pre><code>in1\n  |union(in2, in3) \n</code></pre> <p>The union of 3 nodes (chain expressions)</p> <pre><code>def in1 = \n|mqtt_subscribe()\n.host ... \n\ndef in2  = \n|amqp_consume()\n.host ....\n\ndef in3 = \n...\n\nin1\n  |union(in2, in3) \n</code></pre> <p>with chain expressions</p>"},{"location":"nodes/flow/union.html#parameters","title":"Parameters","text":"Parameter Description Default [node] nodes_in( <code>node_list</code> ) optional"},{"location":"nodes/flowdata/batch.html","title":"The batch node","text":"<p>Used to batch a number of points.  As soon as the node has collected <code>size</code> number of points it will emit them in a data_batch.</p> <ul> <li>A timeout can be set, after which all points currently in the buffer will be emitted, regardless of the number of collected points.</li> <li>The timeout is started on the first datapoint coming in to an empty buffer.</li> </ul>"},{"location":"nodes/flowdata/batch.html#example","title":"Example","text":"<pre><code>|batch(12)\n\n|batch(5)\n.timeout(3s)\n</code></pre> <p>The second example will output a batch with a maximum of 5 points.  A data-batch message will be emitted, if either 5 points have been collected  or 3 seconds have past since the first data-point came in.</p>"},{"location":"nodes/flowdata/batch.html#parameters","title":"Parameters","text":"Parameter Description Default [node] size( <code>integer</code> ) Number of points to batch timeout( <code>duration</code> ) 1h"},{"location":"nodes/flowdata/default.html","title":"The default node","text":"<p>Add fields and/or tags to a data_point or batch if they do not already exist. Does not overwrite or update any fields or tags.</p> <p>Note:  This nodes checks for existence of fields before writing them.  Consider using the set node, if you just want some fields set. It is more performant especially with high frequency data streams.</p>"},{"location":"nodes/flowdata/default.html#examples","title":"Examples","text":"<pre><code>|default()\n.fields('id', 'vs', 'df')\n.field_values('some_id', 1, '05.043')\n</code></pre> <p>The above example will set the field <code>id</code> to the value 'some_id' , if a field with the name <code>id</code> does not already exist. Accordingly <code>vs</code> will be set to 1, <code>df</code> will be set to '05.043'.</p> <pre><code>|default()\n.fields('id', 'vs', 'df')\n.field_values(25.44)\n</code></pre> <p>Since v0.19.41: If exactly 1 value is given for <code>field_values</code>, it is used for every field name given.</p>"},{"location":"nodes/flowdata/default.html#parameters","title":"Parameters","text":"Parameter Description Default fields( <code>string_list</code> ) list of fieldnames [] field_values( <code>list</code> ) list of values for the given fields (exactly one entry or same length as fieldnames) [] tags( <code>string_list</code> ) list of tagnames [] tag_values( <code>list</code> ) list of values for the given tags (exactly one or same length as tagnames) []"},{"location":"nodes/flowdata/delete.html","title":"The delete node","text":"<p>Delete fields and/or tags from a data_point or from all data_points in a data_batch.</p> <p>Optionally a lambda function can be given to perform deletion only on those data_points for which the function returns <code>true</code>.</p>"},{"location":"nodes/flowdata/delete.html#example","title":"Example","text":"<pre><code>|delete()\n.fields('temp', 'data.meta[3]')\n</code></pre> <p>The above example will delete the field named <code>temp</code> and the third array entry of the field <code>data.meta</code> .</p>"},{"location":"nodes/flowdata/delete.html#conditional","title":"Conditional","text":"<pre><code>|delete()\n.fields('temp', 'data.meta[3]')\n.where(lambda: \"data.condition.id\" == 2 OR \"data.condition.name\" == 'warning')\n</code></pre> <p>The above example will delete the field named <code>temp</code> and the third array entry of the field <code>data.meta</code> only if the current point has a field \"data.condition.id\" with value 2, or it has the field \"data.condition.name\" with value 'warning'.</p>"},{"location":"nodes/flowdata/delete.html#parameters","title":"Parameters","text":"Parameter Description Default fields( <code>string_list</code> ) list of fieldnames to delete [] tags( <code>string_list</code> ) list of tagnames to delete [] where( <code>lambda</code> ) lambda function for conditional deleting undefined when_value( <code>any</code> ) delete a field only, if it has the given value undefined"},{"location":"nodes/flowdata/keep.html","title":"The keep node","text":"<p>Keep only those fields and tags specified by the parameters.</p>"},{"location":"nodes/flowdata/keep.html#example","title":"Example","text":"<pre><code>|keep('data.topic', 'data.temperature')\n.as('topic', 'temperature')\n</code></pre>"},{"location":"nodes/flowdata/keep.html#parameters","title":"Parameters","text":"Parameter Description Default [node] fields( <code>string_list</code> ) list of fieldnames to keep from the incoming data tags( <code>string_list</code> ) list of tagnames to keep from the incoming data [] as( <code>string_list</code>) list of new field names for the kept fields, if given, must have the same count of names as <code>fields</code> []"},{"location":"nodes/flowdata/multi_map.html","title":"The multi_map node","text":"<p>Maps fields from a json lookup table with data from the current data_item. This node can basically do what multiple select_first expression can do:</p>"},{"location":"nodes/flowdata/multi_map.html#example","title":"Example","text":"<pre><code>def variables = '[{\"mod_num\":4233, \"alarm_name\": \"E1_failed\", \"alarm_text\": \"Error\", \"type\": \"E1\"}, ...]'\n\n|multi_map()\n.fields('module', 'alarm')                    \n.match_fields('mod_num', 'alarm_name')        % Match these fields from the look-up table \n.select_fields('alarm_text', 'type')    \n.lookup(variables)                                  \n\n% is equivalent to:\n\n|eval(\n    lambda: select_first('alarm_text', [{'mod_num', \"module\"}, {'alarm_name', \"alarm\"}], variables),\n    lambda: select_first('type', [{'mod_num', \"module\"}, {'alarm_name', \"alarm\"}], variables)\n)\n.as('alarm_text', 'type')\n</code></pre> <p>Here we match 'module_number' and 'alarm_name' in the lookup table against the values of the data_point fields 'module' and 'alarm'.  From the resulting object we select two fields 'alarm_text', 'alarm_type' and add the to them data_point.</p>"},{"location":"nodes/flowdata/multi_map.html#parameters","title":"Parameters","text":"Parameter Description Default fields( <code>string_list</code> ) list of fieldnames from the current data_point to match data in the lookup table match_fields( <code>string_list</code> ) list of fields in the lookup table to match the values of <code>fields</code> select_fields( <code>string_list</code> ) select this fields from the lookup for output lookup( <code>string</code> ) json object list in string format as( <code>string_list</code>) list of new field names for the selected fields, if given, must have the same count of names as <code>select_fields</code> undefined"},{"location":"nodes/flowdata/path_split.html","title":"The path_split node","text":"<p>since v1.3.3</p> <p>Split a data-point by field root paths.</p> <p>Outputs a data-point for every object, that is found under a root path. Values that are not objects themselves are ignored.</p> <p>The original data-point will not be emitted by this node.</p> <p>With <code>include_name</code> and <code>include_as</code>, the name of the root path can be used as a new field for the resulting data-points.</p> <p>Note, that field/path names must start with a letter !</p>"},{"location":"nodes/flowdata/path_split.html#examples","title":"Examples","text":"<pre><code>def path_prefix = 'module'\n\n|json_emitter(\n    '{\"module1\" : {\"what\" : \"ever\"}, \"module2\": {\"this\" : 555}, \"module3\" : 14}'\n)\n\n|path_split()\n.include_as(path_prefix)\n</code></pre> <p>Here root paths are: <code>module1</code>, <code>module2</code> and <code>module3</code>.</p> <p>This example will produce 2 new data-points:</p> <pre><code>{\"ts\":  1675147412200, \"module\": \"module1\", \"what\": \"ever\"}\n</code></pre> <p>and</p> <pre><code>{\"ts\":  1675147412200, \"module\": \"module2\", \"this\": 555}\n</code></pre> <p>Data from root path <code>module3</code> is ignored, because it is not an object (the value is '14') .</p>"},{"location":"nodes/flowdata/path_split.html#parameters","title":"Parameters","text":"Parameter Description Default include_name( <code>boolean</code> ) Whether to include the root path name as a new field in the resulting points. true include_as( <code>string</code> ) The field name for the new field, if include_name is true. 'name'"},{"location":"nodes/flowdata/rename.html","title":"The rename node","text":"<p>Rename existing fields and/or tags.</p> <p>If a field or tag given does not exist, it will be ignored.</p>"},{"location":"nodes/flowdata/rename.html#examples","title":"Examples","text":"<pre><code>|rename()\n.fields('topic', 'temperature') \n.as_fields('cipot', 'mean_temp')\n</code></pre> <p>A list of strings for the new field names is given.</p> <p><code>Since 0.19.4</code> : We can now also use lambda expressions for the <code>as_fields</code> parameter.</p> <pre><code>|rename()\n.fields('topic', 'temperature') \n.as_fields('cipot', lambda: str_concat(\"data.prefix\", '_temp'))\n</code></pre> <p>Here we use a mixed list, strings and a lambda expression, for the as_fields parameter.</p>"},{"location":"nodes/flowdata/rename.html#parameters","title":"Parameters","text":"Parameter Description Default fields( <code>string_list</code> ) list of fieldnames to rename [] as_fields( <code>list</code> ) list of strings or lambda expressions (can be mixed) for the new fieldnames [] tags( <code>string_list</code> ) list of tagnames to rename [] as_tags( <code>string_list</code> ) list of new tagnames for renaming []"},{"location":"nodes/flowdata/set.html","title":"The set node","text":"<p>Set fields and/or tags to a data_point or batch. Overwrites any existing fields or tags.</p> <p>If fields or tags should be written only if they do not already exist,  use the default node.</p>"},{"location":"nodes/flowdata/set.html#example","title":"Example","text":"<pre><code>|set()\n.fields('id', 'vs', 'df')\n.field_values('some_id', 1, '05.043')\n</code></pre> <p>The above example will set the field <code>id</code> to the value 'some_id'. Accordingly <code>vs</code> will be set to 1, <code>df</code> will be set to '05.043'.</p>"},{"location":"nodes/flowdata/set.html#parameters","title":"Parameters","text":"Parameter Description Default fields( <code>string_list</code> ) list of fieldnames [] field_values( <code>list</code> ) list of values for the given fields (must have the same length as fieldnames) [] tags( <code>string_list</code> ) list of tagnames [] tag_values( <code>list</code> ) list of values for the given tags (must have the same length as tagnames) []"},{"location":"nodes/flowdata/unbatch.html","title":"The unbatch node","text":"<p>Used to unbatch data_batch items.  This node only emits data_point items. </p> <p>If it receives a <code>data_point</code> item, it will simply pass it on to connected nodes. </p> <p>If the node receives <code>data_batch</code> items, it will emit every single data_point from the batch in sequence.</p>"},{"location":"nodes/flowdata/unbatch.html#example","title":"Example","text":"<pre><code>|unbatch()\n</code></pre>"},{"location":"nodes/flowdata/unbatch.html#parameters","title":"Parameters","text":"Parameter Description Default"},{"location":"nodes/http/http_get.html","title":"The http_get node","text":"<p>Since 0.16.0</p> <p>Requests data from a specified HTTP(s) endpoint via the GET method. If any errors occur during the request, the node will attempt to retry sending.</p> <p>Request are made periodically, if <code>every</code> is given and/or triggered via incoming data-items.</p>"},{"location":"nodes/http/http_get.html#example","title":"Example","text":"<pre><code>|http_get()\n.host('127.0.0.1')\n.port(8081)\n.path('/v1/stats/faxe')\n.every(4s)\n.align()\n.as('get_response') \n</code></pre> <p>Sends a GET request every 4 seconds to the specified host with the URI path <code>/v1/stats/faxe</code>. Response data will be interpreted as JSON and 'get_response' will be the root object for the resulting data-point.</p> <p>Example in json format</p> <p>Http response body:</p> <pre><code>{\"id\": 2233, \"name\": \"takahashu\", \"mode\": 123}\n</code></pre> <p>Resulting data-point</p> <pre><code>{\"ts\": 1629812164152, \n\"get_response\":  \n      {\"id\": 2233, \"name\": \"takahashu\", \"mode\": 123}\n}\n</code></pre> <p>Without the <code>as</code> parameter, the resulting data-point would be:</p> <pre><code>{\"ts\": 1629812164152, \n\"id\": 2233, \"name\": \"takahashu\", \"mode\": 123\n}\n</code></pre>"},{"location":"nodes/http/http_get.html#parameters","title":"Parameters","text":"Parameter Description Default host( <code>string</code> ) hostname or ip address of endpoint port( <code>integer</code> ) port number 80 tls( <code>is_set</code> ) whether to use tls ie. https false (not set) user( <code>string</code> ) username for Basic Authentication undefined pass( <code>string</code> ) password for Basic Authentication undefined path( <code>string</code> ) URI path of the http endpoint '/' every( <code>duration</code> ) interval at which requests are made undefined align( <code>is_set</code> ) align read intervals according to every false (not set) payload_type ( <code>string</code> ) how to interpert the response body, <code>'json'</code> or <code>'plain'</code> 'json' retries( <code>integer</code> ) number of retries, if request failed 2 as( <code>string</code> ) Root-path for the resulting data-point. If not given and <code>payload-type</code> is <code>plain</code>, defaults to 'data'. undefined"},{"location":"nodes/http/http_listen.html","title":"The http_listen node","text":"<p>Since 0.16.0</p> <p>The http_listen node provides an http endpoint, that is listening for incoming data via <code>POST</code> or <code>PUT</code>.</p>"},{"location":"nodes/http/http_listen.html#response","title":"Response","text":"Status Description Body 200 OK OK json : {\"success\": \"true\"} 401 Unauthorized BasicAuth was defined with <code>user and pass</code> options, but Authorization header not present or has wrong value. 405 Method not allowed Http method other than <code>OPTIONS</code>, <code>POST</code> or <code>PUT</code> used. 415 Unsupported Media Type Body is missing or wrong content-type is used."},{"location":"nodes/http/http_listen.html#example-1","title":"Example 1","text":"<pre><code>|http_listen()\n</code></pre> <p>Will set up an <code>http</code> endpoint, waiting for data coming in on <code>port 8899</code> and path '/' and as <code>application/json</code> data. The body will be interpreted as json and inserted with the root-object 'data' in the resulting data-point.</p>"},{"location":"nodes/http/http_listen.html#example-2","title":"Example 2","text":"<pre><code>|http_listen() \n.path('/SInterface/MaintenanceInterface_SaveMaintenanceAlert') \n.as('data.http_res')\n.content_type('application/x-www-form-urlencoded')\n.payload_type('json')\n.tls()\n</code></pre> <p>Will set up an <code>https</code> endpoint, waiting for data coming in on <code>port 8899</code> and with path '/SInterface/MaintenanceInterface_SaveMaintenanceAlert' and as <code>application/x-www-form-urlencoded</code> data. </p> <p>Every field in this urlencoded body will be interpreted as a json string. For example: if there is a field called <code>alert_type</code> in the incoming body with a value of: <code>{\"id\": 0, \"name\": \"notice\"}</code>,  there will be a field <code>data.http_res.alert_type</code> in the resulting data-item:</p> <pre><code>{\"ts\": 1629812164152, \n\"data\": \n    {\"http_res\":  \n      {\"alert_type\":  {\"id\": 0, \"name\": \"notice\"}}\n    }\n}\n</code></pre>"},{"location":"nodes/http/http_listen.html#example-3","title":"Example 3","text":"<pre><code>|http_listen()\n.port(8898)\n.content_type('text/plain')\n.as('data.http_res')\n.payload_type('json')\n</code></pre> <p>Will set up an <code>http</code> (no tls) endpoint, listening on <code>port 8898</code> and path '/'. The body of the message will be interpreted as a json string. The resulting data-structure will be set under <code>data.http_res</code> in the resulting data-item.</p> <p>Example: Raw body value: <code>{\"id\": 2233, \"name\": \"takahashu\", \"mode\": 123}</code>.</p> <p>Resulting data-point in json format:</p> <pre><code>{\"ts\": 1629812164152, \n\"data\": \n    {\"http_res\":\n      {\"id\": 2233, \"name\": \"takahashu\", \"mode\": 123}\n    }\n}\n</code></pre>"},{"location":"nodes/http/http_listen.html#parameters","title":"Parameters","text":"Parameter Description Default port( <code>integer</code> ) port to listen on 8899 tls( <code>is_set</code> ) whether to use tls (https), For ssl options used (tls version, ciphers suites, etc.) see faxe's http API config false (not set) path( <code>string</code> ) URI path of the http endpoint '/' payload_type( <code>string</code> ) how to interpret incoming data, valid values: <code>'plain'</code> or <code>'json'</code> 'plain' content_type( <code>string</code> ) how the message-body will be interpreted, <code>'application/x-www-form-urlencoded'</code>, <code>'text/plain'</code>, <code>'application/json'</code> 'application/json' as( <code>string</code> ) base path for resulting data-items, if not given and content_type is <code>text/plain</code> or <code>application/json</code>, 'data' is the default value undefined / 'data' user( <code>string</code> ) If given, <code>BasicAuth Authorization</code> is to be used in requests to this endpoint. undefined pass( <code>string</code> ) If user is given, pass must be given also. undefined"},{"location":"nodes/http/http_post.html","title":"The http_post node","text":"<p>Sends incoming data to a specified HTTP endpoint via the <code>POST</code> or <code>PUT</code> method as a JSON message. If any errors occur during the request, the node will attempt to retry sending.</p> <p>Content-type header <code>application/json</code> will be used.</p> <p>Note: This node will only output flow-data, if the request was successful.</p>"},{"location":"nodes/http/http_post.html#example","title":"Example","text":"<pre><code>|http_post()\n.host('remote.com')\n.port(8088)\n.path('/receive/json') \n</code></pre> <p>Sends all incoming data to http://remote.com:8088/receive/json in JSON format.</p> <pre><code>|http_post()\n.host('remote.com')\n.port(8088)\n.path('/receive/json') \n.header_names('X-Api-key')\n.header_values('0000-0000-0000-000') \n</code></pre> <p>Custom header <code>X-Api-key</code>.</p>"},{"location":"nodes/http/http_post.html#parameters","title":"Parameters","text":"Parameter Description Default host( <code>string</code> ) hostname or ip address of endpoint port( <code>integer</code> ) port number tls( <code>is_set</code> ) whether to use tls ie. https false (not set) user( <code>string</code> ) username for Basic Authentication undefined pass( <code>string</code> ) password for Basic Authentication undefined path( <code>string</code> ) URI path of the http endpoint '/' method( <code>string</code> ) HTTP method to use, valid options are: 'post' or 'put' 'post' field( <code>string</code> ) if given, only data from the specified field gets sent, otherwise the whole data-item is used undefined header_names( <code>string_list</code> ) list of names for custom headers, that should be sent undefined header_values( <code>string_list</code> ) list of corresponding values for custom headers (must be the same length as <code>header_names</code>) undefined without( <code>string_list</code> ) list of field-paths, that should not be sent undefined response_as( <code>string</code> ) field-path that should be used as the key for the emitted response data 'data'"},{"location":"nodes/logic/case.html","title":"The case node","text":"<p>Evaluates a series of lambda expressions in a top down manner.</p> <ul> <li>The node will output / add the corresponding value of the first lambda expression that evaluates as true.</li> <li>If none of the lambda expressions evaluate as true, a default value will be used</li> </ul> <p>The case node works in a similar way CASE expressions in SQL work.</p>"},{"location":"nodes/logic/case.html#example","title":"Example","text":"<pre><code>|case(\n    lambda: \"data.condition.name\" == 'OK',\n    lambda: \"data.condition.name\" == 'Warning',\n    lambda: \"data.condition.name\" == 'Error'\n)\n.values(\n    '{\"cond\": \"Everything OK!\"}',\n    '{\"cond\": \"Oh, oh, a Warning!\"}',\n    '{\"cond\": \"Damn, Error!\"}'\n)\n.json()\n.as('data')\n.default('{\"cond\": \"Nothing matched!!!\"}')\n</code></pre>"},{"location":"nodes/logic/case.html#parameters","title":"Parameters","text":"Parameter Description Default [node] lambdas( <code>lambda_list</code> ) list of lambda expressions values( <code>string_list\\|text_list</code> ) corresponding values json( <code>is_set</code> ) if set, will treat the <code>values</code> and <code>default</code> parameters as json strings false, not set as (<code>string</code>) field-path for the output value default(<code>any</code>) default value to use, if no case clause matches"},{"location":"nodes/logic/change_detect.html","title":"The change_detect node","text":"<p>Emits new point-values only if different from the previous point.</p> <ul> <li>Multiple fields can be monitored for change by this node.</li> <li>If no fields are given, the complete data-item is compared to the last one.</li> <li>If a reset_timeout is given, all previous values will be reset when no value is received within this amount of time. So that after the timeout the first data_item will be emitted. </li> <li>For value comparison erlang's strict equals (=:=) is used, so 1.0 is not equal to 1.</li> </ul>"},{"location":"nodes/logic/change_detect.html#example","title":"Example","text":"<pre><code>%% detects any changes in data_items\n|change_detect()\n\n%% detect changes in one field, with timeout\n%% outputs at least the first data_item coming in after a 3 second timeout\n|change_detect('val')\n.reset_timeout(3s)\n\n%% detect changes in two fields, with timeout\n|change_detect('data.State.Err', 'data.State.Msg')\n.reset_timeout(60s) \n\n% in-example json notation: \n% {\"data\": {\"x\": {\"temp\": 32.4564}, \"y\" : {\"temp\" : 31.15155}} }\n\n|change_detect('data.x.temp', 'data.y.temp')\n</code></pre>"},{"location":"nodes/logic/change_detect.html#parameters","title":"Parameters","text":"Parameter Description Default [node] fields( <code>string_list</code> ) List of fields to monitor undefined reset_timeout( <code>duration</code> ) Previous values TTL 3h group_by( <code>string</code> ) Group the internal buffer by the value of this field, so that for every different value of it holds its own last value. undefined strict( <code>boolean</code> ) If <code>fields</code> are given, when strict is set to true, all values have to change in order for the node to let a data_point pass. This has no effect, when <code>fields</code> is not given. true"},{"location":"nodes/logic/deadman.html","title":"The deadman node","text":"<p>Emits a point, if there is no item coming in for the given amount of time. For output there are two options:</p> <ul> <li>If <code>repeat_last</code> param is set, the node will output the last message it saw incoming as the dead-message, if there is no last message yet, an empty message will be emitted</li> <li> <p>With <code>fields</code> and <code>field_values</code> a list of values can be provided to be included in the output.</p> </li> <li> <p>If no fields (and field_values) parameter and is given, an empty datapoint will be emitted.</p> </li> <li>The <code>repeat_last</code> parameter will always override the <code>fields</code> and <code>field_values</code> parameter</li> <li>The node will forward every message it gets by default, this can be changed by using the <code>no_forward</code> flag</li> </ul>"},{"location":"nodes/logic/deadman.html#examples","title":"Examples","text":"<pre><code>|deadman(15s)\n</code></pre> <p>Outputs an empty data-point item, if the node does not see any items coming in withing 15 seconds, while simply forwarding any item it gets.  The first timeout starts, when the node is starting.</p> <pre><code>def interval = 15s \n\n|deadman(interval)\n.trigger_on_value()\n.repeat_last()\n.repeat_interval(interval)\n</code></pre> <p>Outputs every item as a passthrough and, if no item is seen within 15 seconds,  outputs the last item with a new timestamp, that is derived from the last iteration and increased by interval (15s). </p>"},{"location":"nodes/logic/deadman.html#parameters","title":"Parameters","text":"Parameter Description Default [node] timeout( <code>duration</code> ) timeout value for the node fields( <code>string_list</code> ) undefined field_values (<code>string_list</code>) undefined repeat_last( is_set) whether to output the last value seen false, not set no_forward( is_set) whether to output every message that comes in (pass through) false, not set repeat_with_new_ts ( <code>bool</code> ) when repeating an item, set the current timestamp to that item before emitting true repeat_interval ( <code>duration</code> ) when repeating an item, adds this amount of time to a buffered timestamp (from the last item seen, or from the last addition of this interval) to genereate a new timestamp for the repeated item, this setting is independent of <code>repeat_with_new_ts</code> and takes precedence over it undefined"},{"location":"nodes/logic/eval.html","title":"The eval node","text":"<p>Evaluate one or more lambda expressions. For an explanation of lambdas, see lambda.</p> <p>The list of lambda expressions given, will be evaluated top down. This means that a lambda can use the result of a previous expression.</p> <p>The resulting fields named with the <code>as</code> parameter will be added to the current data-point.</p>"},{"location":"nodes/logic/eval.html#examples","title":"Examples","text":"<pre><code>|eval(lambda: \"val\" * 2, lambda: \"double\" / 2)\n%% 'double' is also used in the second expression above\n.as('double', 'val')\n</code></pre> <p>This example demonstrates the 'serial' behaviour of the <code>eval</code> node. The second expression uses the field <code>double</code>, which the first expression just created.</p> <pre><code>|eval( \n    lambda: int(str_concat(string(int(\"val\")),string(int(\"val\"))))\n)\n.as('concat_string.int')\n</code></pre> <p>The string value of the field 'val' is concatenated to itself, this is then casted to an int value and  added to the current data-point as the field 'concat_string.int'. </p> <p>For more lambda examples see lambda</p>"},{"location":"nodes/logic/eval.html#parameters","title":"Parameters","text":"Parameter Description Default [node] lambdas( <code>lambda_list</code> ) list of lambda expressions as( <code>string_list</code> ) list of output fieldnames (must have the same length as <code>lambdas</code>) tags ( <code>string_list</code> ) list of output tagnames []"},{"location":"nodes/logic/if.html","title":"The if node","text":"<p>Simple if-then-else logic.</p> <p>Note: If you want to filter data-items from a stream of data, use the where node.</p> <p>Almost everything you can do with this node, can also be done with an eval node, using the if function.</p>"},{"location":"nodes/logic/if.html#example","title":"Example","text":"<pre><code>|json_emitter()\n.json(\n      '{\"address\": \"\", \"value\": 99}',\n      '{\"address\": \"cond_scale\", \"value\": 94}',\n      '{\"address\": \"cond_robot\", \"value\": 95}'\n      )\n.as('data') \n\n|if(lambda: \"data.address\" == '')\n.then('address is empty')\n.else(lambda: str_concat('address is data.address and value is ', string(\"data.value\")))\n.as('data.note')\n</code></pre> <p>The if node adds a new field <code>data.note</code> with either the string 'address is empty' or, if the address is not empty, a string with the address and the value of the field <code>data.value</code>.</p> <p>If we would remove the <code>else</code> parameter in the example, every data-item for which the if test returns false, would not get the field <code>data.note</code> added (and simply left untouched on emit).</p>"},{"location":"nodes/logic/if.html#parameters","title":"Parameters","text":"Parameter Description Default [node] test( <code>lambda</code> ) if test lambda then( <code>any</code> ) value when test evaluates as TRUE, any type including a lambda expression is valid else( <code>any</code> ) value when test evaluates as FALSE, any type including a lambda expression is valid undefined as (<code>string</code>) field-path for the output value"},{"location":"nodes/logic/sample.html","title":"The sample node","text":"<p>Samples the incoming points or batches. One point will be emitted every count or duration specified.</p> <p>When a duration is given, this node will emit the first data-item arriving after the timeout,  then the timeout starts again.</p>"},{"location":"nodes/logic/sample.html#example","title":"Example","text":"<pre><code>|sample(5)\n</code></pre> <p>Keep every 5th data_point or data_batch.</p> <pre><code>|sample(10s)\n</code></pre> <p>Keep the first point or batch after a 10 second interval.</p>"},{"location":"nodes/logic/sample.html#parameters","title":"Parameters","text":"Parameter Description Default [node] rate( <code>integer</code> <code>duration</code> ) sample rate"},{"location":"nodes/logic/shift.html","title":"The shift node","text":"<p>The shift node shifts points and batches in time.  This is useful for comparing batches or points from different times.</p>"},{"location":"nodes/logic/shift.html#example","title":"Example","text":"<pre><code>|shift(5m)\n</code></pre> <p>Shift all data points 5 minutes forward in time.</p> <pre><code>|shift(-10s)\n</code></pre> <p>Shift all data points 10 seconds backwards in time.</p>"},{"location":"nodes/logic/shift.html#parameters","title":"Parameters","text":"Parameter Description Default [node] offset( <code>duration</code> ) time offset"},{"location":"nodes/logic/state_change.html","title":"The state_change node","text":"<p>Computes the duration and count of points of a given state. The state is defined via a lambda expression.</p> <p>For each consecutive point for which the expression evaluates as true, state count and duration will be incremented.</p> <p>This node can be used to track state in data and produce new data based on the state-events. It can produce new data-points every time the state is <code>entered</code> and/or <code>left</code>. </p>"},{"location":"nodes/logic/state_change.html#the-enter-data-point","title":"The enter data-point","text":"<p>If the <code>.enter()</code> option is set, a new data-point will be emitted on state-enter. The new data-point will have a field, named with the <code>.enter_as()</code> option, set to <code>1</code>. This fieldname defaults to <code>state_entered</code>.</p> <p>new since 0.19.51: <code>state_id</code>, uuid v4 is created on every state entry  </p>"},{"location":"nodes/logic/state_change.html#the-leave-data-point","title":"The leave data-point","text":"<p>If the <code>.leave()</code> option is set, a new data-point will be emitted on state-leave. Fields for this data-point:</p> Name Description <code>state_left</code> stateflag, set to <code>1</code> <code>state_start_ts</code> timestamp at which the state has been entered <code>state_end_ts</code> timestamp at which the state-expression has been satisfied the last time <code>state_duration</code> duration of the state in <code>milliseconds</code> <code>state_count</code> number of points, the number of consecutive data-points for which the state-expression returned true <code>state_id</code> new since <code>0.19.51</code>, uuid v4 is created on every state-enter action <p>When the lambda expression generates an error during evaluation, the current point is discarded and does not affect any calculations.</p> <p>Note that while state-count is 1, state-duration will be 0, if there is exactly 1 data-point within the state-window.</p>"},{"location":"nodes/logic/state_change.html#example","title":"Example","text":"<pre><code>%% the lambda defines our state    \n|state_change(lambda: \"val\" &lt; 7 AND \"err\" != 0)\n%% the node will emit a data-point on state-leave only\n.leave()\n%% we keep these fields for the new state-leave data-point\n.leave_keep('err', 'err_code')\n</code></pre> <p>Example output in json:</p> <pre><code>{\n\"ts\": 1232154654655, \n\"err\": 1, \"err_code\": 1492, \"state_left\": 1, \"state_id\" : \"07aae050-9d30-4570-989d-74f5f21d52bf\",\n\"state_start_ts\": 1232154644655, \"state_end_ts\" : 1232154654655,\n\"state_duration\": 10000, \"state_count\": 22\n}\n</code></pre> <pre><code>%% the lambda defines our state    \n|state_change(lambda: \"val\" &gt; 2 OR \"err\" == 1)\n%% the node will emit a data-point on state-enter\n.enter()\n%% the node will emit a data-point on state-leave\n.leave()\n%% we keep these fields for the new state-enter data-point\n.enter_keep('err', 'err_code')\n%% we keep these fields for the new state-leave data-point\n.leave_keep('err', 'err_code')\n%% prefix all fields written by this node with 'my_'\n.prefix('my_')\n</code></pre> <p>Example output in json for the enter data-point:</p> <pre><code>{\n\"ts\": 1232154654655, \"state_id\": \"07aae050-9d30-4570-989d-74f5f21d52bf\", \n\"err\": 1, \"err_code\": 1492, \"my_state_entered\": 1\n}\n</code></pre>"},{"location":"nodes/logic/state_change.html#parameters","title":"Parameters","text":"Parameter Description Default [node] lambda( <code>lambda</code> ) state lambda expression enter( is_set ) emit a datapoint on state-enter undefined leave( is_set ) emit a datapoint on state-leave undefined enter_as( <code>string</code> ) name for the \"enter\" field, it will be set to true 'state_entered' leave_as( <code>string</code> ) name for the \"leave\" field, it will be set to true 'state_left' state_id_as( <code>string</code> ) since 0.19.51: name for the \"state_id\" field, 'state_id' enter_keep( <code>string_list</code> ) a list of fieldnames that should be kept for the <code>enter</code> data-point [] leave_keep( <code>string_list</code> ) a list of fieldnames that should be kept for the <code>leave</code> data-point [] prefix( <code>string</code> ) prefix fields added by this node with a string (<code>keep</code>-fields stay untouched) '' (empty string) <p>At least one of the <code>enter | leave</code> options must be given.</p>"},{"location":"nodes/logic/state_count.html","title":"The state_count node","text":"<p>Computes the number of consecutive points in a given state.  The state is defined via a lambda expression. For each consecutive point for which the expression evaluates as true, the state count will be incremented.</p> <p>When a point evaluates to false, the state count is reset. The state count will be added as an additional int field to each point. If the expression evaluates to false, the value will be -1.</p> <p>If the expression generates an error during evaluation, the point is discarded and does not affect the state count.</p>"},{"location":"nodes/logic/state_count.html#example","title":"Example","text":"<pre><code>|state_count(lambda: \"val\" &lt; 7)\n.as('val_below_7')\n</code></pre> <p>Counts the number of consecutive points which have the value of the <code>val</code> field <code>below 7</code>.</p>"},{"location":"nodes/logic/state_count.html#parameters","title":"Parameters","text":"Parameter Description Default [node] lambda( <code>lambda</code> ) state lambda expression as( <code>string</code> ) name for the added count field 'state_count'"},{"location":"nodes/logic/state_duration.html","title":"The state_duration node","text":"<p>Computes the duration of a given state. The state is defined via a lambda expression.</p> <p>For each consecutive point for which the lambda expression evaluates as true, the state duration will be incremented by the duration between points.</p> <p>When a point evaluates as false, the state duration is reset.</p> <p>The state duration will be added as an additional field to each point and it's unit is <code>milliseconds</code>. If the expression evaluates to false, the value will be -1.</p> <p>When the lambda expression generates an error during evaluation, the point is discarded and does not affect the state duration..</p>"},{"location":"nodes/logic/state_duration.html#example","title":"Example","text":"<pre><code>|state_duration(lambda: \"val\" &lt; 7)\n</code></pre>"},{"location":"nodes/logic/state_duration.html#parameters","title":"Parameters","text":"Parameter Description Default [node] lambda( <code>lambda</code> ) state lambda expression as( <code>string</code> ) name for the added duration field 'state_duration'"},{"location":"nodes/logic/state_sequence.html","title":"The state_sequence node","text":"<p>This node takes a list of lambda expressions representing different states.</p> <p>It will emit values only after each state has evaluated as true in the given order and, for each step in the sequence within the corresponding timeout.</p> <p>A transition timeout must be defined for every state transition with the <code>within</code> parameter.</p> <p>If a timeout occurs at any point the sequence will be reset and started from the first expression again.</p> <p>Note that the sequence timeouts start after the first data_point has satisfied the first lambda expression. Therefore, if 3 lambda states are given, only 2 durations for the <code>within</code> parameter can be defined.</p> <p>With the <code>strict</code> parameter the sequence of states must be met exactly without any intermediary data_points coming in, that do not satisfy the current state expression. Normally this would not reset the sequence of evaluation, in this mode, it will.</p> <p>On a successful evaluation of the whole sequence, the node will simply output the last value, that completed the sequence.</p> <p>The state_sequence node can be used with one or many input nodes.</p>"},{"location":"nodes/logic/state_sequence.html#example","title":"Example","text":"<pre><code>in1\n|state_sequence(in2, in3) %% can use any number of nodes\n.states(\n    lambda: \"data.topic\" == 'in1', %% state 1\n    lambda: \"data.topic\" == 'in2', %% state 2\n    lambda: \"data.topic\" == 'in3'  %% state 3\n)\n.within(\n    25s, %% transition-time from state 1 to state 2\n    20s  %% transition-time from state 2 to state 3\n    ) \n</code></pre>"},{"location":"nodes/logic/state_sequence.html#parameters","title":"Parameters","text":"Parameter Description Default [node] nodes_in( <code>node_list</code> ) a list of node(chains) optional states (<code>lambda_list</code>) the states within( <code>duration_list</code> ) one timeout for every state-transition strict( <code>is_set</code> ) whether the state sequence must be transition exactly false (not set)"},{"location":"nodes/logic/time_diff.html","title":"The time_diff node","text":"<p>The time_diff node adds a field to the current data-item containing the difference between the timestamps of the consecutive items. Note that the difference in time will be calculated from the data-points timestamp fields  and does not reflect the difference in time points coming into the node.</p> <p>For the other behaviour see time_elapsed.</p> <p>The unit for output values is milliseconds.</p>"},{"location":"nodes/logic/time_diff.html#example","title":"Example","text":"<pre><code>|time_diff()\n.as('time_diff')\n</code></pre>"},{"location":"nodes/logic/time_diff.html#parameters","title":"Parameters","text":"Parameter Description Default as( <code>string</code> ) name of the field for parsed data 'timediff' default( <code>any</code> ) default value to use, when no previous time exists 0"},{"location":"nodes/logic/time_elapsed.html","title":"The time_elapsed node","text":"<p>The time_elapsed node adds a field to the current data-item containing the difference in arrival time of consecutive items.</p> <p>See the time_diff node.</p> <p>The unit for output values is milliseconds.</p>"},{"location":"nodes/logic/time_elapsed.html#example","title":"Example","text":"<pre><code>|time_elapsed()\n.as('time_dur')\n</code></pre>"},{"location":"nodes/logic/time_elapsed.html#parameters","title":"Parameters","text":"Parameter Description Default as( <code>string</code> ) name of the field for parsed data 'elapsed' default( <code>any</code> ) default value to use, when no previous time exists 0"},{"location":"nodes/logic/triggered_timeout.html","title":"The triggered_timeout node","text":"<p>Emits a point, if there is no message coming in for the given amount of time.</p> <p>A timeout will be started on an explicit trigger: * When a lambda expression is given for parameter <code>timeout_trigger</code>, this expression must evaluate as true to start (and after a timeout has occurred to restart) a timeout.</p> <ul> <li>If no lambda expression is given for the <code>timeout_trigger</code>, the trigger is any data_point coming in on port 1, the so called <code>trigger_port</code>.</li> </ul> <p>A new trigger does not restart a running timeout. After a timeout occurred, the node waits for a new trigger to come in before it starts a new timeout.</p> <p>After a timeout is started the node waits for data coming in, that either does not satisfy the trigger expression(when a lambda expression is given for the <code>timeout_trigger</code> parameter) or is coming in on any port except the <code>trigger_port</code> (port 1).</p> <p>Data for the outgoing data-point can be defined with the <code>fields</code> and <code>field_values</code> parameters. This node can have any number of input-nodes.</p>"},{"location":"nodes/logic/triggered_timeout.html#example","title":"Example","text":"<pre><code>def timeout = 30s\n% ...\n\nin1\n|triggered_timeout(in2)\n.timeout(timeout)\n.timeout_trigger(lambda: \"data.topic\" == 'in1')\n\n\ndef condition_reason = 'oh no !!'\n\nrobot_state\n|triggered_timeout(orderlog)\n.timeout(timeout)\n.fields(\n    'combined.condition.name', \n    'combined.condition_reason', \n    'combined.condition.id')\n.field_values(\n    'ERROR', \n    condition_reason, \n    2)\n%.cancel_fields('combined.condition.name', 'combined.condition_reason', 'combined.condition.id')\n%.cancel_field_values('OK', '', 0)\n</code></pre>"},{"location":"nodes/logic/triggered_timeout.html#parameters","title":"Parameters","text":"Parameter Description Default timeout( <code>duration</code> ) timeout_trigger( <code>lambda</code> ) lambda expression which triggers the timeout optional fields (<code>string_list</code>) paths for the output fields optional field_values( <code>list</code> ) values for the output fields optional"},{"location":"nodes/logic/value_diff.html","title":"The value_diff node","text":"<p>Outputs the difference to a previous value for multiple fields.</p> <p>If no previous value is found for a specific field, a configurable default value is used.</p> <p>This node can handle <code>numeric</code> values only.</p>"},{"location":"nodes/logic/value_diff.html#example","title":"Example","text":"<pre><code>|value_diff()\n.fields('value')\n.as('value_diff')\n</code></pre>"},{"location":"nodes/logic/value_diff.html#parameters","title":"Parameters","text":"Parameter Description Default fields( <code>string_list</code> ) as( <code>string_list</code> ) names of the ouptput fields, if not specified all <code>fields</code> will be overwritten with diff values defaults to <code>fields</code> default( <code>number</code> ) Value to use a default current value of field mode( <code>string</code> ) diff modes: <code>abs</code>(absolute difference between previous and current), <code>c-p</code> (current minus previous value), <code>p-c</code> (previous minus current) 'abs'"},{"location":"nodes/logic/where.html","title":"The where node","text":"<p>Filter points and batches with a lambda expression, which returns a boolean value.</p> <p>Data-items for which the lambda expression evaluates as <code>false</code> will be discarded. </p>"},{"location":"nodes/logic/where.html#example","title":"Example","text":"<pre><code>|where(lambda: hour(\"ts\") &lt; 18 AND hour(\"ts\") &gt; 8 )\n</code></pre> <p>Discards every point who's timestamp is not between 09:00 and 18:00 UTC.</p>"},{"location":"nodes/logic/where.html#parameters","title":"Parameters","text":"Parameter Description Default [node] lambda( <code>lambda</code> ) The lambda filter expression"},{"location":"nodes/messaging/amqp_consume.html","title":"The amqp_consume node","text":"<p>Consume data from an amqp-broker like RabbitMQ.</p> <p>This node accepts regular amqp routing keys as well as MQTT style topic strings for <code>bindings</code>/<code>routing_key</code>.</p>"},{"location":"nodes/messaging/amqp_consume.html#in-safe-mode","title":"In safe mode","text":"<p>Once a data-item is received by the node, it will be immediately stored in an on-disk queue for data-safety. Only after this will the item be acknowledged to the amqp broker.</p>"},{"location":"nodes/messaging/amqp_consume.html#message-deduplication","title":"Message deduplication","text":"<p>If the amqp <code>correlation-id</code> property is set (to a unique value per message), this node can perform efficient message deduplication.</p> <p>See amqp_publish for details on this.</p>"},{"location":"nodes/messaging/amqp_consume.html#prefetch-count-ack_every-and-dedup_size","title":"Prefetch count, ack_every and dedup_size","text":"<p>For a description of these settings, see table below.</p> <p>As they relate to one another in some kind, here is a rule of thumb for how to set <code>ack_every</code> and <code>dedup_size</code> when <code>prefetch</code> is changed:</p> <ul> <li>set <code>ack_every</code> to one third of <code>prefetch</code></li> <li>set <code>dedup_size</code> to 3 times the <code>prefetch</code> value</li> </ul> <p>Example: prefetch = 100, ack_every = 35, dedup_size = 300</p>"},{"location":"nodes/messaging/amqp_consume.html#queue-migration","title":"Queue migration","text":"<p>Since v1.4.2</p> <p>This feature can be used to migrate the flow of data to a new queue.</p> <p>The reasons for migrating a queue could be: </p> <ul> <li><code>Rename</code> a queue (maybe to use a different policy)</li> <li>Use a different <code>type of queue</code></li> <li>Migrate a queue to a <code>different vhost</code></li> </ul> <p>For this to work without losing any data, that is still living in the existing queue, the node will temporarily setup a second consumer for this queue and consumes from both queues in parallel, until it sees the same messages coming in on both queues (duplicates will be removed). At this point the consumer for the existing queue will be stopped and the queue will be deleted from the broker. If a flow that is setup to do a queue migration is restarted after the migration has already finished, the takeover_queue is not declared again, because the takeover_queue declaration is done in passive mode, which means that a queue can only be used, if it already exists on the broker.</p> <p>New node parameters are added (See parameters table below):</p> <ul> <li><code>takeover</code>: enable/disable queue takeover mode</li> <li><code>takeover_queue</code>: Name of the existing queue from which to takeover the data stream.</li> <li><code>takeover_queue_type</code>: \"x-queue-type\" amqp argument</li> <li><code>takeover_queue_prefix</code>: prefix for takeover queues</li> <li><code>takeover_vhost</code> </li> </ul> <p>All other parameters are the same for both consumers, except:</p> <ul> <li><code>queue_prefix</code>: Does not affect the takeover_queue, you must use the full (existing) queue name.</li> <li><code>exchange</code> and <code>exchange_prefix</code>: This will always be the same for both consumers.</li> <li><code>consumer_tag</code>: a postfix will be added for the take-over consumer.</li> </ul> <p>When <code>vhost</code> and <code>takeover_vhost</code> are different, you must make sure, that the data is published to both vhosts. The name of the exchanges must be the same on each vhost. Exchanges and queues are specific to a vhost, so we cannot bind a queue on one vhost to an exchange on another vhost.</p> <p>At the moment this node can only set up and work with <code>topic</code> exchanges.</p>"},{"location":"nodes/messaging/amqp_consume.html#examples","title":"Examples","text":"<pre><code>|amqp_consume()\n.host('deves-amqp-cluster1.internal') \n.bindings('my.routing.key')\n.exchange('x_xchange')\n.queue('faxe_test')\n.dt_field('UTC-Time')\n.dt_format('float_micro')\n</code></pre> <pre><code>|amqp_consume()\n.host('deves-amqp-cluster1.internal') \n.bindings('my.routing.key')\n.exchange('x_xchange')\n.queue('faxe_test')\n.queue_type('quorum')\n.takeover_queue('existing_queue') \n.dt_field('UTC-Time')\n.dt_format('float_micro')\n</code></pre> <p>Migrate from a queue named 'exisiting_queue' to a new queue named 'faxe_test' with type 'quorum'.</p>"},{"location":"nodes/messaging/amqp_consume.html#parameters","title":"Parameters","text":"Parameter Description Default host( <code>string</code> ) Ip address or hostname of the broker config: <code>amqp.host</code>/<code>FAXE_AMQP_HOST</code> port( <code>integer</code> ) The broker's port config: <code>amqp.port</code>/<code>FAXE_AMQP_PORT</code> user( <code>string</code> ) AMQP user config: <code>amqp.user</code>/<code>FAXE_AMQP_USER</code> pass( <code>string</code> ) AMQP password config: <code>amqp.pass</code>/<code>FAXE_AMQP_PASS</code> vhost( <code>string</code> ) vhost to connect to on the broker '/' vhost_prefix( <code>string</code> ) prefix for the <code>vhost</code> and <code>takeover_vhost</code> name, will be applied only, if <code>vhost</code> or <code>takeover_vhost</code> is not the default of <code>'/'</code> config: <code>rabbitmq.vhost_prefix</code>/<code>FAXE_RABBITMQ_VHOST_PREFIX</code> routing_key( <code>string</code> ) deprecated routing key to use for queue binding undefined bindings( <code>string_list</code> ) list of queue bindings keys [] queue( <code>string</code> ) name of the queue to bind to the exchange FlowName + '_' + NodeName queue_prefix( <code>string</code> ) prefix for the queue-name that will be ensured to exist for <code>queue</code> config: <code>rabbitmq.queue_prefix</code>/<code>FAXE_RABBITMQ_QUEUE_PREFIX</code> queue_type( <code>string</code> ) Queue type, Valid values are: <code>''</code>, <code>'quorum'</code>, <code>'classic'</code>. The value will be used for the \"x-queue-type\" argument while declaring a queue. With <code>''</code>, the type of the queue will be the default type defined for the vhost. '' exchange( <code>string</code> ) name of the exchange to bind to the source exchange FlowName + '_' + NodeName exchange_prefix( <code>string</code> ) prefix for the exchange-name that will be ensured to exist for <code>exchange</code> config: <code>rabbitmq.exchange_prefix</code>/<code>FAXE_RABBITMQ_EXCHANGE_PREFIX</code> prefetch( <code>integer</code> ) prefetch count to use 70 consumer_tag( <code>string</code> ) Identifier for the queue consumer 'c_' + FlowName + '_' + NodeName ack_every( <code>integer</code> ) number of messages to consume before acknowledging them to the broker 20 ack_after( <code>duration</code> ) timeout after which all currently not acknowledged messages will be acknowledged, regardless of the <code>ack_every</code> setting 5s use_flow_ack( <code>bool</code> ) special ack mode, where message acknowledgement is dependend on other nodes in the flow (see crate_out), <code>ack_every</code> and <code>ack_after</code> have no effect with this mode false dedup_size( <code>integer</code> ) number of correlation-ids to hold in memory for message deduplication 200 dt_field( <code>string</code> ) name of the timestamp field that is expected 'ts' dt_format( <code>string</code> ) timestamp or datetime format that is expected (see datetime-parsing) 'millisecond' include_topic ( <code>bool</code> ) whether to include the routingkey in the resulting datapoints true topic_as ( <code>string</code> ) if <code>include_topic</code> is true, this will be the fieldname for the routingkey value 'topic' as ( <code>string</code> ) base object for the output data-point undefined ssl( is_set ) whether to use tls, if true, ssl options from faxe's config for amqp connections will be used config: <code>amqp.ssl.enable</code>/<code>FAXE_AMQP_SSL_ENABLE</code> confirm ( <code>boolean</code> ) whether to acknowledge consumed messages to the amqp broker, when set to <code>false</code>, the channel will be set to auto-ack , throughput can be increased with the danger of data-loss true safe ( <code>boolean</code> ) whether to use faxe's internal queue. If <code>true</code>, messages consumed from the amqp broker will be stored in an internal ondisc queue before they get sent to downstream nodes, to avoid losing data. false takeover ( <code>boolean</code> ) Enable/disable queue takeover mode. config: <code>rabbitmq.takeover</code>/<code>FAXE_RABBITMQ_TAKEOVER</code> takeover_queue ( <code>string</code> ) Name for the take-over queue. FlowName + '_' + NodeName takeover_queue_prefix ( <code>string</code> ) prefix for the queue-name that will be ensured to exist for <code>takeover_queue</code> config: <code>rabbitmq.takeover_queue_prefix</code>/<code>FAXE_RABBITMQ_TAKEOVER_QUEUE_PREFIX</code> takeover_queue_type ( <code>string</code> ) Queue type for the take-over queue. Valid values are: <code>''</code>, <code>'quorum'</code>, <code>'classic'</code>. The value will be used for the \"x-queue-type\" argument while declaring a queue. With '', the type of the queue will be the default type defined for the vhost. config: <code>rabbitmq.takeover_queue_type</code>/<code>FAXE_RABBITMQ_TAKEOVER_QUEUE_TYPE</code> takeover_vhost ( <code>string</code> ) Name of the vhost for the take-over queue. value of <code>vhost</code> <p>Exactly one of these must be provided: <code>routing_key</code>, <code>bindings</code>.</p>"},{"location":"nodes/messaging/amqp_publish.html","title":"The amqp_publish node","text":"<p>Publish data to an amqp-broker exchange. The most popular amqp-broker is RabbitMQ.</p> <p>Incoming data is converted to JSON before sending.</p> <p>This node accepts regular amqp routing keys as well as MQTT style topic strings for each of the <code>routing_key(...)</code> params.</p> <p>The amqp <code>correlation-id</code> property will be set to phash2(routing_key + payload) using erlang's phash2 function on every published message:</p> <pre><code>The erlang documentation on phash2:\n\nPortable hash function that gives the same hash for the same Erlang term regardless of machine architecture and ERTS version.\n</code></pre> <p>(phash2 outputs an integer which gets casted to a string to be used as a correlation-id)</p> <p>The amqp_consume node will use this values to perform deduplication on message receiving.</p> <p>Note: This node is a sink node and does not output any flow-data, therefore any node connected to it will not receive any data from this node.</p>"},{"location":"nodes/messaging/amqp_publish.html#example","title":"Example","text":"<pre><code>|amqp_publish()\n.host('127.0.0.1') \n.routing_key('my.routing.key')\n.exchange('x_xchange')\n</code></pre>"},{"location":"nodes/messaging/amqp_publish.html#parameters","title":"Parameters","text":"Parameter Description Default host( <code>string</code> ) Ip address or hostname of the broker config: <code>amqp.host</code>/<code>FAXE_AMQP_HOST</code> port( <code>integer</code> ) The broker's port config: <code>amqp.port</code>/<code>FAXE_AMQP_PORT</code> user( <code>string</code> ) AMQP user config: <code>amqp.user</code>/<code>FAXE_AMQP_USER</code> pass( <code>string</code> ) AMQP password config: <code>amqp.pass</code>/<code>FAXE_AMQP_PASS</code> vhost( <code>string</code> ) vhost to connect to on the broker '/' vhost_prefix( <code>string</code> ) prefix for the <code>vhost</code> name, will be applied only, if <code>vhost</code> is not the default of <code>'/'</code> config: <code>rabbitmq.vhost_prefix</code>/<code>FAXE_RABBITMQ_VHOST_PREFIX</code> routing_key( <code>string</code> ) routing key for the published messages undefined routing_key_lambda( <code>lambda</code> ) lambda expression producing a routing key for the published messages undefined routing_key_field( <code>string</code> ) path to a field in the current data-item, who's value should be used as the routing-key undefined exchange( <code>string</code> ) name of the exchange to publish to qos( <code>integer</code> ) publish quality, see table below for details 1 persistent( <code>bool</code> ) whether to send the amqp messages with delivery-mode 2 (persistent) false (delivery_mode = 1) ssl( is_set ) whether to use ssl config: <code>amqp.ssl.enable</code>/<code>FAXE_AMQP_SSL_ENABLE</code> <p>One of <code>routing_key</code>, <code>routing_key_lambda</code>, <code>routing_key_field</code> is required.</p>"},{"location":"nodes/messaging/amqp_publish.html#qos","title":"Qos","text":"Qos description consequences 0 In memory queuing of messages, in case of network issues. Does not use publisher confirm on the channel. Highest throuput. 1 On disc queuing of messages, in case of network issues. Does not use publisher confirm on the channel. Not yet published messages will survive a flow crash. At least once. 2 On disc queue + acknowledgment according to acknowledgement from the amqp broker (publisher confirm). Most safe data delivery. Aims at exactly once semantics."},{"location":"nodes/messaging/mqtt_amqp_bridge.html","title":"The mqtt_amqp_bridge node","text":"<p>The mqtt_amqp-bridge node provides a message-order preserving and fail-safe mqtt-to-amqp bridge. It is designed for minimized overhead, high throughput and fault-tolerant message delivery. Receives data from an mqtt-broker and writes each indiviual topic with an amqp-publisher via an internal on-disk queue.</p> <p>This node starts 1 mqtt-subscriber and up to <code>max_publishers</code> number of amqp-publishers.</p> <p>The node does only work standalone at the moment, meaning you cannot connect it to other nodes.</p> <p>The mqtt_amqp_bridge is completely unaware of the message content. For performance reasons the node does not parse incoming data or use data_items as every other node in faxe does, instead internally it will work with the raw binaries received from the mqtt broker and pass them through to the amqp publishers.</p>"},{"location":"nodes/messaging/mqtt_amqp_bridge.html#example","title":"Example","text":"<pre><code>def topic = 'my/topic/#'\n|mqtt_amqp_bridge() \n.topics(topic)\n%% amqp params\n.amqp_host('10.11.12.13') \n.amqp_user('user')\n.amqp_pass('pass')\n.amqp_exchange('x_exchange')\n.amqp_ssl()\n.max_publishers(5) \n</code></pre>"},{"location":"nodes/messaging/mqtt_amqp_bridge.html#parameters","title":"Parameters","text":"Parameter Description Default host( <code>string</code> ) Ip address or hostname of the mqtt broker from config port( <code>integer</code> ) The mqtt broker's port 1883 from config user( <code>string</code> ) username for the mqtt connection from config pass( <code>string</code> ) password for the mqtt connection from config ssl( is_set ) whether to use ssl for the mqtt connection false (not set) topics( <code>string_list</code> ) mqtt topic to use qos( <code>integer</code> ) Quality of service for mqtt, one of 0, 1 or 2 1 amqp_host( <code>string</code> ) Ip address or hostname of the amqp broker from config file amqp_port( <code>integer</code> ) The amqp broker's port 1883 from config file amqp_user( <code>string</code> ) username for amqp connections from config file amqp_pass( <code>string</code> ) password for amqp connections from config file amqp_ssl( is_set ) whether to use ssl for the amqp connection false (not set) amqp_vhost( <code>string</code> ) VHost for the amqp broker '/' amqp_exchange( <code>string</code> ) name of the amqp exchange to publish to max_publishers( <code>integer</code> ) max number of amqp publishers that will be started 3 safe( is_set) whether to use queue acknowledgement for the internal on-disc queue false (not set) persistent( <code>bool</code> ) whether to send the amqp messages with delivery-mode 2 (persistent) false (delivery_mode = 1) reset_timeout( <code>duration</code> ) when the bridge does not see any new message for a topic for this amount of time, it will try to stop the corresponding queue and amqp-publisher process, if appropiate 5m"},{"location":"nodes/messaging/mqtt_publish.html","title":"The mqtt_publish node","text":"<p>Publish data to a mqtt-broker. Incoming data is converted to JSON before sending.</p> <p>If the <code>save()</code> parameter is given, every message first gets stored to an on-disk queue before sending, this way we can make sure no message gets lost when disconnected from the broker.</p> <p>Note: This node is a sink node and does not output any flow-data, therefore any node connected to this node will not get any data.</p>"},{"location":"nodes/messaging/mqtt_publish.html#example","title":"Example","text":"<pre><code>def topic = 'top/track/pressure'\n\n|mqtt_publish() \n.topic(topic)\n.retained()\n</code></pre> <p>Using a lambda expression for the topic:</p> <pre><code>def topic_base = 'top/'\n\n|mqtt_publish()\n.topic_lambda(lambda: str_concat([topic_base, \"type\", '/', \"measurement\"])\n</code></pre> <p>Here the topic string is built with a lambda expression using the <code>topic_base</code> declaration, the <code>string '/'</code> and two fields from the current data_point. The topic string may be a different one with every data_point that gets published.</p>"},{"location":"nodes/messaging/mqtt_publish.html#parameters","title":"Parameters","text":"Parameter Description Default host( <code>string</code> ) Ip address or hostname of the broker config: <code>mqtt.host</code>/<code>FAXE_MQTT_HOST</code> port( <code>integer</code> ) The broker's port config: <code>mqtt.port</code>/<code>FAXE_MQTT_PORT</code> user( <code>string</code> ) username config: <code>mqtt.user</code>/<code>FAXE_MQTT_USER</code> pass( <code>string</code> ) password config: <code>mqtt.pass</code>/<code>FAXE_MQTT_PASS</code> client_id( <code>string</code> ) mqtt client id, defaults to a combination of flow-id and node-id undefined topic( <code>string</code> ) mqtt topic to use undefined topic_lambda( <code>lambda</code> ) mqtt topic to use evaluated via a lambda expression undefined topic_field( <code>string</code> ) [since 0.19.9] path to a field in the current data-item, who's value should be used as the topic undefined qos( <code>integer</code> ) Quality of service, one of 0, 1 or 2 1 retained( is_set ) whether the message should be retained on the broker false (not set) save( is_set ) send save (on-disk queuing) false (not set) ssl( is_set ) whether to use ssl config: <code>mqtt.ssl.enable</code>/<code>FAXE_MQTT_SSL_ENABLE</code> <p><code>topic</code> or <code>topic_lambda</code> or <code>topic_field</code> must be provided.</p>"},{"location":"nodes/messaging/mqtt_subscribe.html","title":"The mqtt_subscribe node","text":"<p>Subscribe to an mqtt-broker and get data from one or more topics. </p>"},{"location":"nodes/messaging/mqtt_subscribe.html#example","title":"Example","text":"<pre><code>|mqtt_subscribe()\n.topics('top/grips/#')\n.dt_field('UTC-Stamp')\n.dt_format('float_micro')\n</code></pre>"},{"location":"nodes/messaging/mqtt_subscribe.html#parameters","title":"Parameters","text":"Parameter Description Default host( <code>string</code> ) Ip address or hostname of the broker config: <code>mqtt.host</code>/<code>FAXE_MQTT_HOST</code> port( <code>integer</code> ) The broker's port config: <code>mqtt.port</code>/<code>FAXE_MQTT_PORT</code> user( <code>string</code> ) username config: <code>mqtt.user</code>/<code>FAXE_MQTT_USER</code> pass( <code>string</code> ) password config: <code>mqtt.pass</code>/<code>FAXE_MQTT_PASS</code> client_id( <code>string</code> ) mqtt client id, defaults to a combination of flow-id and node-id undefined topics( <code>string_list</code> ) mqtt topic(s) to use undefined topic( <code>string</code> ) mqtt topic to use undefined qos( <code>integer</code> ) Quality of service, one of 0, 1 or 2 1 dt_field( <code>string</code> ) name of the timestamp field that is expected 'ts' dt_format( <code>string</code> ) timestamp or datetime format that is expected (see datetime-parsing) 'millisecond' include_topic ( <code>bool</code> ) whether to include the mqtt-topic in the resulting datapoints true topic_as ( <code>string</code> ) if <code>include_topic</code> is true, this will be the fieldname for the mqtt-topic value 'topic' as ( <code>string</code> ) base object for the output data-point undefined ssl( is_set ) whether to use ssl config: <code>mqtt.ssl.enable</code>/<code>FAXE_MQTT_SSL_ENABLE</code> <p>One of <code>topic</code>, <code>topics</code> must be specified.</p>"},{"location":"nodes/statistics/avg.html","title":"The avg node","text":"<p>Compute the average.</p> <p>See the stats node</p>"},{"location":"nodes/statistics/avg.html#example","title":"Example","text":"<pre><code>|avg()\n.field('temperature') \n</code></pre>"},{"location":"nodes/statistics/avg.html#parameters","title":"Parameters","text":"<p>all statistics nodes have the following parameters</p> Parameter Description Default field( <code>string</code> ) name of the field used for computation as( <code>string</code> ) name for the field for output values defaults to the name of the stats-node"},{"location":"nodes/statistics/bottom.html","title":"The sum node","text":"<p>Select the bottom num points for field.</p> <p>See the stats node</p>"},{"location":"nodes/statistics/bottom.html#example","title":"Example","text":"<pre><code>|bottom()\n.field('temperature') \n</code></pre>"},{"location":"nodes/statistics/bottom.html#parameters","title":"Parameters","text":"<p>all statistics nodes have the following parameters</p> Parameter Description Default field( <code>string</code> ) name of the field used for computation as( <code>string</code> ) name for the field for output values defaults to the name of the stats-node num( integer ) number of points to select 1"},{"location":"nodes/statistics/count.html","title":"The count node","text":"<p>Count the number of points.</p> <p>See the stats node</p>"},{"location":"nodes/statistics/count.html#example","title":"Example","text":"<pre><code>|count()\n.field('over_ts') \n</code></pre>"},{"location":"nodes/statistics/count.html#parameters","title":"Parameters","text":"<p>all statistics nodes have the following parameters</p> Parameter Description Default field( <code>string</code> ) name of the field used for computation as( <code>string</code> ) name for the field for output values defaults to the name of the stats-node"},{"location":"nodes/statistics/count_change.html","title":"The count_change node","text":"<p>Count the number value changes.</p> <p>See the stats node</p>"},{"location":"nodes/statistics/count_change.html#example","title":"Example","text":"<pre><code>|count_change()\n.field('value')\n.as('changed') \n</code></pre>"},{"location":"nodes/statistics/count_change.html#parameters","title":"Parameters","text":"<p>all statistics nodes have (at least) the following parameters</p> Parameter Description Default field( <code>string</code> ) name of the field used for computation as( <code>string</code> ) name for the field for output values defaults to the name of the stats-node"},{"location":"nodes/statistics/count_distinct.html","title":"The count node","text":"<p>Count the number of distinct values.</p> <p>See the stats node</p>"},{"location":"nodes/statistics/count_distinct.html#example","title":"Example","text":"<pre><code>|count()\n.field('product') \n.as('distinct_products')\n</code></pre>"},{"location":"nodes/statistics/count_distinct.html#parameters","title":"Parameters","text":"<p>all statistics nodes have (at least) the following parameters</p> Parameter Description Default field( <code>string</code> ) name of the field used for computation as( <code>string</code> ) name for the field for output values defaults to the name of the stats-node"},{"location":"nodes/statistics/distinct.html","title":"The distinct node","text":"<p>Select unique values.</p> <p>See the stats node</p>"},{"location":"nodes/statistics/distinct.html#example","title":"Example","text":"<pre><code>|distinct()\n.field('status') \n</code></pre>"},{"location":"nodes/statistics/distinct.html#parameters","title":"Parameters","text":"<p>all statistics nodes have the following parameters</p> Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node"},{"location":"nodes/statistics/elapsed.html","title":"The elapsed node","text":"<p>Compute the elapsed time between points.</p> <p>See the stats node</p>"},{"location":"nodes/statistics/elapsed.html#example","title":"Example","text":"<pre><code>|elapsed()\n.field('trigger') \n</code></pre>"},{"location":"nodes/statistics/elapsed.html#parameters","title":"Parameters","text":"<p>all statistics nodes have the following parameters</p> Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node"},{"location":"nodes/statistics/first.html","title":"The first node","text":"<p>Select the first that means the oldest point.</p> <p>See the stats node</p>"},{"location":"nodes/statistics/first.html#example","title":"Example","text":"<pre><code>|first()\n.field('temperature') \n</code></pre>"},{"location":"nodes/statistics/first.html#parameters","title":"Parameters","text":"<p>all statistics nodes have the following parameters</p> Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node"},{"location":"nodes/statistics/geometric_mean.html","title":"The geometric_mean node","text":"<p>Compute the geometric_mean.</p> <p>See the stats node</p>"},{"location":"nodes/statistics/geometric_mean.html#example","title":"Example","text":"<pre><code>|geometric_mean()\n.field('pressure') \n</code></pre>"},{"location":"nodes/statistics/geometric_mean.html#parameters","title":"Parameters","text":"<p>all statistics nodes have the following parameters</p> Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node"},{"location":"nodes/statistics/kurtosis.html","title":"The kurtosis node","text":"<p>Compute the kurtosis of data.</p> <p>See the stats node</p>"},{"location":"nodes/statistics/kurtosis.html#example","title":"Example","text":"<pre><code>|kurtosis()\n.field('temperature') \n</code></pre>"},{"location":"nodes/statistics/kurtosis.html#parameters","title":"Parameters","text":"<p>all statistics nodes have the following parameters</p> Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node"},{"location":"nodes/statistics/last.html","title":"The last node","text":"<p>Select the last, that means the newest point.</p> <p>See the stats node</p>"},{"location":"nodes/statistics/last.html#example","title":"Example","text":"<pre><code>|last()\n.field('chair') \n</code></pre>"},{"location":"nodes/statistics/last.html#parameters","title":"Parameters","text":"<p>all statistics nodes have the following parameters</p> Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node"},{"location":"nodes/statistics/max.html","title":"The max node","text":"<p>Compute the maximum value.</p> <p>See the stats node</p>"},{"location":"nodes/statistics/max.html#example","title":"Example","text":"<pre><code>|max()\n.field('temperature') \n</code></pre>"},{"location":"nodes/statistics/max.html#parameters","title":"Parameters","text":"<p>all statistics nodes have the following parameters</p> Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node"},{"location":"nodes/statistics/mean.html","title":"The mean node","text":"<p>Compute the mean of data.</p> <p>See the stats node</p>"},{"location":"nodes/statistics/mean.html#example","title":"Example","text":"<pre><code>|mean()\n.field('current') \n</code></pre>"},{"location":"nodes/statistics/mean.html#parameters","title":"Parameters","text":"<p>all statistics nodes have the following parameters</p> Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node"},{"location":"nodes/statistics/median.html","title":"The median node","text":"<p>Compute the median of data.</p> <p>See the stats node</p>"},{"location":"nodes/statistics/median.html#example","title":"Example","text":"<pre><code>|median()\n.field('temperature') \n</code></pre>"},{"location":"nodes/statistics/median.html#parameters","title":"Parameters","text":"<p>all statistics nodes have the following parameters</p> Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node"},{"location":"nodes/statistics/min.html","title":"The min node","text":"<p>Compute the minimum of data.</p> <p>See the stats node</p>"},{"location":"nodes/statistics/min.html#example","title":"Example","text":"<pre><code>|min()\n.field('temperature') \n</code></pre>"},{"location":"nodes/statistics/min.html#parameters","title":"Parameters","text":"<p>all statistics nodes have the following parameters</p> Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node"},{"location":"nodes/statistics/percentile.html","title":"The percentile node","text":"<p>Select a point at the given percentile. This is a selector function, no interpolation between points is performed. This node expects a data-batch item.</p>"},{"location":"nodes/statistics/percentile.html#example","title":"Example","text":"<pre><code>|percentile()\n.perc(95)\n.field('temperature') \n</code></pre>"},{"location":"nodes/statistics/percentile.html#parameters","title":"Parameters","text":"<p>all statistics nodes have the following parameters</p> Parameter Description Default field( <code>string</code> ) name of the field used for computation as( <code>string</code> ) name for the field for output values defaults to the name of the stats-node at ( <code>integer</code> ) select percentile 75"},{"location":"nodes/statistics/stddev.html","title":"The stddev node","text":"<p>Compute the standard deviation of the data.</p> <p>See the stats node</p>"},{"location":"nodes/statistics/stddev.html#example","title":"Example","text":"<pre><code>|stddev()\n.field('temperature') \n</code></pre>"},{"location":"nodes/statistics/stddev.html#parameters","title":"Parameters","text":"<p>all statistics nodes have the following parameters</p> Parameter Description Default field( <code>string</code> ) name of the field used for computation as( <code>string</code> ) name for the field for output values defaults to the name of the stats-node"},{"location":"nodes/statistics/sum.html","title":"The sum node","text":"<p>Compute the sum of data.</p> <p>See the stats node</p>"},{"location":"nodes/statistics/sum.html#example","title":"Example","text":"<pre><code>|sum()\n.field('temperature') \n</code></pre>"},{"location":"nodes/statistics/sum.html#parameters","title":"Parameters","text":"<p>all statistics nodes have the following parameters</p> Parameter Description Default field( <code>string</code> ) name of the field used for computation as( <code>string</code> ) name for the field for output values defaults to the name of the stats-node"},{"location":"nodes/statistics/top.html","title":"The top node","text":"<p>Select the top num points.</p> <p>See the stats node</p>"},{"location":"nodes/statistics/top.html#example","title":"Example","text":"<pre><code>|top()\n.field('temperature') \n</code></pre>"},{"location":"nodes/statistics/top.html#parameters","title":"Parameters","text":"<p>all statistics nodes have the following parameters</p> Parameter Description Default field( <code>string</code> ) name of the field used for computation as( <code>string</code> ) name for the field for output values defaults to the name of the stats-node num( <code>integer</code> ) number of points to select 1"},{"location":"nodes/statistics/variance.html","title":"The variance node","text":"<p>Compute the data's variance.</p> <p>See the stats node</p>"},{"location":"nodes/statistics/variance.html#example","title":"Example","text":"<pre><code>|variance()\n.field('temperature') \n</code></pre>"},{"location":"nodes/statistics/variance.html#parameters","title":"Parameters","text":"<p>all statistics nodes have the following parameters</p> Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node"},{"location":"nodes/window/win_clock.html","title":"The win_clock node","text":"<p>A window node is for batching data_points, therefore all window nodes will output <code>data_batch</code> items.</p> <p>This window-type has wall-clock timing, timestamps contained in incoming events are not relevant here.</p> <p>When the <code>align</code> option is true, window boundaries are aligned according to the <code>every</code> option, this means when every is 5s and an event comes into the window at time 15:03:27, this event will be member of the window that starts at 15:03:25, otherwise the window would start at 15:03:27. By default, the boundaries are defined relative to the first data point the window node receives.</p> <p>With <code>fill_period</code> given, the window will not emit before \"period\" time has elapsed (for the first time). This only applies if the <code>period</code> is greater than the <code>every</code> value.</p>"},{"location":"nodes/window/win_clock.html#example","title":"Example","text":"<pre><code>|win_clock()\n.every(5s)\n.period(15s)\n.fill_period()\n.align()\n</code></pre> <p>The window will emit every 5 seconds, but only after initially 15 seconds have passed (due to <code>fill_period</code>), it has its boundaries aligned to 5 second intervals.</p>"},{"location":"nodes/window/win_clock.html#parameters","title":"Parameters","text":"Parameter Description Default period( <code>duration</code> ) Window length defaults to <code>every</code> (giving us a tumbling window) every( <code>duration</code> ) Output window contents every align( is_set ) Align the window boundaries false (not set) fill_period( is_set ) Window output only when period time has elapsed false (not set)"},{"location":"nodes/window/win_event.html","title":"The win_event node","text":"<p>A window node is for batching data_points, therefore all window nodes will output <code>data_batch</code> items.</p> <p>This window holds <code>period</code> number of data_points and emits every <code>every</code> incoming point. </p> <p>With <code>fill_period</code> given, the window will only emit when it is filled with period points. This only applies if the <code>period</code> is greater than the <code>every</code> value.</p>"},{"location":"nodes/window/win_event.html#examples","title":"Examples","text":"<pre><code>|win_event()\n.every(5)\n.period(15)\n.fill_period() \n</code></pre> <p>The window will emit it's contents every 5 incoming points, but only after the window is filled with 15 points.</p> <pre><code>|win_event()\n.every(5)\n.period(15)\n</code></pre> <p>The window will emit it's contents every 5 incoming points. On first emit 5 points will be outputted, on the second emit 10 points will be emitted. From the third emit onwards, the window will output 15 points. Starting with the 4th emit, the window will output 15 data_points - with 10 old and 5 new  points (Sliding window).</p>"},{"location":"nodes/window/win_event.html#parameters","title":"Parameters","text":"Parameter Description Default period( <code>integer</code> ) Window length, number of points defaults to <code>every</code> (tumbling window) every( <code>integer</code> ) Output window contents every n incoming points fill_period( is_set ) Output only when window is filled false (not set)"},{"location":"nodes/window/win_session.html","title":"The win_session node","text":"<p><code>Experimental</code>.</p> <p>Since 0.17.2</p> <p>This window refers it's timing to the timestamp contained in the incoming data-items.</p> <p>A session window aggregates records into a session, which represents a period of activity separated by a specified gap of inactivity. Any data_points with timestamps that occur within the inactivity gap of existing sessions will be added to this session. If a data_point's timestamp occurs outside the session gap, a new session is created. A new session window starts if the last record that arrived is further back in time than the specified inactivity gap.</p>"},{"location":"nodes/window/win_session.html#example","title":"Example","text":"<pre><code>|value_emitter()\n.every(500ms)\n.jitter(4600ms)\n\n|win_session()\n.session_timeout(4500ms)\n\n|debug('info')\n</code></pre> <p>Every data_point, that has a timestamp &lt; last-timestamp + session_timeout, will be member of the current window.</p>"},{"location":"nodes/window/win_session.html#parameters","title":"Parameters","text":"Parameter Description Default session_timeout( <code>duration</code> ) also called <code>inactivity gap</code> 3m"},{"location":"nodes/window/win_time.html","title":"The win_time node","text":"<p>A window node is for batching data_points, therefore all window nodes will output <code>data_batch</code> items.</p> <p>This window refers it's timing to the timestamp contained in the incoming data-items.</p> <p>With <code>fill_period</code> given, the window will not emit before \"period\" time has elapsed (for the first time).</p> <p>Note that, since this window type does not rely on wall clock, but on the points timestamps, it is possible that no data is emitted, if there are no new points coming in.</p>"},{"location":"nodes/window/win_time.html#example","title":"Example","text":"<pre><code>|win_time()\n.every(5s)\n.period(15s) \n</code></pre> <p>The window will emit it's contents every 5 seconds.</p> <pre><code>|win_time()\n.every(1m) \n</code></pre> <p>Period is 1 minute here (period defaults to every)</p>"},{"location":"nodes/window/win_time.html#parameters","title":"Parameters","text":"Parameter Description Default period( <code>duration</code> ) Window length defaults to <code>every</code> (giving us a tumbling window) every( <code>duration</code> ) Output window contents every fill_period( is_set ) Window output only when period time has accumulated false (not set)"}]}