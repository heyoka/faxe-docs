{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"General Flow based data-collector and time-centric data-processor. Faxe's inner core is based on a dataflow computing engine and it's components also called nodes can be freely combined into an acyclic graph. Unlike other flowbased frameworks (node_red, ...) in Faxe computing graphs are built with a DSL called dfs . Rest Api FAXE can be managed via its rest api . General Data in faxe In faxe we deal with data_points and data_batches . These data-items are emitted by nodes Every data_point consists of a ts field, fields and tags . The value of the ts field is always: unix-timestamp in millisecond precision without a timezone . fields and tags are essentially key-value maps . Valid data-types for field and tag values are: string, integer, float, key-value map (also deeply nested) and lists . The only valid data-type for field and tag keys is string . A data_batch consists of a list of data_points ordered by timestamp. Most faxe nodes can deal with both points and batches. Value referencing As field and tag values can be deeply nested maps and lists, it is possible to use a JSON-path like syntax to declare and reference these values: Valid examples: averages axis.z.cur value.sub[2].data averages.emitted[5]","title":"General"},{"location":"index.html#general","text":"Flow based data-collector and time-centric data-processor. Faxe's inner core is based on a dataflow computing engine and it's components also called nodes can be freely combined into an acyclic graph. Unlike other flowbased frameworks (node_red, ...) in Faxe computing graphs are built with a DSL called dfs .","title":"General"},{"location":"index.html#rest-api","text":"FAXE can be managed via its rest api .","title":"Rest Api"},{"location":"index.html#general_1","text":"","title":"General"},{"location":"index.html#data-in-faxe","text":"In faxe we deal with data_points and data_batches . These data-items are emitted by nodes Every data_point consists of a ts field, fields and tags . The value of the ts field is always: unix-timestamp in millisecond precision without a timezone . fields and tags are essentially key-value maps . Valid data-types for field and tag values are: string, integer, float, key-value map (also deeply nested) and lists . The only valid data-type for field and tag keys is string . A data_batch consists of a list of data_points ordered by timestamp. Most faxe nodes can deal with both points and batches.","title":"Data in faxe"},{"location":"index.html#value-referencing","text":"As field and tag values can be deeply nested maps and lists, it is possible to use a JSON-path like syntax to declare and reference these values: Valid examples: averages axis.z.cur value.sub[2].data averages.emitted[5]","title":"Value referencing"},{"location":"configuration.html","text":"Configuration FAXE supports a sysctl like configuration syntax. Here are the simple rules of the syntax: Everything you need to know about a single setting is on one line Lines are structured Key = Value Any line starting with # is a comment, and will be ignored. Every config item can be overwritten with OS Environment variables (see 'ENV-Key'). ## Name of the Erlang node ## ## Default: faxe@127.0.0.1 ## ## ENV-Key: FAXE_NODENAME ## ## Acceptable values: ## - text nodename = faxe@127.0.0.1 ## Cookie for distributed node communication. All nodes in the ## same cluster should use the same cookie or they will not be able to ## communicate. ## ## Default: distrepl_proc_cookie ## ## ENV-Key: FAXE_DISTRIBUTED_COOKIE ## ## Acceptable values: ## - text distributed_cookie = distrepl_proc_cookie ## Base directory for mnesia files ## ## Default: ./mnesia_data ## ## ENV-Key: FAXE_MNESIA_DIR ## ## Acceptable values: ## - the path to a directory mnesia_dir = ./mnesia_data ## Sets the number of threads in async thread pool, valid range ## is 0-1024. If thread support is available, the default is 64. ## More information at: http://erlang.org/doc/man/erl.html ## ## Default: 64 ## ## ENV-Key: FAXE_ERLANG_ASYNC_THREADS ## ## Acceptable values: ## - an integer erlang.async_threads = 64 ## The number of concurrent ports/sockets ## Valid range is 1024-134217727 ## ## Default: 262144 ## ## ENV-Key: FAXE_ERLANG_MAX_PORTS ## ## Acceptable values: ## - an integer erlang.max_ports = 262144 ## -------------------------------------------------------------------------- ## Erlangs timewarp and time-correction behaviour ## More info: https://www.erlang.org/doc/apps/erts/time_correction.html#Multi_Time_Warp_Mode ## ------------------------------------------------------------------------------ ## What time wrap mode to use for the erlang runtime system ## ## Default: multi_time_warp ## ## ENV-Key: FAXE_ERLANG_TIME_WARP_MODE ## ## Acceptable values: ## - one of: no_time_warp, single_time_warp, multi_time_warp ## erlang.time.warp_mode = multi_time_warp ## Whether to use time correction ## ## Default: false ## ## ENV-Key: FAXE_ERLANG_TIME_CORRECTION ## ## Acceptable values: ## - one of: true, false ## erlang.time.correction = false ## -------------------------------------------------------------------------- ## Erlangs scheduler busy wait threshold ## More info: https://www.erlang.org/doc/man/erl.html ## ------------------------------------------------------------------------------ ## scheduler busy wait threshold ## ## Default: medium ## ## ENV-Key: FAXE_ERLANG_BUSY_WAIT_CPU_SCHEDULER ## ## Acceptable values: ## - one of: none, very_short, short, medium, long, very_long ## erlang.busy_wait.cpu_scheduler = medium ## ## Default: short ## ## ENV-Key: FAXE_ERLANG_BUSY_WAIT_DIRTY_CPU_SCHEDULER ## ## Acceptable values: ## - one of: none, very_short, short, medium, long, very_long ## erlang.busy_wait.dirty_cpu_scheduler = short ## ## Default: short ## ## ENV-Key: FAXE_ERLANG_BUSY_WAIT_DIRTY_IO_SCHEDULER ## ## Acceptable values: ## - one of: none, very_short, short, medium, long, very_long ## erlang.busy_wait.dirty_io_scheduler = short ## -------------------------------------------------------------- ## LOGGING ## -------------------------------------------------------------- ## set the logging level for console ## ## Default: info ## ## ENV-Key: FAXE_LOG_CONSOLE_LEVEL ## ## Acceptable values: ## - one of: debug, info, notice, warning, error, alert ## log.console_level = info ## set the log level for the emit backend ## ## Default: warning ## ## ENV-Key: FAXE_LOG_EMIT_LEVEL ## ## Acceptable values: ## - one of: debug, info, notice, warning, error, alert ## log.emit_level = warning ## whether to send logs to logstash ## logs will be sent via a udp or tcp socket to the configured logstash host ## ## Default: off ## ## ENV-Key: FAXE_LOG_LOGSTASH_BACKEND_ENABLE ## ## Acceptable values: ## - on or off log.logstash_backend_enable = off ## whether to send logs to logstash using the udp or tcp protocol ## ## Default: udp ## ## ENV-Key: FAXE_LOG_LOGSTASH_BACKEND_PROTOCOL ## ## Acceptable values: ## - one of: udp, tcp ## log.logstash_backend_protocol = udp ## enable/disable tls ## enable the use of tls for the logstash handler, if tcp is used ## ## Default: off ## ## ENV-Key: FAXE_LOG_LOGSTASH_BACKEND_SSL_ENABLE ## ## Acceptable values: ## - on or off ## log.logstash_backend.ssl_enable = off ## logstash host name or address ## ## Default: 127.0.0.1 ## ## ENV-Key: FAXE_LOG_LOGSTASH_HOST ## ## Acceptable values: ## - text ## log.logstash_host = 127.0.0.1 ## logstash port ## ## Default: 9125 ## ## ENV-Key: FAXE_LOG_LOGSTASH_PORT ## ## Acceptable values: ## - an integer ## log.logstash_port = 9125 ## set the log level for the logstash backend ## ## Default: info ## ## ENV-Key: FAXE_LOG_LOGSTASH_LEVEL ## ## Acceptable values: ## - one of: debug, info, notice, warning, error, alert ## log.logstash_level = info ## -------------------------------------------------------------- ## AUTO START faxe flows (tasks) ## -------------------------------------------------------------- ## whether to start tasks marked \"permanent\" automatically on node startup ## ## Default: off ## ## ENV-Key: FAXE_FLOW_AUTO_START ## ## Acceptable values: ## - on or off ## flow_auto_start = off ## -------------------------------------------------------------- ## AUTO RELOAD faxe flows (tasks) ## -------------------------------------------------------------- ## whether to reload all tasks automatically on node startup ## ## Default: off ## ## ENV-Key: FAXE_FLOW_AUTO_RELOAD ## ## Acceptable values: ## - on or off ## flow_auto_reload = off ## -------------------------------------------------------------- ## DFS ## -------------------------------------------------------------- ## path to folder where dfs scripts live ## ## Default: /home/heyoka/workspace/faxe/dfs/ ## ## ENV-Key: FAXE_DFS_SCRIPT_PATH ## ## Acceptable values: ## - the path to a directory ## dfs.script_path = /home/heyoka/workspace/faxe/dfs/ ## ---------------------------------------------------------------- ## API USER - default user, that will be created on first startup ## ---------------------------------------------------------------- ## anonymous access to the api endpoint ## set to false for production use ## ## Default: true ## ## ENV-Key: FAXE_ALLOW_ANONYMOUS ## ## Acceptable values: ## - true or false allow_anonymous = true ## ## Default: user ## ## ENV-Key: FAXE_DEFAULT_USERNAME ## ## Acceptable values: ## - text ## default_username = user ## ## Default: pass ## ## ENV-Key: FAXE_DEFAULT_PASSWORD ## ## Acceptable values: ## - text ## default_password = pass ## ## Default: false ## ## ENV-Key: FAXE_RESET_USER_ON_STARTUP ## ## Acceptable values: ## - true or false ## reset_user_on_startup = false ## ---------------------------------------------------------------- ## API AUTH with JWT ## ---------------------------------------------------------------- ## ## Default: /path/to/cacertfile.pem ## ## ENV-Key: FAXE_HTTP_API_JWT_PUBLIC_KEY_FILE ## ## Acceptable values: ## - the path to a file ## http_api.jwt.public_key_file = /path/to/cacertfile.pem ## ---------------------------------------------------------------- ## REST API ## ---------------------------------------------------------------- ## http port for rest api endpoint ## ## Default: 8081 ## ## ENV-Key: FAXE_HTTP_API_PORT ## ## Acceptable values: ## - an integer http_api_port = 8081 ## ## Default: 3000000 ## ## ENV-Key: FAXE_HTTP_API_MAX_UPLOAD_SIZE ## ## Acceptable values: ## - an integer http_api.max_upload_size = 3000000 ## http-api tls ## enable the use of tls for the http-api ## ## Default: off ## ## ENV-Key: FAXE_HTTP_API_TLS_ENABLE ## ## Acceptable values: ## - on or off ## http_api.tls.enable = on ## http-api ssl certificate ## ## Default: /path/to/certfile.pem ## ## ENV-Key: FAXE_HTTP_API_SSL_CERTFILE ## ## Acceptable values: ## - the path to a file ## http_api.ssl.certfile = /path/to/certfile.pem ## http-api ssl ca certificate ## ## Default: /path/to/cacertfile.pem ## ## ENV-Key: FAXE_HTTP_API_SSL_CACERTFILE ## ## Acceptable values: ## - the path to a file ## http_api.ssl.cacertfile = /path/to/cacertfile.pem ## http-api ssl key file ## ## Default: /path/to/keyfile.key ## ## ENV-Key: FAXE_HTTP_API_SSL_KEYFILE ## ## Acceptable values: ## - the path to a file ## http_api.ssl.keyfile = /path/to/cert.key ## a list of ciphers to use for the http listener ## ## Default: ECDHE-RSA-AES256-GCM-SHA384, ECDHE-RSA-AES128-GCM-SHA256, DHE-RSA-AES256-GCM-SHA384, DHE-RSA-AES128-GCM-SHA256, TLS_AES_256_GCM_SHA384,ECDHE-RSA-AES256-GCM-SHA384 ## ## ENV-Key: FAXE_HTTP_API_CIPHERS ## ## Acceptable values: ## - text ## http_api.ciphers = ECDHE-RSA-AES256-GCM-SHA384, ECDHE-RSA-AES128-GCM-SHA256, DHE-RSA-AES256-GCM-SHA384, DHE-RSA-AES128-GCM-SHA256, TLS_AES_256_GCM_SHA384,ECDHE-RSA-AES256-GCM-SHA384 ## ----------------------------------------------------------- ## PYTHON ## ----------------------------------------------------------------- ## python version ## ## Default: 3 ## ## ENV-Key: FAXE_PYTHON_VERSION ## ## Acceptable values: ## - text ## python.version = 3 ## path to custom python files ## ## Default: /home/heyoka/workspace/faxe/python/ ## ## ENV-Key: FAXE_PYTHON_SCRIPT_PATH ## ## Acceptable values: ## - the path to a directory python.script_path = /home/heyoka/workspace/faxe/python/ ## ------------------------------------------------------------------- ## ESQ ## ------------------------------------------------------------------- ## several faxe nodes use persistent queues for safe data-delivery and buffering in ## case any upstream services are disconnected temporarily ## These queues can be configured with the following settings. ## base directory for persistent queues ## ## Default: /tmp ## ## ENV-Key: FAXE_QUEUE_BASE_DIR ## ## Acceptable values: ## - the path to a directory queue_base_dir = /tmp ## queue message time to live ## expired messages are evicted from queue ## ## Default: 4h ## ## ENV-Key: FAXE_QUEUE_TTL ## ## Acceptable values: ## - a time duration with units, e.g. '10s' for 10 seconds queue_ttl = 4h ## queue sync to disc interval ## queue time-to-sync (rotate) file segments. ## Any enqueued message might remain invisible until sync is performed. ## ## Default: 300ms ## ## ENV-Key: FAXE_QUEUE_TTS ## ## Acceptable values: ## - a time duration with units, e.g. '10s' for 10 seconds queue_tts = 300ms ## queue time to flight ## ttf message time-to-flight in milliseconds, ## the time required to deliver message acknowledgment before it reappears to client(s) again. ## ## Default: 20000ms ## ## ENV-Key: FAXE_QUEUE_TTF ## ## Acceptable values: ## - a time duration with units, e.g. '10s' for 10 seconds queue_ttf = 20000ms ## capacity defines the size of in-memory queue. ## The queue would not fetch anything from disk into memory buffer if capacity is 0. ## ## Default: 30 ## ## ENV-Key: FAXE_QUEUE_CAPACITY ## ## Acceptable values: ## - an integer queue_capacity = 30 ## dequeue interval. ## Start interval at which the queue is asked for an element. ## ## Default: 15ms ## ## ENV-Key: FAXE_DEQUEUE_INTERVAL ## ## Acceptable values: ## - a time duration with units, e.g. '10s' for 10 seconds dequeue_interval = 15ms ## dequeue min interval. ## Min interval at which the queue is asked for an element. ## ## Default: 3ms ## ## ENV-Key: FAXE_DEQUEUE_MIN_INTERVAL ## ## Acceptable values: ## - a time duration with units, e.g. '10s' for 10 seconds dequeue.min_interval = 3ms ## dequeue max interval. ## Max interval at which the queue is asked for an element. ## ## Default: 200ms ## ## ENV-Key: FAXE_DEQUEUE_MAX_INTERVAL ## ## Acceptable values: ## - a time duration with units, e.g. '10s' for 10 seconds dequeue.max_interval = 200ms ## interval change step size. ## Step size for dequeue interval changes ## ## Default: 3 ## ## ENV-Key: FAXE_DEQUEUE_STEP_SIZE ## ## Acceptable values: ## - an integer dequeue.step_size = 3 ## ------------------------------------------------------------------------- ## S7 DEFAULTS ## ------------------------------------------------------------------------- ## for every unique ip address used by s7_read nodes, ## faxe will maintain a separate connection pool, ## each pool will have at least 's7pool.min_size' connections ## and a maximum of 's7pool.max_size' connections ## s7 connection pool min size ## ## Default: 2 ## ## ENV-Key: FAXE_S7POOL_MIN_SIZE ## ## Acceptable values: ## - an integer s7pool.min_size = 2 ## s7 connection pool max size ## ## Default: 16 ## ## ENV-Key: FAXE_S7POOL_MAX_SIZE ## ## Acceptable values: ## - an integer s7pool.max_size = 16 ## whether to use the s7 pool (default for all s7read nodes) ## ## Default: off ## ## ENV-Key: FAXE_S7POOL_ENABLE ## ## Acceptable values: ## - on or off s7pool.enable = off ## whether to use the optimized s7 reader ## ## Default: off ## ## ENV-Key: FAXE_S7READER_OPTIMIZED ## ## Acceptable values: ## - on or off s7reader.optimized = off ## ------------------------------------------------------------------------------- ## MQTT defaults ## ------------------------------------------------------------------------------- ## ## Default: on ## ## ENV-Key: FAXE_MQTT_POOL_ENABLE ## ## Acceptable values: ## - on or off mqtt_pool.enable = on ## max size (maximum number of connections) for the mqtt connection pool ## ## Default: 30 ## ## ENV-Key: FAXE_MQTT_POOL_MAX_SIZE ## ## Acceptable values: ## - an integer mqtt_pool.max_size = 30 ## ## Default: 10.14.204.20 ## ## ENV-Key: FAXE_MQTT_HOST ## ## Acceptable values: ## - text mqtt.host = 10.14.204.20 ## mqtt port ## ## Default: 1883 ## ## ENV-Key: FAXE_MQTT_PORT ## ## Acceptable values: ## - an integer mqtt.port = 1883 ## mqtt user ## ## Default: username ## ## ENV-Key: FAXE_MQTT_USER ## ## Acceptable values: ## - text ## mqtt.user = username ## mqtt pass ## ## Default: password ## ## ENV-Key: FAXE_MQTT_PASS ## ## Acceptable values: ## - text ## mqtt.pass = password ## mqtt ssl ## enable the use of ssl for mqtt connections ## ## Default: off ## ## ENV-Key: FAXE_MQTT_SSL_ENABLE ## ## Acceptable values: ## - on or off ## mqtt.ssl.enable = off ## mqtt ssl peer verification ## ## Default: verify_none ## ## ENV-Key: FAXE_MQTT_SSL_VERIFY ## ## Acceptable values: ## - one of: verify_none, verify_peer ## mqtt.ssl.verify = verify_none ## mqtt ssl certificate ## ## Default: ## ## ENV-Key: FAXE_MQTT_SSL_CERTFILE ## ## Acceptable values: ## - the path to a file ## mqtt.ssl.certfile = /path/to/certfile.pem ## mqtt ssl ca certificate ## ## Default: ## ## ENV-Key: FAXE_MQTT_SSL_CACERTFILE ## ## Acceptable values: ## - the path to a file ## mqtt.ssl.cacertfile = /path/to/cacertfile.pem ## mqtt ssl key file ## ## Default: ## ## ENV-Key: FAXE_MQTT_SSL_KEYFILE ## ## Acceptable values: ## - the path to a file ## mqtt.ssl.keyfile = /path/to/cert.key ## ## Default: 2 ## ## ENV-Key: FAXE_MQTT_PUB_POOL_MIN_SIZE ## ## Acceptable values: ## - an integer mqtt_pub_pool.min_size = 2 ## mqtt publisher connection pool max size ## ## Default: 30 ## ## ENV-Key: FAXE_MQTT_PUB_POOL_MAX_SIZE ## ## Acceptable values: ## - an integer mqtt_pub_pool.max_size = 30 ## mqtt publisher connection pool max worker rate (message throughput per second) ## this is used for the elastic pool grow/shrink mechanism ## ## Default: 70 ## ## ENV-Key: FAXE_MQTT_PUB_POOL_WORKER_MAX_RATE ## ## Acceptable values: ## - an integer mqtt_pub_pool.worker_max_rate = 70 ## whether to use the s7 pool (default for all s7read nodes) ## ## Default: on ## ## ENV-Key: FAXE_MQTT_PUB_POOL_ENABLE ## ## Acceptable values: ## - on or off mqtt_pub_pool.enable = on ## ------------------------------------------------------------------------------- ## AMQP defaults ## ------------------------------------------------------------------------------- ## amqp host ## ## Default: 10.14.204.28 ## ## ENV-Key: FAXE_AMQP_HOST ## ## Acceptable values: ## - text amqp.host = 10.14.204.28 ## amqp port ## ## Default: 5672 ## ## ENV-Key: FAXE_AMQP_PORT ## ## Acceptable values: ## - an integer amqp.port = 5672 ## amqp user ## ## Default: guest ## ## ENV-Key: FAXE_AMQP_USER ## ## Acceptable values: ## - text ## amqp.user = username ## amqp pass ## ## Default: guest ## ## ENV-Key: FAXE_AMQP_PASS ## ## Acceptable values: ## - text ## amqp.pass = password ## amqp heartbeat interval ## ## Default: 60s ## ## ENV-Key: FAXE_AMQP_HEARTBEAT ## ## Acceptable values: ## - a time duration with units, e.g. '10s' for 10 seconds ## amqp.heartbeat = 60s ## amqp ssl ## enable the use of ssl for amqp connections ## ## Default: off ## ## ENV-Key: FAXE_AMQP_SSL_ENABLE ## ## Acceptable values: ## - on or off ## amqp.ssl.enable = off ## amqp ssl certificate ## ## Default: ## ## ENV-Key: FAXE_AMQP_SSL_CERTFILE ## ## Acceptable values: ## - the path to a file ## amqp.ssl.certfile = /path/to/certfile.pem ## amqp ssl ca certificate ## ## Default: ## ## ENV-Key: FAXE_AMQP_SSL_CACERTFILE ## ## Acceptable values: ## - the path to a file ## amqp.ssl.cacertfile = /path/to/cacertfile.pem ## amqp ssl key file ## ## Default: ## ## ENV-Key: FAXE_AMQP_SSL_KEYFILE ## ## Acceptable values: ## - the path to a file ## amqp.ssl.keyfile = /path/to/cert.key ## amqp ssl peer verification ## ## Default: verify_none ## ## ENV-Key: FAXE_AMQP_SSL_VERIFY ## ## Acceptable values: ## - one of: verify_none, verify_peer ## amqp.ssl.verify = verify_none ## ------------------------------------------------------------------------------- ## RabbitMQ defaults ## ------------------------------------------------------------------------------- ## rabbitmq default exchange ## the amqp_publish node will use this exchange as default ## ## Default: x_lm_fanout ## ## ENV-Key: FAXE_RABBITMQ_ROOT_EXCHANGE ## ## Acceptable values: ## - text rabbitmq.root_exchange = x_lm_fanout ## ## Default: q_ ## ## ENV-Key: FAXE_RABBITMQ_QUEUE_PREFIX ## ## Acceptable values: ## - text rabbitmq.queue_prefix = q_ ## ## Default: x_ ## ## ENV-Key: FAXE_RABBITMQ_EXCHANGE_PREFIX ## ## Acceptable values: ## - text rabbitmq.exchange_prefix = x_ ## ------------------------------------------------------------------------------- ## CrateDB defaults (postgreSQL connect) ## ------------------------------------------------------------------------------- ## CrateDB host ## ## Default: crate.example.com ## ## ENV-Key: FAXE_CRATE_HOST ## ## Acceptable values: ## - text crate.host = crate.example.com ## CrateDB port ## ## Default: 5432 ## ## ENV-Key: FAXE_CRATE_PORT ## ## Acceptable values: ## - an integer crate.port = 5432 ## CrateDB user ## ## Default: crate ## ## ENV-Key: FAXE_CRATE_USER ## ## Acceptable values: ## - text crate.user = crate ## CrateDB password ## ## Default: ## ## ENV-Key: FAXE_CRATE_PASS ## ## Acceptable values: ## - text ## crate.pass = pass ## CrateDB database ## ## Default: doc ## ## ENV-Key: FAXE_CRATE_DATABASE ## ## Acceptable values: ## - text crate.database = doc ## crate tls ## enable the use of tls for crate postgre connections ## ## Default: off ## ## ENV-Key: FAXE_CRATE_TLS_ENABLE ## ## Acceptable values: ## - on or off ## crate.tls.enable = off ## ------------------------------------------------------------------------------- ## CrateDB defaults (http api) ## ------------------------------------------------------------------------------- ## CrateDB host ## ## Default: 10.14.204.10 ## ## ENV-Key: FAXE_CRATE_HTTP_HOST ## ## Acceptable values: ## - text crate_http.host = 10.14.204.10 ## CrateDB port ## ## Default: 4200 ## ## ENV-Key: FAXE_CRATE_HTTP_PORT ## ## Acceptable values: ## - an integer crate_http.port = 4200 ## CrateDB user ## ## Default: crate ## ## ENV-Key: FAXE_CRATE_HTTP_USER ## ## Acceptable values: ## - text crate_http.user = crate ## CrateDB password ## ## Default: ## ## ENV-Key: FAXE_CRATE_HTTP_PASS ## ## Acceptable values: ## - text ## crate_http.pass = pass ## CrateDB database ## ## Default: doc ## ## ENV-Key: FAXE_CRATE_HTTP_DATABASE ## ## Acceptable values: ## - text crate_http.database = doc ## crate tls ## enable the use of tls for crate http connections ## ## Default: off ## ## ENV-Key: FAXE_CRATE_HTTP_TLS_ENABLE ## ## Acceptable values: ## - on or off ## crate_http.tls.enable = off ## ------------------------------------------------------------------------------- ## InfluxDB defaults (http api) ## ------------------------------------------------------------------------------- ## InfluxDB host ## ## Default: influx.example.com ## ## ENV-Key: FAXE_INFLUX_HTTP_HOST ## ## Acceptable values: ## - text influx_http.host = influx.example.com ## InfluxDB port ## ## Default: 8086 ## ## ENV-Key: FAXE_INFLUX_HTTP_PORT ## ## Acceptable values: ## - an integer influx_http.port = 8086 ## InfluxDB user ## ## Default: influx ## ## ENV-Key: FAXE_INFLUX_HTTP_USER ## ## Acceptable values: ## - text influx_http.user = influx ## InfluxDB pass ## ## Default: ## ## ENV-Key: FAXE_INFLUX_HTTP_PASS ## ## Acceptable values: ## - text ## influx_http.pass = password ## ---------------------------------------------------------------------------- ## EMAIL defaults ## ---------------------------------------------------------------------------- ## email from address ## ## Default: noreply@example.com ## ## ENV-Key: FAXE_EMAIL_FROM ## ## Acceptable values: ## - text email.from = noreply@example.com ## email smtp relay ## ## Default: smtp.example.com ## ## ENV-Key: FAXE_EMAIL_SMTP ## ## Acceptable values: ## - text email.smtp = smtp.example.com ## email smtp port ## ## Default: 25 ## ## ENV-Key: FAXE_EMAIL_PORT ## ## Acceptable values: ## - an integer email.port = 25 ## email smtp tls, whether to use tls ## ## Default: off ## ## ENV-Key: FAXE_EMAIL_TLS ## ## Acceptable values: ## - on or off email.tls = off ## email smtp user ## ## Default: username ## ## ENV-Key: FAXE_EMAIL_USER ## ## Acceptable values: ## - text ## email.user = username ## email smtp pass ## ## Default: password ## ## ENV-Key: FAXE_EMAIL_PASS ## ## Acceptable values: ## - text ## email.pass = password ## email html template ## ## Default: /home/user/template.html ## ## ENV-Key: FAXE_EMAIL_TEMPLATE ## ## Acceptable values: ## - text email.template = /home/user/template.html ## -------------------------------------------------------------------------- ## AZURE BLOB ## ------------------------------------------------------------------------------- ## account-url ## ## Default: https://someblob.blob.core.windows.net ## ## ENV-Key: FAXE_AZURE_BLOB_ACCOUNT_URL ## ## Acceptable values: ## - text azure_blob.account_url = https://someblob.blob.core.windows.net ## ## Default: azblob-secret ## ## ENV-Key: FAXE_AZURE_BLOB_ACCOUNT_SECRET ## ## Acceptable values: ## - text azure_blob.account_secret = azblob-secret ## ## -------------------------------------------------------------------- ## DEBUG, LOGS, METRICS, CONNECTION STATUS, FLOW_CHANGED ## -------------------------------------------------------------------- ## There are mqtt handlers for debug, logs, metrics, connection-status and flow_changed events. ## Note that the base options for these mqtt connections come from the 'mqtt' options above. ## If needed you can override these default mqtt-options for every handler type. ## ## Default: ## ## ENV-Key: FAXE_REPORT_DEBUG_MQTT_HOST ## ## Acceptable values: ## - text ## report_debug.mqtt_host = example.com ## ## ----------------------- METRICS ------------------------------ ## Metrics handler MQTT sends metric events to an mqtt broker ## ## Default: off ## ## ENV-Key: FAXE_METRICS_HANDLER_MQTT_ENABLE ## ## Acceptable values: ## - on or off metrics.handler.mqtt.enable = off ## metrics handler mqtt host ## ## Default: ## ## ENV-Key: FAXE_METRICS_HANDLER_MQTT_HOST ## ## Acceptable values: ## - text ## metrics.handler.mqtt.host = example.com ## metrics handler mqtt port ## ## ENV-Key: FAXE_METRICS_HANDLER_MQTT_PORT ## ## Acceptable values: ## - an integer ## metrics.handler.mqtt.port = 1883 ## metrics handler mqtt base topic ## The mqtt handler will prefix its topic with this value, ## note that it must be a valid mqtt topic string. ## ## Default: sys/faxe ## ## ENV-Key: FAXE_METRICS_HANDLER_MQTT_BASE_TOPIC ## ## Acceptable values: ## - text ## metrics.handler.mqtt.base_topic = sys/faxe ## flow-metrics publish interval ## Interval at which flow-metrics get publish to the handler ## ## Default: 30s ## ## ENV-Key: FAXE_METRICS_PUBLISH_INTERVAL ## ## Acceptable values: ## - text metrics.publish_interval = 30s ## ----------------------- CONNECTION STATUS ------------------------ ## Conn_status handler MQTT sends connection status events to an mqtt broker. ## ## Default: on ## ## ENV-Key: FAXE_CONN_STATUS_HANDLER_MQTT_ENABLE ## ## Acceptable values: ## - on or off conn_status.handler.mqtt.enable = on ## ## Default: ## ## ENV-Key: FAXE_CONN_STATUS_HANDLER_MQTT_HOST ## ## Acceptable values: ## - text ## conn_status.handler.mqtt.host = example.com ## connection status handler mqtt port ## ## ENV-Key: FAXE_CONN_STATUS_HANDLER_MQTT_PORT ## ## Acceptable values: ## - an integer ## conn_status.handler.mqtt.port = 1883 ## connection status handler mqtt base topic ## ## Default: sys/faxe ## ## ENV-Key: FAXE_CONN_STATUS_HANDLER_MQTT_BASE_TOPIC ## ## Acceptable values: ## - text ## conn_status.handler.mqtt.base_topic = sys/faxe ## ----------------------- DEBUG AND TRACE -------------------------- ## Debug trace handler MQTT ## enable/disable debug_trace handler mqtt ## ## Default: off ## ## ENV-Key: FAXE_DEBUG_HANDLER_MQTT_ENABLE ## ## Acceptable values: ## - on or off debug.handler.mqtt.enable = off ## debug_trace handler mqtt host ## ## Default: ## ## ENV-Key: FAXE_DEBUG_HANDLER_MQTT_HOST ## ## Acceptable values: ## - text ## debug.handler.mqtt.host = example.com ## debug_trace handler mqtt port ## ## ENV-Key: FAXE_DEBUG_HANDLER_MQTT_PORT ## ## Acceptable values: ## - an integer ## debug.handler.mqtt.port = 1883 ## debug_trace handler mqtt base topic ## ## Default: sys/faxe ## ## ENV-Key: FAXE_DEBUG_HANDLER_MQTT_BASE_TOPIC ## ## Acceptable values: ## - text ## debug.handler.mqtt.base_topic = sys/faxe ## time debug and node-metric messages will be published to the configured endpoints ## ## Default: 3m ## ## ENV-Key: FAXE_DEBUG_TIME ## ## Acceptable values: ## - a time duration with units, e.g. '10s' for 10 seconds debug.time = 3m ## ----------------------- FLOW_CHANGED -------------------------- ## flow_changed handler MQTT ## enable/disable flow_changed handler mqtt ## ## Default: off ## ## ENV-Key: FAXE_FLOW_CHANGED_HANDLER_MQTT_ENABLE ## ## Acceptable values: ## - on or off flow_changed.handler.mqtt.enable = off ## flow_changed handler mqtt host ## ## Default: ## ## ENV-Key: FAXE_FLOW_CHANGED_HANDLER_MQTT_HOST ## ## Acceptable values: ## - text ## flow_changed.handler.mqtt.host = example.com ## flow_changed handler mqtt port ## ## ENV-Key: FAXE_FLOW_CHANGED_HANDLER_MQTT_PORT ## ## Acceptable values: ## - an integer ## flow_changed.handler.mqtt.port = 1883 ## flow_changed handler mqtt base topic ## ## Default: sys/faxe ## ## ENV-Key: FAXE_FLOW_CHANGED_HANDLER_MQTT_BASE_TOPIC ## ## Acceptable values: ## - text ## flow_changed.handler.mqtt.base_topic = sys/faxe","title":"Configuration"},{"location":"configuration.html#configuration","text":"FAXE supports a sysctl like configuration syntax. Here are the simple rules of the syntax: Everything you need to know about a single setting is on one line Lines are structured Key = Value Any line starting with # is a comment, and will be ignored. Every config item can be overwritten with OS Environment variables (see 'ENV-Key'). ## Name of the Erlang node ## ## Default: faxe@127.0.0.1 ## ## ENV-Key: FAXE_NODENAME ## ## Acceptable values: ## - text nodename = faxe@127.0.0.1 ## Cookie for distributed node communication. All nodes in the ## same cluster should use the same cookie or they will not be able to ## communicate. ## ## Default: distrepl_proc_cookie ## ## ENV-Key: FAXE_DISTRIBUTED_COOKIE ## ## Acceptable values: ## - text distributed_cookie = distrepl_proc_cookie ## Base directory for mnesia files ## ## Default: ./mnesia_data ## ## ENV-Key: FAXE_MNESIA_DIR ## ## Acceptable values: ## - the path to a directory mnesia_dir = ./mnesia_data ## Sets the number of threads in async thread pool, valid range ## is 0-1024. If thread support is available, the default is 64. ## More information at: http://erlang.org/doc/man/erl.html ## ## Default: 64 ## ## ENV-Key: FAXE_ERLANG_ASYNC_THREADS ## ## Acceptable values: ## - an integer erlang.async_threads = 64 ## The number of concurrent ports/sockets ## Valid range is 1024-134217727 ## ## Default: 262144 ## ## ENV-Key: FAXE_ERLANG_MAX_PORTS ## ## Acceptable values: ## - an integer erlang.max_ports = 262144 ## -------------------------------------------------------------------------- ## Erlangs timewarp and time-correction behaviour ## More info: https://www.erlang.org/doc/apps/erts/time_correction.html#Multi_Time_Warp_Mode ## ------------------------------------------------------------------------------ ## What time wrap mode to use for the erlang runtime system ## ## Default: multi_time_warp ## ## ENV-Key: FAXE_ERLANG_TIME_WARP_MODE ## ## Acceptable values: ## - one of: no_time_warp, single_time_warp, multi_time_warp ## erlang.time.warp_mode = multi_time_warp ## Whether to use time correction ## ## Default: false ## ## ENV-Key: FAXE_ERLANG_TIME_CORRECTION ## ## Acceptable values: ## - one of: true, false ## erlang.time.correction = false ## -------------------------------------------------------------------------- ## Erlangs scheduler busy wait threshold ## More info: https://www.erlang.org/doc/man/erl.html ## ------------------------------------------------------------------------------ ## scheduler busy wait threshold ## ## Default: medium ## ## ENV-Key: FAXE_ERLANG_BUSY_WAIT_CPU_SCHEDULER ## ## Acceptable values: ## - one of: none, very_short, short, medium, long, very_long ## erlang.busy_wait.cpu_scheduler = medium ## ## Default: short ## ## ENV-Key: FAXE_ERLANG_BUSY_WAIT_DIRTY_CPU_SCHEDULER ## ## Acceptable values: ## - one of: none, very_short, short, medium, long, very_long ## erlang.busy_wait.dirty_cpu_scheduler = short ## ## Default: short ## ## ENV-Key: FAXE_ERLANG_BUSY_WAIT_DIRTY_IO_SCHEDULER ## ## Acceptable values: ## - one of: none, very_short, short, medium, long, very_long ## erlang.busy_wait.dirty_io_scheduler = short ## -------------------------------------------------------------- ## LOGGING ## -------------------------------------------------------------- ## set the logging level for console ## ## Default: info ## ## ENV-Key: FAXE_LOG_CONSOLE_LEVEL ## ## Acceptable values: ## - one of: debug, info, notice, warning, error, alert ## log.console_level = info ## set the log level for the emit backend ## ## Default: warning ## ## ENV-Key: FAXE_LOG_EMIT_LEVEL ## ## Acceptable values: ## - one of: debug, info, notice, warning, error, alert ## log.emit_level = warning ## whether to send logs to logstash ## logs will be sent via a udp or tcp socket to the configured logstash host ## ## Default: off ## ## ENV-Key: FAXE_LOG_LOGSTASH_BACKEND_ENABLE ## ## Acceptable values: ## - on or off log.logstash_backend_enable = off ## whether to send logs to logstash using the udp or tcp protocol ## ## Default: udp ## ## ENV-Key: FAXE_LOG_LOGSTASH_BACKEND_PROTOCOL ## ## Acceptable values: ## - one of: udp, tcp ## log.logstash_backend_protocol = udp ## enable/disable tls ## enable the use of tls for the logstash handler, if tcp is used ## ## Default: off ## ## ENV-Key: FAXE_LOG_LOGSTASH_BACKEND_SSL_ENABLE ## ## Acceptable values: ## - on or off ## log.logstash_backend.ssl_enable = off ## logstash host name or address ## ## Default: 127.0.0.1 ## ## ENV-Key: FAXE_LOG_LOGSTASH_HOST ## ## Acceptable values: ## - text ## log.logstash_host = 127.0.0.1 ## logstash port ## ## Default: 9125 ## ## ENV-Key: FAXE_LOG_LOGSTASH_PORT ## ## Acceptable values: ## - an integer ## log.logstash_port = 9125 ## set the log level for the logstash backend ## ## Default: info ## ## ENV-Key: FAXE_LOG_LOGSTASH_LEVEL ## ## Acceptable values: ## - one of: debug, info, notice, warning, error, alert ## log.logstash_level = info ## -------------------------------------------------------------- ## AUTO START faxe flows (tasks) ## -------------------------------------------------------------- ## whether to start tasks marked \"permanent\" automatically on node startup ## ## Default: off ## ## ENV-Key: FAXE_FLOW_AUTO_START ## ## Acceptable values: ## - on or off ## flow_auto_start = off ## -------------------------------------------------------------- ## AUTO RELOAD faxe flows (tasks) ## -------------------------------------------------------------- ## whether to reload all tasks automatically on node startup ## ## Default: off ## ## ENV-Key: FAXE_FLOW_AUTO_RELOAD ## ## Acceptable values: ## - on or off ## flow_auto_reload = off ## -------------------------------------------------------------- ## DFS ## -------------------------------------------------------------- ## path to folder where dfs scripts live ## ## Default: /home/heyoka/workspace/faxe/dfs/ ## ## ENV-Key: FAXE_DFS_SCRIPT_PATH ## ## Acceptable values: ## - the path to a directory ## dfs.script_path = /home/heyoka/workspace/faxe/dfs/ ## ---------------------------------------------------------------- ## API USER - default user, that will be created on first startup ## ---------------------------------------------------------------- ## anonymous access to the api endpoint ## set to false for production use ## ## Default: true ## ## ENV-Key: FAXE_ALLOW_ANONYMOUS ## ## Acceptable values: ## - true or false allow_anonymous = true ## ## Default: user ## ## ENV-Key: FAXE_DEFAULT_USERNAME ## ## Acceptable values: ## - text ## default_username = user ## ## Default: pass ## ## ENV-Key: FAXE_DEFAULT_PASSWORD ## ## Acceptable values: ## - text ## default_password = pass ## ## Default: false ## ## ENV-Key: FAXE_RESET_USER_ON_STARTUP ## ## Acceptable values: ## - true or false ## reset_user_on_startup = false ## ---------------------------------------------------------------- ## API AUTH with JWT ## ---------------------------------------------------------------- ## ## Default: /path/to/cacertfile.pem ## ## ENV-Key: FAXE_HTTP_API_JWT_PUBLIC_KEY_FILE ## ## Acceptable values: ## - the path to a file ## http_api.jwt.public_key_file = /path/to/cacertfile.pem ## ---------------------------------------------------------------- ## REST API ## ---------------------------------------------------------------- ## http port for rest api endpoint ## ## Default: 8081 ## ## ENV-Key: FAXE_HTTP_API_PORT ## ## Acceptable values: ## - an integer http_api_port = 8081 ## ## Default: 3000000 ## ## ENV-Key: FAXE_HTTP_API_MAX_UPLOAD_SIZE ## ## Acceptable values: ## - an integer http_api.max_upload_size = 3000000 ## http-api tls ## enable the use of tls for the http-api ## ## Default: off ## ## ENV-Key: FAXE_HTTP_API_TLS_ENABLE ## ## Acceptable values: ## - on or off ## http_api.tls.enable = on ## http-api ssl certificate ## ## Default: /path/to/certfile.pem ## ## ENV-Key: FAXE_HTTP_API_SSL_CERTFILE ## ## Acceptable values: ## - the path to a file ## http_api.ssl.certfile = /path/to/certfile.pem ## http-api ssl ca certificate ## ## Default: /path/to/cacertfile.pem ## ## ENV-Key: FAXE_HTTP_API_SSL_CACERTFILE ## ## Acceptable values: ## - the path to a file ## http_api.ssl.cacertfile = /path/to/cacertfile.pem ## http-api ssl key file ## ## Default: /path/to/keyfile.key ## ## ENV-Key: FAXE_HTTP_API_SSL_KEYFILE ## ## Acceptable values: ## - the path to a file ## http_api.ssl.keyfile = /path/to/cert.key ## a list of ciphers to use for the http listener ## ## Default: ECDHE-RSA-AES256-GCM-SHA384, ECDHE-RSA-AES128-GCM-SHA256, DHE-RSA-AES256-GCM-SHA384, DHE-RSA-AES128-GCM-SHA256, TLS_AES_256_GCM_SHA384,ECDHE-RSA-AES256-GCM-SHA384 ## ## ENV-Key: FAXE_HTTP_API_CIPHERS ## ## Acceptable values: ## - text ## http_api.ciphers = ECDHE-RSA-AES256-GCM-SHA384, ECDHE-RSA-AES128-GCM-SHA256, DHE-RSA-AES256-GCM-SHA384, DHE-RSA-AES128-GCM-SHA256, TLS_AES_256_GCM_SHA384,ECDHE-RSA-AES256-GCM-SHA384 ## ----------------------------------------------------------- ## PYTHON ## ----------------------------------------------------------------- ## python version ## ## Default: 3 ## ## ENV-Key: FAXE_PYTHON_VERSION ## ## Acceptable values: ## - text ## python.version = 3 ## path to custom python files ## ## Default: /home/heyoka/workspace/faxe/python/ ## ## ENV-Key: FAXE_PYTHON_SCRIPT_PATH ## ## Acceptable values: ## - the path to a directory python.script_path = /home/heyoka/workspace/faxe/python/ ## ------------------------------------------------------------------- ## ESQ ## ------------------------------------------------------------------- ## several faxe nodes use persistent queues for safe data-delivery and buffering in ## case any upstream services are disconnected temporarily ## These queues can be configured with the following settings. ## base directory for persistent queues ## ## Default: /tmp ## ## ENV-Key: FAXE_QUEUE_BASE_DIR ## ## Acceptable values: ## - the path to a directory queue_base_dir = /tmp ## queue message time to live ## expired messages are evicted from queue ## ## Default: 4h ## ## ENV-Key: FAXE_QUEUE_TTL ## ## Acceptable values: ## - a time duration with units, e.g. '10s' for 10 seconds queue_ttl = 4h ## queue sync to disc interval ## queue time-to-sync (rotate) file segments. ## Any enqueued message might remain invisible until sync is performed. ## ## Default: 300ms ## ## ENV-Key: FAXE_QUEUE_TTS ## ## Acceptable values: ## - a time duration with units, e.g. '10s' for 10 seconds queue_tts = 300ms ## queue time to flight ## ttf message time-to-flight in milliseconds, ## the time required to deliver message acknowledgment before it reappears to client(s) again. ## ## Default: 20000ms ## ## ENV-Key: FAXE_QUEUE_TTF ## ## Acceptable values: ## - a time duration with units, e.g. '10s' for 10 seconds queue_ttf = 20000ms ## capacity defines the size of in-memory queue. ## The queue would not fetch anything from disk into memory buffer if capacity is 0. ## ## Default: 30 ## ## ENV-Key: FAXE_QUEUE_CAPACITY ## ## Acceptable values: ## - an integer queue_capacity = 30 ## dequeue interval. ## Start interval at which the queue is asked for an element. ## ## Default: 15ms ## ## ENV-Key: FAXE_DEQUEUE_INTERVAL ## ## Acceptable values: ## - a time duration with units, e.g. '10s' for 10 seconds dequeue_interval = 15ms ## dequeue min interval. ## Min interval at which the queue is asked for an element. ## ## Default: 3ms ## ## ENV-Key: FAXE_DEQUEUE_MIN_INTERVAL ## ## Acceptable values: ## - a time duration with units, e.g. '10s' for 10 seconds dequeue.min_interval = 3ms ## dequeue max interval. ## Max interval at which the queue is asked for an element. ## ## Default: 200ms ## ## ENV-Key: FAXE_DEQUEUE_MAX_INTERVAL ## ## Acceptable values: ## - a time duration with units, e.g. '10s' for 10 seconds dequeue.max_interval = 200ms ## interval change step size. ## Step size for dequeue interval changes ## ## Default: 3 ## ## ENV-Key: FAXE_DEQUEUE_STEP_SIZE ## ## Acceptable values: ## - an integer dequeue.step_size = 3 ## ------------------------------------------------------------------------- ## S7 DEFAULTS ## ------------------------------------------------------------------------- ## for every unique ip address used by s7_read nodes, ## faxe will maintain a separate connection pool, ## each pool will have at least 's7pool.min_size' connections ## and a maximum of 's7pool.max_size' connections ## s7 connection pool min size ## ## Default: 2 ## ## ENV-Key: FAXE_S7POOL_MIN_SIZE ## ## Acceptable values: ## - an integer s7pool.min_size = 2 ## s7 connection pool max size ## ## Default: 16 ## ## ENV-Key: FAXE_S7POOL_MAX_SIZE ## ## Acceptable values: ## - an integer s7pool.max_size = 16 ## whether to use the s7 pool (default for all s7read nodes) ## ## Default: off ## ## ENV-Key: FAXE_S7POOL_ENABLE ## ## Acceptable values: ## - on or off s7pool.enable = off ## whether to use the optimized s7 reader ## ## Default: off ## ## ENV-Key: FAXE_S7READER_OPTIMIZED ## ## Acceptable values: ## - on or off s7reader.optimized = off ## ------------------------------------------------------------------------------- ## MQTT defaults ## ------------------------------------------------------------------------------- ## ## Default: on ## ## ENV-Key: FAXE_MQTT_POOL_ENABLE ## ## Acceptable values: ## - on or off mqtt_pool.enable = on ## max size (maximum number of connections) for the mqtt connection pool ## ## Default: 30 ## ## ENV-Key: FAXE_MQTT_POOL_MAX_SIZE ## ## Acceptable values: ## - an integer mqtt_pool.max_size = 30 ## ## Default: 10.14.204.20 ## ## ENV-Key: FAXE_MQTT_HOST ## ## Acceptable values: ## - text mqtt.host = 10.14.204.20 ## mqtt port ## ## Default: 1883 ## ## ENV-Key: FAXE_MQTT_PORT ## ## Acceptable values: ## - an integer mqtt.port = 1883 ## mqtt user ## ## Default: username ## ## ENV-Key: FAXE_MQTT_USER ## ## Acceptable values: ## - text ## mqtt.user = username ## mqtt pass ## ## Default: password ## ## ENV-Key: FAXE_MQTT_PASS ## ## Acceptable values: ## - text ## mqtt.pass = password ## mqtt ssl ## enable the use of ssl for mqtt connections ## ## Default: off ## ## ENV-Key: FAXE_MQTT_SSL_ENABLE ## ## Acceptable values: ## - on or off ## mqtt.ssl.enable = off ## mqtt ssl peer verification ## ## Default: verify_none ## ## ENV-Key: FAXE_MQTT_SSL_VERIFY ## ## Acceptable values: ## - one of: verify_none, verify_peer ## mqtt.ssl.verify = verify_none ## mqtt ssl certificate ## ## Default: ## ## ENV-Key: FAXE_MQTT_SSL_CERTFILE ## ## Acceptable values: ## - the path to a file ## mqtt.ssl.certfile = /path/to/certfile.pem ## mqtt ssl ca certificate ## ## Default: ## ## ENV-Key: FAXE_MQTT_SSL_CACERTFILE ## ## Acceptable values: ## - the path to a file ## mqtt.ssl.cacertfile = /path/to/cacertfile.pem ## mqtt ssl key file ## ## Default: ## ## ENV-Key: FAXE_MQTT_SSL_KEYFILE ## ## Acceptable values: ## - the path to a file ## mqtt.ssl.keyfile = /path/to/cert.key ## ## Default: 2 ## ## ENV-Key: FAXE_MQTT_PUB_POOL_MIN_SIZE ## ## Acceptable values: ## - an integer mqtt_pub_pool.min_size = 2 ## mqtt publisher connection pool max size ## ## Default: 30 ## ## ENV-Key: FAXE_MQTT_PUB_POOL_MAX_SIZE ## ## Acceptable values: ## - an integer mqtt_pub_pool.max_size = 30 ## mqtt publisher connection pool max worker rate (message throughput per second) ## this is used for the elastic pool grow/shrink mechanism ## ## Default: 70 ## ## ENV-Key: FAXE_MQTT_PUB_POOL_WORKER_MAX_RATE ## ## Acceptable values: ## - an integer mqtt_pub_pool.worker_max_rate = 70 ## whether to use the s7 pool (default for all s7read nodes) ## ## Default: on ## ## ENV-Key: FAXE_MQTT_PUB_POOL_ENABLE ## ## Acceptable values: ## - on or off mqtt_pub_pool.enable = on ## ------------------------------------------------------------------------------- ## AMQP defaults ## ------------------------------------------------------------------------------- ## amqp host ## ## Default: 10.14.204.28 ## ## ENV-Key: FAXE_AMQP_HOST ## ## Acceptable values: ## - text amqp.host = 10.14.204.28 ## amqp port ## ## Default: 5672 ## ## ENV-Key: FAXE_AMQP_PORT ## ## Acceptable values: ## - an integer amqp.port = 5672 ## amqp user ## ## Default: guest ## ## ENV-Key: FAXE_AMQP_USER ## ## Acceptable values: ## - text ## amqp.user = username ## amqp pass ## ## Default: guest ## ## ENV-Key: FAXE_AMQP_PASS ## ## Acceptable values: ## - text ## amqp.pass = password ## amqp heartbeat interval ## ## Default: 60s ## ## ENV-Key: FAXE_AMQP_HEARTBEAT ## ## Acceptable values: ## - a time duration with units, e.g. '10s' for 10 seconds ## amqp.heartbeat = 60s ## amqp ssl ## enable the use of ssl for amqp connections ## ## Default: off ## ## ENV-Key: FAXE_AMQP_SSL_ENABLE ## ## Acceptable values: ## - on or off ## amqp.ssl.enable = off ## amqp ssl certificate ## ## Default: ## ## ENV-Key: FAXE_AMQP_SSL_CERTFILE ## ## Acceptable values: ## - the path to a file ## amqp.ssl.certfile = /path/to/certfile.pem ## amqp ssl ca certificate ## ## Default: ## ## ENV-Key: FAXE_AMQP_SSL_CACERTFILE ## ## Acceptable values: ## - the path to a file ## amqp.ssl.cacertfile = /path/to/cacertfile.pem ## amqp ssl key file ## ## Default: ## ## ENV-Key: FAXE_AMQP_SSL_KEYFILE ## ## Acceptable values: ## - the path to a file ## amqp.ssl.keyfile = /path/to/cert.key ## amqp ssl peer verification ## ## Default: verify_none ## ## ENV-Key: FAXE_AMQP_SSL_VERIFY ## ## Acceptable values: ## - one of: verify_none, verify_peer ## amqp.ssl.verify = verify_none ## ------------------------------------------------------------------------------- ## RabbitMQ defaults ## ------------------------------------------------------------------------------- ## rabbitmq default exchange ## the amqp_publish node will use this exchange as default ## ## Default: x_lm_fanout ## ## ENV-Key: FAXE_RABBITMQ_ROOT_EXCHANGE ## ## Acceptable values: ## - text rabbitmq.root_exchange = x_lm_fanout ## ## Default: q_ ## ## ENV-Key: FAXE_RABBITMQ_QUEUE_PREFIX ## ## Acceptable values: ## - text rabbitmq.queue_prefix = q_ ## ## Default: x_ ## ## ENV-Key: FAXE_RABBITMQ_EXCHANGE_PREFIX ## ## Acceptable values: ## - text rabbitmq.exchange_prefix = x_ ## ------------------------------------------------------------------------------- ## CrateDB defaults (postgreSQL connect) ## ------------------------------------------------------------------------------- ## CrateDB host ## ## Default: crate.example.com ## ## ENV-Key: FAXE_CRATE_HOST ## ## Acceptable values: ## - text crate.host = crate.example.com ## CrateDB port ## ## Default: 5432 ## ## ENV-Key: FAXE_CRATE_PORT ## ## Acceptable values: ## - an integer crate.port = 5432 ## CrateDB user ## ## Default: crate ## ## ENV-Key: FAXE_CRATE_USER ## ## Acceptable values: ## - text crate.user = crate ## CrateDB password ## ## Default: ## ## ENV-Key: FAXE_CRATE_PASS ## ## Acceptable values: ## - text ## crate.pass = pass ## CrateDB database ## ## Default: doc ## ## ENV-Key: FAXE_CRATE_DATABASE ## ## Acceptable values: ## - text crate.database = doc ## crate tls ## enable the use of tls for crate postgre connections ## ## Default: off ## ## ENV-Key: FAXE_CRATE_TLS_ENABLE ## ## Acceptable values: ## - on or off ## crate.tls.enable = off ## ------------------------------------------------------------------------------- ## CrateDB defaults (http api) ## ------------------------------------------------------------------------------- ## CrateDB host ## ## Default: 10.14.204.10 ## ## ENV-Key: FAXE_CRATE_HTTP_HOST ## ## Acceptable values: ## - text crate_http.host = 10.14.204.10 ## CrateDB port ## ## Default: 4200 ## ## ENV-Key: FAXE_CRATE_HTTP_PORT ## ## Acceptable values: ## - an integer crate_http.port = 4200 ## CrateDB user ## ## Default: crate ## ## ENV-Key: FAXE_CRATE_HTTP_USER ## ## Acceptable values: ## - text crate_http.user = crate ## CrateDB password ## ## Default: ## ## ENV-Key: FAXE_CRATE_HTTP_PASS ## ## Acceptable values: ## - text ## crate_http.pass = pass ## CrateDB database ## ## Default: doc ## ## ENV-Key: FAXE_CRATE_HTTP_DATABASE ## ## Acceptable values: ## - text crate_http.database = doc ## crate tls ## enable the use of tls for crate http connections ## ## Default: off ## ## ENV-Key: FAXE_CRATE_HTTP_TLS_ENABLE ## ## Acceptable values: ## - on or off ## crate_http.tls.enable = off ## ------------------------------------------------------------------------------- ## InfluxDB defaults (http api) ## ------------------------------------------------------------------------------- ## InfluxDB host ## ## Default: influx.example.com ## ## ENV-Key: FAXE_INFLUX_HTTP_HOST ## ## Acceptable values: ## - text influx_http.host = influx.example.com ## InfluxDB port ## ## Default: 8086 ## ## ENV-Key: FAXE_INFLUX_HTTP_PORT ## ## Acceptable values: ## - an integer influx_http.port = 8086 ## InfluxDB user ## ## Default: influx ## ## ENV-Key: FAXE_INFLUX_HTTP_USER ## ## Acceptable values: ## - text influx_http.user = influx ## InfluxDB pass ## ## Default: ## ## ENV-Key: FAXE_INFLUX_HTTP_PASS ## ## Acceptable values: ## - text ## influx_http.pass = password ## ---------------------------------------------------------------------------- ## EMAIL defaults ## ---------------------------------------------------------------------------- ## email from address ## ## Default: noreply@example.com ## ## ENV-Key: FAXE_EMAIL_FROM ## ## Acceptable values: ## - text email.from = noreply@example.com ## email smtp relay ## ## Default: smtp.example.com ## ## ENV-Key: FAXE_EMAIL_SMTP ## ## Acceptable values: ## - text email.smtp = smtp.example.com ## email smtp port ## ## Default: 25 ## ## ENV-Key: FAXE_EMAIL_PORT ## ## Acceptable values: ## - an integer email.port = 25 ## email smtp tls, whether to use tls ## ## Default: off ## ## ENV-Key: FAXE_EMAIL_TLS ## ## Acceptable values: ## - on or off email.tls = off ## email smtp user ## ## Default: username ## ## ENV-Key: FAXE_EMAIL_USER ## ## Acceptable values: ## - text ## email.user = username ## email smtp pass ## ## Default: password ## ## ENV-Key: FAXE_EMAIL_PASS ## ## Acceptable values: ## - text ## email.pass = password ## email html template ## ## Default: /home/user/template.html ## ## ENV-Key: FAXE_EMAIL_TEMPLATE ## ## Acceptable values: ## - text email.template = /home/user/template.html ## -------------------------------------------------------------------------- ## AZURE BLOB ## ------------------------------------------------------------------------------- ## account-url ## ## Default: https://someblob.blob.core.windows.net ## ## ENV-Key: FAXE_AZURE_BLOB_ACCOUNT_URL ## ## Acceptable values: ## - text azure_blob.account_url = https://someblob.blob.core.windows.net ## ## Default: azblob-secret ## ## ENV-Key: FAXE_AZURE_BLOB_ACCOUNT_SECRET ## ## Acceptable values: ## - text azure_blob.account_secret = azblob-secret ## ## -------------------------------------------------------------------- ## DEBUG, LOGS, METRICS, CONNECTION STATUS, FLOW_CHANGED ## -------------------------------------------------------------------- ## There are mqtt handlers for debug, logs, metrics, connection-status and flow_changed events. ## Note that the base options for these mqtt connections come from the 'mqtt' options above. ## If needed you can override these default mqtt-options for every handler type. ## ## Default: ## ## ENV-Key: FAXE_REPORT_DEBUG_MQTT_HOST ## ## Acceptable values: ## - text ## report_debug.mqtt_host = example.com ## ## ----------------------- METRICS ------------------------------ ## Metrics handler MQTT sends metric events to an mqtt broker ## ## Default: off ## ## ENV-Key: FAXE_METRICS_HANDLER_MQTT_ENABLE ## ## Acceptable values: ## - on or off metrics.handler.mqtt.enable = off ## metrics handler mqtt host ## ## Default: ## ## ENV-Key: FAXE_METRICS_HANDLER_MQTT_HOST ## ## Acceptable values: ## - text ## metrics.handler.mqtt.host = example.com ## metrics handler mqtt port ## ## ENV-Key: FAXE_METRICS_HANDLER_MQTT_PORT ## ## Acceptable values: ## - an integer ## metrics.handler.mqtt.port = 1883 ## metrics handler mqtt base topic ## The mqtt handler will prefix its topic with this value, ## note that it must be a valid mqtt topic string. ## ## Default: sys/faxe ## ## ENV-Key: FAXE_METRICS_HANDLER_MQTT_BASE_TOPIC ## ## Acceptable values: ## - text ## metrics.handler.mqtt.base_topic = sys/faxe ## flow-metrics publish interval ## Interval at which flow-metrics get publish to the handler ## ## Default: 30s ## ## ENV-Key: FAXE_METRICS_PUBLISH_INTERVAL ## ## Acceptable values: ## - text metrics.publish_interval = 30s ## ----------------------- CONNECTION STATUS ------------------------ ## Conn_status handler MQTT sends connection status events to an mqtt broker. ## ## Default: on ## ## ENV-Key: FAXE_CONN_STATUS_HANDLER_MQTT_ENABLE ## ## Acceptable values: ## - on or off conn_status.handler.mqtt.enable = on ## ## Default: ## ## ENV-Key: FAXE_CONN_STATUS_HANDLER_MQTT_HOST ## ## Acceptable values: ## - text ## conn_status.handler.mqtt.host = example.com ## connection status handler mqtt port ## ## ENV-Key: FAXE_CONN_STATUS_HANDLER_MQTT_PORT ## ## Acceptable values: ## - an integer ## conn_status.handler.mqtt.port = 1883 ## connection status handler mqtt base topic ## ## Default: sys/faxe ## ## ENV-Key: FAXE_CONN_STATUS_HANDLER_MQTT_BASE_TOPIC ## ## Acceptable values: ## - text ## conn_status.handler.mqtt.base_topic = sys/faxe ## ----------------------- DEBUG AND TRACE -------------------------- ## Debug trace handler MQTT ## enable/disable debug_trace handler mqtt ## ## Default: off ## ## ENV-Key: FAXE_DEBUG_HANDLER_MQTT_ENABLE ## ## Acceptable values: ## - on or off debug.handler.mqtt.enable = off ## debug_trace handler mqtt host ## ## Default: ## ## ENV-Key: FAXE_DEBUG_HANDLER_MQTT_HOST ## ## Acceptable values: ## - text ## debug.handler.mqtt.host = example.com ## debug_trace handler mqtt port ## ## ENV-Key: FAXE_DEBUG_HANDLER_MQTT_PORT ## ## Acceptable values: ## - an integer ## debug.handler.mqtt.port = 1883 ## debug_trace handler mqtt base topic ## ## Default: sys/faxe ## ## ENV-Key: FAXE_DEBUG_HANDLER_MQTT_BASE_TOPIC ## ## Acceptable values: ## - text ## debug.handler.mqtt.base_topic = sys/faxe ## time debug and node-metric messages will be published to the configured endpoints ## ## Default: 3m ## ## ENV-Key: FAXE_DEBUG_TIME ## ## Acceptable values: ## - a time duration with units, e.g. '10s' for 10 seconds debug.time = 3m ## ----------------------- FLOW_CHANGED -------------------------- ## flow_changed handler MQTT ## enable/disable flow_changed handler mqtt ## ## Default: off ## ## ENV-Key: FAXE_FLOW_CHANGED_HANDLER_MQTT_ENABLE ## ## Acceptable values: ## - on or off flow_changed.handler.mqtt.enable = off ## flow_changed handler mqtt host ## ## Default: ## ## ENV-Key: FAXE_FLOW_CHANGED_HANDLER_MQTT_HOST ## ## Acceptable values: ## - text ## flow_changed.handler.mqtt.host = example.com ## flow_changed handler mqtt port ## ## ENV-Key: FAXE_FLOW_CHANGED_HANDLER_MQTT_PORT ## ## Acceptable values: ## - an integer ## flow_changed.handler.mqtt.port = 1883 ## flow_changed handler mqtt base topic ## ## Default: sys/faxe ## ## ENV-Key: FAXE_FLOW_CHANGED_HANDLER_MQTT_BASE_TOPIC ## ## Acceptable values: ## - text ## flow_changed.handler.mqtt.base_topic = sys/faxe","title":"Configuration"},{"location":"custom_nodes.html","text":"Custom nodes written in python Faxe allows for custom nodes to be used in any flow just like any of the built-in nodes. Therefore, a dedicated interface can be used, which will be described here. Custom nodes are written in python >= 3.6 in FAXE. The syntax for calling a custom node in DFS is exactly the same as for any built-in node, but instead of a pipe symbol, we use the @ symbol. | built_in_node () . opt1 ( 'somestring' ) . opt2 ( 11 ) @ custom_node () . option1 ( 'astring' ) . option2 ( 22 ) Faxe base class When writing a custom node in python, we have to create a python class, that inherits from the base class Faxe . from faxe import Faxe class Mynode (Faxe): ... In our class we can use a bunch of callbacks: Callbacks All callbacks are optional. options (static) The options callback is used to tell FAXE, what node options you want to use for your python node. The return type for this callback is a list of 2-or-3 tuples . options is the only static callback and the only one, that has to return a value. The first two elements of the tuples must be strings, the third, if given, depends on the data_type . Every option, that has no default value (3rd element in the tuple) is mandatory in DFS scripts. ( \"name_of_the_option\" , \"data_type\" , {optional_default_value}) Example from faxe import Faxe class Mynode (Faxe): @staticmethod def options (): opts = [ # mandatory ( \"field\" , \"string\" ), # optional ( \"val\" , \"integer\" , 33 ), ] return opts ... The above example node can be used in DFS like so: @ mynode () % mandatory . field ( 'some_field_path' ) % optional . val ( 44 ) Data types for options The second element of an options tuple defines the data type, that must be given in DFS. A subset of the option types used for built-in nodes can be used. type description DFS example string .option('string_value') integer .option(123) float .option(123.1564) number integer or float .option(456.1564) double same as float .option(13.98741755) bool .option(false) string_list 1 or more string values .option('string1', 'string2', 'string3') integer_list 1 or more integer values .option(1, 2, 3, 4445) float_list 1 or more float values .option(1.11, 2.456486, 3.0, 44.45) number_list 1 or more number values .option(1.11, 2, 3.0, 45) list list of possibly mixed data types .option('name', 11, 234.3, 'one', 'two') init The init callback is called on class instatiation, it gets injected a dictionary with the option values given in the DFS script. Do not overwrite the __init__ method. The callback will not work in this case. from faxe import Faxe class Mynode (Faxe): ... def init (self, args = None ): # store the option values for later usage self . fieldname = args[ \"field\" ] self . value = args[ \"val\" ] ... handle_point handle_point is called every time the custom node receives a data-point structure from upstream nodes in a FAXE flow. For details on the point structure see FAXE Data items - data_point below. from faxe import Faxe class Mynode (Faxe): ... def handle_point (self, point_data): # use the inherited emit method to emit data to downstream nodes in the flow self . emit(point_data) ... handle_batch handle_batch is called every time the custom node receives a data-batch structure from upstream nodes in a FAXE flow. For details on the batch structure see FAXE Data items - data_batch below. from faxe import Faxe class Mynode (Faxe): ... def handle_batch (self, batch_data): # use the inherited emit method to emit data to downstream nodes in the flow self . emit(batch_data) ... Inherited methods from the Faxe class emit def emit (self, emit_data: dict): The emit method inherited from the base class (Faxe), is the only way send data to downstream nodes in a FAXE flow. It can take both point and batch data structures. from faxe import Faxe class Mynode (Faxe): ... def my_method (self): batch_data = self . my_batch_data self . emit(batch_data) ... log def log (self, msg: str, level = 'notice' : str): \"\"\" :param level: 'debug' | 'info' | 'notice' | 'warning' | 'error' | 'critical' | 'alert' \"\"\" The log method inherited from the base class (Faxe), can be used for logging. Using this method makes sure, your log data will be injected into FAXE's logging infrastructure. from faxe import Faxe class Mynode (Faxe): ... def my_method (self, param): self . log( f\"my_method is called with { param }\" , 'info' ) ... ... now (static) @staticmethod def now (): \"\"\" unix timestamp in milliseconds (utc) :return: int \"\"\" Used to retrieve the current timestamp in milliseconds. from faxe import Faxe class Mynode (Faxe): ... def my_method (self, _param): now = Faxe . now() ... ... Data types Comparing data types on each side FAXE python string string binary string integer integer floating point number floating point number map dictionary list list ---------------- ------------------ data-point dictionary, see below data-batch dictionary, see below FAXE Data items As you remember, in FAXE we know two types of data-items, data-point and data-batch. How do data-items look like in python ? data-point ## data-point { 'fields' : { 'f1' : 1 , 'f2' : { 'f2_1' : 'mode_on' }}, 'ts' : 1669407781437 , 'dtag' : None , 'tags' : {}} field data type meaning ts integer millisecond timestamp fields dictionary a dictionary of fields, a data-point carries, tags dictionary a dictionary of tags, a data-point carries dtag integer or None delivery tag, see description below fields A key in the fields dict is called fieldname and is always a string . A dict value can be of any of the above listed data-types, including dictionary and list. Examples { 'field1' : 'string' } { 'field2' : 1235468486 } { 'field3' : 12354.68486 } { 'field4' : { 'field4_1' : [ 1 , 43 , 4 , 67.7 ]}} tags For tags , keys and values are strings only. dtag A delivery tag is used to acknowledge a data-item to upstream nodes. Can be ignored for python nodes at the moment. data-batch ## data-batch { 'points' : [ { 'fields' : { 'f1' : 1 , 'f2' : { 'f2_1' : 'mode_on' }}, 'ts' : 1669407781437 , 'dtag' : None , 'tags' : {}}, { 'fields' : { 'f1' : 2 , 'f2' : { 'f2_1' : 'mode_off' }}, 'ts' : 1669407781438 , 'dtag' : None , 'tags' : {}}, { 'fields' : { 'f1' : 3 , 'f2' : { 'f2_1' : 'mode_on' }}, 'ts' : 1669407781439 , 'dtag' : None , 'tags' : {}}, ], 'start_ts' : 1669407781437 , 'dtag' : None } field data type meaning start_ts integer or None millisecond timestamp, not always set points list a list of data-points, sorted by their timestamps ascending dtag integer or None delivery tag, see description below points In a data-batch, points is a list of data-points , the list can be of any length (Note :there is no length check in place at the moment). dtag See data-point for a description. Helper classes There are helper classes to make it easier to work with data coming from the faxe engine. All functions are static so you do not instanciate them and nothing is stored inside an object, making it possible to mix the usage of the helpers with inline code. To use the helper classes, just import faxe.Point and/or faxe.Batch . from faxe import Faxe, Point, Batch class Mynode (Faxe): ... Helper class for working with data-point objects. class Point : \"\"\" Completely static helper class for data-point structures (dicts) point = dict() point['ts'] = int millisecond timestamp point['fields'] = dict() point['tags'] = dict() point['dtag'] = int \"\"\" @staticmethod def new (ts = None ): p = dict() p[ 'fields' ] = dict() p[ 'tags' ] = dict() p[ 'dtag' ] = None p[ 'ts' ] = ts return p @staticmethod def fields (point_data, newfields = None ): \"\"\" get or set all the fields (dict) :param point_data: dict() :param newfields: dict() :return: dict() \"\"\" if newfields is not None : point_data[ 'fields' ] = newfields return point_data return dict(point_data[ 'fields' ]) @staticmethod def value (point_data, path, value = None ): \"\"\" get or set a specific field :param point_data: dict() :param path: string :param value: any :return: None, if field is not found / \"\"\" if value is not None : Jsn . set(point_data, path, value) return point_data return Jsn . get(point_data, path) @staticmethod def values (point_data, paths, value = None ): \"\"\" get or set a specific field :param point_data: dict :param paths: list :param value: any :return: point_data|list \"\"\" if value is not None : for path in paths: Jsn . set(point_data, path, value) return point_data out = list() for path in paths: out . append(Jsn . get(point_data, path)) return out @staticmethod def default (point_data, path, value): \"\"\" :param point_data: :param path: :param value: :return: \"\"\" if Point . value(point_data, path) is None : Point . value(point_data, path, value) return point_data @staticmethod def tags (point_data, newtags = None ): if newtags is not None : point_data[ 'tags' ] = newtags return point_data return point_data[ 'tags' ] @staticmethod def ts (point_data, newts = None ): \"\"\" get or set the timestamp of this point :param point_data: dict :param newts: integer :return: integer|dict \"\"\" if newts is not None : point_data[ 'ts' ] = int(newts) return point_data return point_data[ 'ts' ] @staticmethod def dtag (point_data, newdtag = None ): if newdtag is not None : point_data[ 'dtag' ] = newdtag return point_data return point_data[ 'dtag' ] Helper class for working with data-batch objects. class Batch : \"\"\" Completely static helper class for data-batch structures (dicts) batch = dict() batch['points'] = list() of point dicts sorted by their timestamps batch['start_ts'] = int millisecond unix timestamp denoting the start of this batch batch['dtag'] = int \"\"\" @staticmethod def new (start_ts = None ): b = dict() b[ 'points' ] = list() b[ 'dtag' ] = None b[ 'start_ts' ] = start_ts return b @staticmethod def empty (batch_data): \"\"\" a Batch is empty, if it has no points :param batch_data: :return: True | False \"\"\" return ( 'points' not in batch_data) or (batch_data[ 'points' ] == []) @staticmethod def points (batch_data, points = None ): \"\"\" :param points: None | list() :param batch_data: dict :return: list \"\"\" if points is not None : batch_data[ 'points' ] = points Batch . sort_points(batch_data) return batch_data return list(batch_data[ 'points' ]) @staticmethod def value (batch_data, path, value = None ): \"\"\" get or set path from/to every point in a batch :param batch_data: :param path: :param value: :return: list \"\"\" out = list() points = batch_data[ 'points' ] for p in points: out . append(Point . value(p, path, value)) if value is not None : return batch_data return out @staticmethod def values (batch_data, paths, value = None ): \"\"\" get or set path from/to every point in a batch :param batch_data: :param paths: :param value: :return: list \"\"\" if not isinstance(paths, list): raise TypeError ( 'Batch.values() - paths must be a list of strings' ) if value is not None : points = batch_data[ 'points' ] for p in points: for path in paths: Point . value(p, path, value) # batch_data['points'] = points return batch_data else : out = list() points = batch_data[ 'points' ] for p in points: odict = dict() for path in paths: odict[path] = Point . value(p, path, value) out . append(odict) return out @staticmethod def default (batch_data, path, value): \"\"\" :param batch_data: :param path: :param value: :return: \"\"\" points = batch_data[ 'points' ] for p in points: Point . default(p, path, value) return batch_data @staticmethod def dtag (batch_data): return batch_data[ 'dtag' ] @staticmethod def start_ts (batch_data, newts = None ): if newts is not None : batch_data[ 'start_ts' ] = newts return batch_data return batch_data[ 'start_ts' ] @staticmethod def add (batch_data, point): if ( 'points' not in batch_data) or (type(batch_data[ 'points' ]) != list): batch_data[ 'points' ] = list() batch_data[ 'points' ] . append(point) else : batch_data[ 'points' ] . append(point) Batch . sort_points(batch_data) return batch_data","title":"Custom nodes written in python"},{"location":"custom_nodes.html#custom-nodes-written-in-python","text":"Faxe allows for custom nodes to be used in any flow just like any of the built-in nodes. Therefore, a dedicated interface can be used, which will be described here. Custom nodes are written in python >= 3.6 in FAXE. The syntax for calling a custom node in DFS is exactly the same as for any built-in node, but instead of a pipe symbol, we use the @ symbol. | built_in_node () . opt1 ( 'somestring' ) . opt2 ( 11 ) @ custom_node () . option1 ( 'astring' ) . option2 ( 22 )","title":"Custom nodes written in python"},{"location":"custom_nodes.html#faxe-base-class","text":"When writing a custom node in python, we have to create a python class, that inherits from the base class Faxe . from faxe import Faxe class Mynode (Faxe): ... In our class we can use a bunch of callbacks:","title":"Faxe base class"},{"location":"custom_nodes.html#callbacks","text":"All callbacks are optional.","title":"Callbacks"},{"location":"custom_nodes.html#options-static","text":"The options callback is used to tell FAXE, what node options you want to use for your python node. The return type for this callback is a list of 2-or-3 tuples . options is the only static callback and the only one, that has to return a value. The first two elements of the tuples must be strings, the third, if given, depends on the data_type . Every option, that has no default value (3rd element in the tuple) is mandatory in DFS scripts. ( \"name_of_the_option\" , \"data_type\" , {optional_default_value})","title":"options (static)"},{"location":"custom_nodes.html#example","text":"from faxe import Faxe class Mynode (Faxe): @staticmethod def options (): opts = [ # mandatory ( \"field\" , \"string\" ), # optional ( \"val\" , \"integer\" , 33 ), ] return opts ... The above example node can be used in DFS like so: @ mynode () % mandatory . field ( 'some_field_path' ) % optional . val ( 44 )","title":"Example"},{"location":"custom_nodes.html#data-types-for-options","text":"The second element of an options tuple defines the data type, that must be given in DFS. A subset of the option types used for built-in nodes can be used. type description DFS example string .option('string_value') integer .option(123) float .option(123.1564) number integer or float .option(456.1564) double same as float .option(13.98741755) bool .option(false) string_list 1 or more string values .option('string1', 'string2', 'string3') integer_list 1 or more integer values .option(1, 2, 3, 4445) float_list 1 or more float values .option(1.11, 2.456486, 3.0, 44.45) number_list 1 or more number values .option(1.11, 2, 3.0, 45) list list of possibly mixed data types .option('name', 11, 234.3, 'one', 'two')","title":"Data types for options"},{"location":"custom_nodes.html#init","text":"The init callback is called on class instatiation, it gets injected a dictionary with the option values given in the DFS script. Do not overwrite the __init__ method. The callback will not work in this case. from faxe import Faxe class Mynode (Faxe): ... def init (self, args = None ): # store the option values for later usage self . fieldname = args[ \"field\" ] self . value = args[ \"val\" ] ...","title":"init"},{"location":"custom_nodes.html#handle_point","text":"handle_point is called every time the custom node receives a data-point structure from upstream nodes in a FAXE flow. For details on the point structure see FAXE Data items - data_point below. from faxe import Faxe class Mynode (Faxe): ... def handle_point (self, point_data): # use the inherited emit method to emit data to downstream nodes in the flow self . emit(point_data) ...","title":"handle_point"},{"location":"custom_nodes.html#handle_batch","text":"handle_batch is called every time the custom node receives a data-batch structure from upstream nodes in a FAXE flow. For details on the batch structure see FAXE Data items - data_batch below. from faxe import Faxe class Mynode (Faxe): ... def handle_batch (self, batch_data): # use the inherited emit method to emit data to downstream nodes in the flow self . emit(batch_data) ...","title":"handle_batch"},{"location":"custom_nodes.html#inherited-methods-from-the-faxe-class","text":"","title":"Inherited methods from the Faxe class"},{"location":"custom_nodes.html#emit","text":"def emit (self, emit_data: dict): The emit method inherited from the base class (Faxe), is the only way send data to downstream nodes in a FAXE flow. It can take both point and batch data structures. from faxe import Faxe class Mynode (Faxe): ... def my_method (self): batch_data = self . my_batch_data self . emit(batch_data) ...","title":"emit"},{"location":"custom_nodes.html#log","text":"def log (self, msg: str, level = 'notice' : str): \"\"\" :param level: 'debug' | 'info' | 'notice' | 'warning' | 'error' | 'critical' | 'alert' \"\"\" The log method inherited from the base class (Faxe), can be used for logging. Using this method makes sure, your log data will be injected into FAXE's logging infrastructure. from faxe import Faxe class Mynode (Faxe): ... def my_method (self, param): self . log( f\"my_method is called with { param }\" , 'info' ) ... ...","title":"log"},{"location":"custom_nodes.html#now-static","text":"@staticmethod def now (): \"\"\" unix timestamp in milliseconds (utc) :return: int \"\"\" Used to retrieve the current timestamp in milliseconds. from faxe import Faxe class Mynode (Faxe): ... def my_method (self, _param): now = Faxe . now() ... ...","title":"now (static)"},{"location":"custom_nodes.html#data-types","text":"Comparing data types on each side FAXE python string string binary string integer integer floating point number floating point number map dictionary list list ---------------- ------------------ data-point dictionary, see below data-batch dictionary, see below","title":"Data types"},{"location":"custom_nodes.html#faxe-data-items","text":"As you remember, in FAXE we know two types of data-items, data-point and data-batch. How do data-items look like in python ?","title":"FAXE Data items"},{"location":"custom_nodes.html#data-point","text":"## data-point { 'fields' : { 'f1' : 1 , 'f2' : { 'f2_1' : 'mode_on' }}, 'ts' : 1669407781437 , 'dtag' : None , 'tags' : {}} field data type meaning ts integer millisecond timestamp fields dictionary a dictionary of fields, a data-point carries, tags dictionary a dictionary of tags, a data-point carries dtag integer or None delivery tag, see description below","title":"data-point"},{"location":"custom_nodes.html#fields","text":"A key in the fields dict is called fieldname and is always a string . A dict value can be of any of the above listed data-types, including dictionary and list. Examples { 'field1' : 'string' } { 'field2' : 1235468486 } { 'field3' : 12354.68486 } { 'field4' : { 'field4_1' : [ 1 , 43 , 4 , 67.7 ]}}","title":"fields"},{"location":"custom_nodes.html#tags","text":"For tags , keys and values are strings only.","title":"tags"},{"location":"custom_nodes.html#dtag","text":"A delivery tag is used to acknowledge a data-item to upstream nodes. Can be ignored for python nodes at the moment.","title":"dtag"},{"location":"custom_nodes.html#data-batch","text":"## data-batch { 'points' : [ { 'fields' : { 'f1' : 1 , 'f2' : { 'f2_1' : 'mode_on' }}, 'ts' : 1669407781437 , 'dtag' : None , 'tags' : {}}, { 'fields' : { 'f1' : 2 , 'f2' : { 'f2_1' : 'mode_off' }}, 'ts' : 1669407781438 , 'dtag' : None , 'tags' : {}}, { 'fields' : { 'f1' : 3 , 'f2' : { 'f2_1' : 'mode_on' }}, 'ts' : 1669407781439 , 'dtag' : None , 'tags' : {}}, ], 'start_ts' : 1669407781437 , 'dtag' : None } field data type meaning start_ts integer or None millisecond timestamp, not always set points list a list of data-points, sorted by their timestamps ascending dtag integer or None delivery tag, see description below","title":"data-batch"},{"location":"custom_nodes.html#points","text":"In a data-batch, points is a list of data-points , the list can be of any length (Note :there is no length check in place at the moment).","title":"points"},{"location":"custom_nodes.html#dtag_1","text":"See data-point for a description.","title":"dtag"},{"location":"custom_nodes.html#helper-classes","text":"There are helper classes to make it easier to work with data coming from the faxe engine. All functions are static so you do not instanciate them and nothing is stored inside an object, making it possible to mix the usage of the helpers with inline code. To use the helper classes, just import faxe.Point and/or faxe.Batch . from faxe import Faxe, Point, Batch class Mynode (Faxe): ...","title":"Helper classes"},{"location":"custom_nodes.html#helper-class-for-working-with-data-point-objects","text":"class Point : \"\"\" Completely static helper class for data-point structures (dicts) point = dict() point['ts'] = int millisecond timestamp point['fields'] = dict() point['tags'] = dict() point['dtag'] = int \"\"\" @staticmethod def new (ts = None ): p = dict() p[ 'fields' ] = dict() p[ 'tags' ] = dict() p[ 'dtag' ] = None p[ 'ts' ] = ts return p @staticmethod def fields (point_data, newfields = None ): \"\"\" get or set all the fields (dict) :param point_data: dict() :param newfields: dict() :return: dict() \"\"\" if newfields is not None : point_data[ 'fields' ] = newfields return point_data return dict(point_data[ 'fields' ]) @staticmethod def value (point_data, path, value = None ): \"\"\" get or set a specific field :param point_data: dict() :param path: string :param value: any :return: None, if field is not found / \"\"\" if value is not None : Jsn . set(point_data, path, value) return point_data return Jsn . get(point_data, path) @staticmethod def values (point_data, paths, value = None ): \"\"\" get or set a specific field :param point_data: dict :param paths: list :param value: any :return: point_data|list \"\"\" if value is not None : for path in paths: Jsn . set(point_data, path, value) return point_data out = list() for path in paths: out . append(Jsn . get(point_data, path)) return out @staticmethod def default (point_data, path, value): \"\"\" :param point_data: :param path: :param value: :return: \"\"\" if Point . value(point_data, path) is None : Point . value(point_data, path, value) return point_data @staticmethod def tags (point_data, newtags = None ): if newtags is not None : point_data[ 'tags' ] = newtags return point_data return point_data[ 'tags' ] @staticmethod def ts (point_data, newts = None ): \"\"\" get or set the timestamp of this point :param point_data: dict :param newts: integer :return: integer|dict \"\"\" if newts is not None : point_data[ 'ts' ] = int(newts) return point_data return point_data[ 'ts' ] @staticmethod def dtag (point_data, newdtag = None ): if newdtag is not None : point_data[ 'dtag' ] = newdtag return point_data return point_data[ 'dtag' ]","title":"Helper class for working with data-point objects."},{"location":"custom_nodes.html#helper-class-for-working-with-data-batch-objects","text":"class Batch : \"\"\" Completely static helper class for data-batch structures (dicts) batch = dict() batch['points'] = list() of point dicts sorted by their timestamps batch['start_ts'] = int millisecond unix timestamp denoting the start of this batch batch['dtag'] = int \"\"\" @staticmethod def new (start_ts = None ): b = dict() b[ 'points' ] = list() b[ 'dtag' ] = None b[ 'start_ts' ] = start_ts return b @staticmethod def empty (batch_data): \"\"\" a Batch is empty, if it has no points :param batch_data: :return: True | False \"\"\" return ( 'points' not in batch_data) or (batch_data[ 'points' ] == []) @staticmethod def points (batch_data, points = None ): \"\"\" :param points: None | list() :param batch_data: dict :return: list \"\"\" if points is not None : batch_data[ 'points' ] = points Batch . sort_points(batch_data) return batch_data return list(batch_data[ 'points' ]) @staticmethod def value (batch_data, path, value = None ): \"\"\" get or set path from/to every point in a batch :param batch_data: :param path: :param value: :return: list \"\"\" out = list() points = batch_data[ 'points' ] for p in points: out . append(Point . value(p, path, value)) if value is not None : return batch_data return out @staticmethod def values (batch_data, paths, value = None ): \"\"\" get or set path from/to every point in a batch :param batch_data: :param paths: :param value: :return: list \"\"\" if not isinstance(paths, list): raise TypeError ( 'Batch.values() - paths must be a list of strings' ) if value is not None : points = batch_data[ 'points' ] for p in points: for path in paths: Point . value(p, path, value) # batch_data['points'] = points return batch_data else : out = list() points = batch_data[ 'points' ] for p in points: odict = dict() for path in paths: odict[path] = Point . value(p, path, value) out . append(odict) return out @staticmethod def default (batch_data, path, value): \"\"\" :param batch_data: :param path: :param value: :return: \"\"\" points = batch_data[ 'points' ] for p in points: Point . default(p, path, value) return batch_data @staticmethod def dtag (batch_data): return batch_data[ 'dtag' ] @staticmethod def start_ts (batch_data, newts = None ): if newts is not None : batch_data[ 'start_ts' ] = newts return batch_data return batch_data[ 'start_ts' ] @staticmethod def add (batch_data, point): if ( 'points' not in batch_data) or (type(batch_data[ 'points' ]) != list): batch_data[ 'points' ] = list() batch_data[ 'points' ] . append(point) else : batch_data[ 'points' ] . append(point) Batch . sort_points(batch_data) return batch_data","title":"Helper class for working with data-batch objects."},{"location":"datetime-parsing.html","text":"Timestamps and Datetime strings in Faxe In Faxe we deal a lot with timestamps and datetime strings, in fact, every message that flows through all the nodes, has a timestamp with it. Its inner representation of points in time is always an integer denoting the milliseconds that past since 1.1.1970. There is currently no timezone handling in Faxe. Datetime string parsing Faxe uses its own library to parse datetime strings (from the outside) into it's internal timestamp format. This is done in different nodes such as the mqtt_subscribe and the amqp_consume node. Furthermore, datetime parsing can be done in lambda-expression with the dt_parse function. When parsing datetime strings with subsecond values smaller than milliseconds (microseconds and nanoseconds), if present, will be rounded to millisecond values. Parsing examples datetime matching format string 'Mon, 15 Jun 2009 20:45:30 GMT' 'a, d b Y H:M:S Z' '19.08.01 17:33:44,867000 ' 'd.m.y H:M:S,u ' '8/28/2033 8:03:45.576000 PM' 'n/d/Y l:M:S.u p' Conversion Specification The format used to parse and format dates closely resembles the one used in strftime() [1]. The most notable exception is that meaningful characters are not prefixed with a percentage sign (%) in datestring . Characters not matching a conversion specification will be copied to the output verbatim when formatting and matched against input when parsing. Meaningful characters can be escaped with a backslash (\\). Character Sequence Parsing Formatting Description a Yes* Yes Abbreviated weekday name (\"Mon\", \"Tue\") A Yes Yes Weekday name (\"Monday\", \"Tuesday\") b Yes* Yes Abbreviated month name (\"Jan\", \"Feb\") B Yes* Yes Month name (\"January\", \"February\") d Yes Yes Day of month with leading zero (\"01\", \"31\") e Yes Yes Day of month without leading zero (\"1\", \"31\") F No Yes ISO 8601 date format (shortcut for \"Y-m-d\") H Yes Yes Hour (24 hours) with leading zero (\"01\", \"23\") I Yes Yes Hour (12 hours) with leading zero (\"01\", \"11\") k Yes Yes Hour (24 hours) without leading zero (\"1\", \"23\") l Yes Yes Hour (12 hours) without leading zero (\"1\", \"11\") m Yes Yes Month with leading zero (\"1\", \"12\") M Yes Yes Minute with leading zero (\"00\", \"59\") n Yes Yes Month without leading zero (\"1\", \"12\") o Yes Yes Ordinal number suffix abbreviation (st, nd, rd, th) p Yes* Yes AM/PM P Yes* Yes a.m./p.m. R No Yes The time as H:M (24 hour format) (\"23:59\") S Yes Yes Seconds with leading zero (\"00\", \"59\") T No Yes The time as H:M:S (24 hour format) (\"23:49:49\") c Yes No Milliseconds, 3 digits with leading zero (\"034\") u Yes Yes Microseconds, 6 digits with leading zero (\"000034\") f Yes No Nanoseconds, 9 digits with leading zero (\"000034000\") y Yes** Yes Year without century (\"02\", \"12\") Y Yes Yes Year including century (\"2002\", \"2012\") z Yes No UTC offset (+0100, +01:00, +01, -0100, -01:00, -01) Z Yes No Abbreviated timezone (UTC, GMT, CET etc) * Case-insensitive when parsing ** Falls back on current century of system when parsing years without century. Special formats dt_format description example 'millisecond' timestamp UTC in milliseconds 1565343079000 'microsecond' timestamp UTC in microseconds 1565343079000123 'nanosecond' timestamp UTC in nanoseconds 1565343079000123456 'second' timestamp UTC in seconds 1565343079 'float_nano' timestamp UTC float with nanosecond precision 1565343079.173588126 'float_micro' timestamp UTC float with microsecond precision 1565343079.173588 'float_millisecond' timestamp UTC float with millisecond precision 1565343079.173 'ISO8601' ISO8601 Datetime format string '2011-10-05T14:48:00.000Z' 'RFC3339' RFC3339 Datetime format string '2018-02-01 15:18:02.088Z'","title":"Timestamps and Datetime strings in Faxe"},{"location":"datetime-parsing.html#timestamps-and-datetime-strings-in-faxe","text":"In Faxe we deal a lot with timestamps and datetime strings, in fact, every message that flows through all the nodes, has a timestamp with it. Its inner representation of points in time is always an integer denoting the milliseconds that past since 1.1.1970. There is currently no timezone handling in Faxe.","title":"Timestamps and Datetime strings in Faxe"},{"location":"datetime-parsing.html#datetime-string-parsing","text":"Faxe uses its own library to parse datetime strings (from the outside) into it's internal timestamp format. This is done in different nodes such as the mqtt_subscribe and the amqp_consume node. Furthermore, datetime parsing can be done in lambda-expression with the dt_parse function. When parsing datetime strings with subsecond values smaller than milliseconds (microseconds and nanoseconds), if present, will be rounded to millisecond values.","title":"Datetime string parsing"},{"location":"datetime-parsing.html#parsing-examples","text":"datetime matching format string 'Mon, 15 Jun 2009 20:45:30 GMT' 'a, d b Y H:M:S Z' '19.08.01 17:33:44,867000 ' 'd.m.y H:M:S,u ' '8/28/2033 8:03:45.576000 PM' 'n/d/Y l:M:S.u p'","title":"Parsing examples"},{"location":"datetime-parsing.html#conversion-specification","text":"The format used to parse and format dates closely resembles the one used in strftime() [1]. The most notable exception is that meaningful characters are not prefixed with a percentage sign (%) in datestring . Characters not matching a conversion specification will be copied to the output verbatim when formatting and matched against input when parsing. Meaningful characters can be escaped with a backslash (\\). Character Sequence Parsing Formatting Description a Yes* Yes Abbreviated weekday name (\"Mon\", \"Tue\") A Yes Yes Weekday name (\"Monday\", \"Tuesday\") b Yes* Yes Abbreviated month name (\"Jan\", \"Feb\") B Yes* Yes Month name (\"January\", \"February\") d Yes Yes Day of month with leading zero (\"01\", \"31\") e Yes Yes Day of month without leading zero (\"1\", \"31\") F No Yes ISO 8601 date format (shortcut for \"Y-m-d\") H Yes Yes Hour (24 hours) with leading zero (\"01\", \"23\") I Yes Yes Hour (12 hours) with leading zero (\"01\", \"11\") k Yes Yes Hour (24 hours) without leading zero (\"1\", \"23\") l Yes Yes Hour (12 hours) without leading zero (\"1\", \"11\") m Yes Yes Month with leading zero (\"1\", \"12\") M Yes Yes Minute with leading zero (\"00\", \"59\") n Yes Yes Month without leading zero (\"1\", \"12\") o Yes Yes Ordinal number suffix abbreviation (st, nd, rd, th) p Yes* Yes AM/PM P Yes* Yes a.m./p.m. R No Yes The time as H:M (24 hour format) (\"23:59\") S Yes Yes Seconds with leading zero (\"00\", \"59\") T No Yes The time as H:M:S (24 hour format) (\"23:49:49\") c Yes No Milliseconds, 3 digits with leading zero (\"034\") u Yes Yes Microseconds, 6 digits with leading zero (\"000034\") f Yes No Nanoseconds, 9 digits with leading zero (\"000034000\") y Yes** Yes Year without century (\"02\", \"12\") Y Yes Yes Year including century (\"2002\", \"2012\") z Yes No UTC offset (+0100, +01:00, +01, -0100, -01:00, -01) Z Yes No Abbreviated timezone (UTC, GMT, CET etc) * Case-insensitive when parsing ** Falls back on current century of system when parsing years without century.","title":"Conversion Specification"},{"location":"datetime-parsing.html#special-formats","text":"dt_format description example 'millisecond' timestamp UTC in milliseconds 1565343079000 'microsecond' timestamp UTC in microseconds 1565343079000123 'nanosecond' timestamp UTC in nanoseconds 1565343079000123456 'second' timestamp UTC in seconds 1565343079 'float_nano' timestamp UTC float with nanosecond precision 1565343079.173588126 'float_micro' timestamp UTC float with microsecond precision 1565343079.173588 'float_millisecond' timestamp UTC float with millisecond precision 1565343079.173 'ISO8601' ISO8601 Datetime format string '2011-10-05T14:48:00.000Z' 'RFC3339' RFC3339 Datetime format string '2018-02-01 15:18:02.088Z'","title":"Special formats"},{"location":"flow_concurrency.html","text":"Flow concurrency From every task (flow) registered with Faxe a number of copies (concurrent flows) can be started, this is called a task-group. Flow concurrency helps with building bridging functionality where it is useful to increase throughput by concurrency. Every member of a task-group will run exactly the same flow.","title":"Flow concurrency"},{"location":"flow_concurrency.html#flow-concurrency","text":"From every task (flow) registered with Faxe a number of copies (concurrent flows) can be started, this is called a task-group. Flow concurrency helps with building bridging functionality where it is useful to increase throughput by concurrency. Every member of a task-group will run exactly the same flow.","title":"Flow concurrency"},{"location":"introduction.html","text":"Intro Faxe at it's core implements the architectural ideas of dataflow computing from the 1970s and 80s. The central idea was to replace the classic von Neumann architecture with something more powerful. In a von Neumann architecture, the processor follows explicit control flow, executing instructions one after another. In a dataflow processor, by contrast, an instruction is ready to execute as soon as all its inputs (typically referred to as \u201ctokens\u201d) are available, rather than when the control flow gets to it. This design promised efficient parallel execution in hardware when many \u201ctokens\u201d were ready to execute. -- The remarkable utility of dataflow computing Nowadays with the need to process large amounts of (flowing) data and with the rise of machine learning frameworks, dataflow computing has gained in popularity again. For example, machine learning frameworks like TensorFlow represent model training and inference as dataflow graphs, and the state transitions of actors (e.g., simulators used in reinforcement learning training) can be represented as dataflow edges, too. Other research has extended the original dataflow graph abstraction for streaming computations. Instead of evaluating the dataflow graph once, with all inputs set at the beginning and all outputs produced at the end of evaluation, a streaming dataflow system continuously executes the dataflow in response to new inputs. This requires incremental processing and a stateful dataflow. In this setting, new inputs from a stream of input data combine with existing computation state inside the dataflow graph (e.g., an accumulator for a streaming sum). -- The remarkable utility of dataflow computing This is exactly where FAXE picks up the dataflow computing idea. To get started, we look at some of these concepts in Faxe. Nodes The components that make up the computing graph are called nodes in faxe. There are many built-in nodes for various different tasks, such as getting data from PLCs or Modbus devices reading and writing data from different databases publishing and consuming data from mqtt or other message brokers like RabbitMQ windowing statistics manipulating fields in data, that flows through the computing graph .... Besides these nodes, FAXE users can also write custom nodes implemented with python , that can be used like the built-in ones in dataflows. FAXE is implemented in Erlang/OTP which makes it possible, that each of the nodes in a flow is running in its own seperate process. These processes share nothing in between them and only communicate with each other through message passing . This makes up for massive parallelism (concurrency) within a flow and also between all the flows running in a FAXE instance, matching exactly the architecture of the dataflow programming paradigm. A FAXE instance can easily have thousands of processes runnning in parallel, which results in great throughput for a lot of data-streams. Data data-point The smallest piece of data-item in FAXE is called a data-point . Since FAXE is mainly used for time series processes, these data-points always carry a unix timestamp in a field called ts with them. So a data-point holds data for a specific point in time and a series of such data-points then form this unbounded stream of data we call time series data. How does this data look like ? We can think of the before mentioned data-point as a JSON object (though internally it is not exactly JSON). { \"ts\" : 1629812164152 , \"value\" : 2.33 } The timestamp is always there and the field holding it is always called ts . Next to the timestamp a data-point can have any number of other fields , a field can be of the basic data-type like int, string, float , or it can be an object itself, possibly deeply nested. Basically everything that is allowed in the JSON format. { \"ts\" : 1629812164152 , \"values\" :{ \"value1\" : 12 , \"value2\" : \"a string\" , \"value3\" :{ \"value3_1\" : 4.232341 } } } { \"ts\" : 1629812164152 , \"values\" :[ { \"value1\" : 12 }, { \"value2\" : \"a string\" } ] } In order to deal with these data structures, we can use a basic form of JSON-Path . For example to reference the field value2 in the second example, we would use the following path : values.value2 The path for the field value3_1 in the second example: values.value3.value_3_1 In the third example we have a json-array, we can reference the field value2 like so: values[2].value2 data-batch There is a second type of data-item called data-batch , which is simply a list of data-points . The list of data-points that make up a data-batch is ordered by the points' timestamps. In JSON notation a data-batch will look like this: [ { \"ts\" : 1629812164152 , \"values\" :{ \"value1\" : 12 , \"value2\" : \"res-433\" } }, { \"ts\" : 1629812164154 , \"values\" :{ \"value1\" : 13 , \"value2\" : \"res-124\" } }, { \"ts\" : 1629812164156 , \"values\" :{ \"value1\" : 11 , \"value2\" : \"res-712\" } } ] Strings and References Unlike what is possible in some programming languages, where you can use two different string notations: 'a string' or \"also a string\" , in DFS these two notations have a completely different meaning. In DFS, single quotes are used for strings 'faxe is canned beer' 'baseField.subField' or for text ' SELECT * FROM table ORDER BY timestamp LIMIT 5; ' Double quotes are used for references and are used only in lambda-expressions, to retrieve the value of the specified field from the current data-point. lambda : \"data.value\" > 3 Return whether the value at data.value is greater than 3. Lambda expressions Rest Api How nodes are connected","title":"Intro"},{"location":"introduction.html#intro","text":"Faxe at it's core implements the architectural ideas of dataflow computing from the 1970s and 80s. The central idea was to replace the classic von Neumann architecture with something more powerful. In a von Neumann architecture, the processor follows explicit control flow, executing instructions one after another. In a dataflow processor, by contrast, an instruction is ready to execute as soon as all its inputs (typically referred to as \u201ctokens\u201d) are available, rather than when the control flow gets to it. This design promised efficient parallel execution in hardware when many \u201ctokens\u201d were ready to execute. -- The remarkable utility of dataflow computing Nowadays with the need to process large amounts of (flowing) data and with the rise of machine learning frameworks, dataflow computing has gained in popularity again. For example, machine learning frameworks like TensorFlow represent model training and inference as dataflow graphs, and the state transitions of actors (e.g., simulators used in reinforcement learning training) can be represented as dataflow edges, too. Other research has extended the original dataflow graph abstraction for streaming computations. Instead of evaluating the dataflow graph once, with all inputs set at the beginning and all outputs produced at the end of evaluation, a streaming dataflow system continuously executes the dataflow in response to new inputs. This requires incremental processing and a stateful dataflow. In this setting, new inputs from a stream of input data combine with existing computation state inside the dataflow graph (e.g., an accumulator for a streaming sum). -- The remarkable utility of dataflow computing This is exactly where FAXE picks up the dataflow computing idea. To get started, we look at some of these concepts in Faxe.","title":"Intro"},{"location":"introduction.html#nodes","text":"The components that make up the computing graph are called nodes in faxe. There are many built-in nodes for various different tasks, such as getting data from PLCs or Modbus devices reading and writing data from different databases publishing and consuming data from mqtt or other message brokers like RabbitMQ windowing statistics manipulating fields in data, that flows through the computing graph .... Besides these nodes, FAXE users can also write custom nodes implemented with python , that can be used like the built-in ones in dataflows. FAXE is implemented in Erlang/OTP which makes it possible, that each of the nodes in a flow is running in its own seperate process. These processes share nothing in between them and only communicate with each other through message passing . This makes up for massive parallelism (concurrency) within a flow and also between all the flows running in a FAXE instance, matching exactly the architecture of the dataflow programming paradigm. A FAXE instance can easily have thousands of processes runnning in parallel, which results in great throughput for a lot of data-streams.","title":"Nodes"},{"location":"introduction.html#data","text":"","title":"Data"},{"location":"introduction.html#data-point","text":"The smallest piece of data-item in FAXE is called a data-point . Since FAXE is mainly used for time series processes, these data-points always carry a unix timestamp in a field called ts with them. So a data-point holds data for a specific point in time and a series of such data-points then form this unbounded stream of data we call time series data.","title":"data-point"},{"location":"introduction.html#how-does-this-data-look-like","text":"We can think of the before mentioned data-point as a JSON object (though internally it is not exactly JSON). { \"ts\" : 1629812164152 , \"value\" : 2.33 } The timestamp is always there and the field holding it is always called ts . Next to the timestamp a data-point can have any number of other fields , a field can be of the basic data-type like int, string, float , or it can be an object itself, possibly deeply nested. Basically everything that is allowed in the JSON format. { \"ts\" : 1629812164152 , \"values\" :{ \"value1\" : 12 , \"value2\" : \"a string\" , \"value3\" :{ \"value3_1\" : 4.232341 } } } { \"ts\" : 1629812164152 , \"values\" :[ { \"value1\" : 12 }, { \"value2\" : \"a string\" } ] } In order to deal with these data structures, we can use a basic form of JSON-Path . For example to reference the field value2 in the second example, we would use the following path : values.value2 The path for the field value3_1 in the second example: values.value3.value_3_1 In the third example we have a json-array, we can reference the field value2 like so: values[2].value2","title":"How does this data look like ?"},{"location":"introduction.html#data-batch","text":"There is a second type of data-item called data-batch , which is simply a list of data-points . The list of data-points that make up a data-batch is ordered by the points' timestamps. In JSON notation a data-batch will look like this: [ { \"ts\" : 1629812164152 , \"values\" :{ \"value1\" : 12 , \"value2\" : \"res-433\" } }, { \"ts\" : 1629812164154 , \"values\" :{ \"value1\" : 13 , \"value2\" : \"res-124\" } }, { \"ts\" : 1629812164156 , \"values\" :{ \"value1\" : 11 , \"value2\" : \"res-712\" } } ]","title":"data-batch"},{"location":"introduction.html#strings-and-references","text":"Unlike what is possible in some programming languages, where you can use two different string notations: 'a string' or \"also a string\" , in DFS these two notations have a completely different meaning. In DFS, single quotes are used for strings 'faxe is canned beer' 'baseField.subField' or for text ' SELECT * FROM table ORDER BY timestamp LIMIT 5; ' Double quotes are used for references and are used only in lambda-expressions, to retrieve the value of the specified field from the current data-point. lambda : \"data.value\" > 3 Return whether the value at data.value is greater than 3.","title":"Strings and References"},{"location":"introduction.html#lambda-expressions","text":"","title":"Lambda expressions"},{"location":"introduction.html#rest-api","text":"","title":"Rest Api"},{"location":"introduction.html#how-nodes-are-connected","text":"","title":"How nodes are connected"},{"location":"metrics.html","text":"Metrics, Connection status, Debug events and Logs For debugging and observability Faxe exposes internal metric as well as connection status events. Furthermore, every running flow can emit debugging and log events. All these events can be published to a mqtt broker. Topics and routing keys Topics for the mqtt emitters can be prefixed with the config-value base_topic (see config ). Note: MQTT topics should not start with a / character. type topic base_topic default metrics per node { base_topic }/metrics/{flow_id}/{node_id}/{metric_name} sys/ metrics per flow { base_topic }/metrics/{flow_id} sys/ conn_status { base_topic }/conn_status/{flow_id}/{node_id} sys/ debug per node { base_topic }/debug/{flow_id}/{node_id}/{debug_type} sys/ logs per node { base_topic }/log/{flow_id}/{node_id} sys/ Node Metrics Faxe will collect and periodically emit various metrics to configurable endpoints. Metrics are collected for each individual node and a summery for whole tasks. These are the metrics that will be collected for every node running in a task: metric name description metric fields items_in number of items a node received from other nodes or over the network items_out number of items a node emitted to other nodes or over some network connection processing_errors the number of errors that occurred during processing mem_used memory usage in bytes msg_q_size number of items currently in the node-process' message-queue processing_time time in milliseconds it took the node to process 1 item Nodes that start a network connection have additional metrics (such as the modbus, s7read, mqtt, ... - nodes): metric name description metric fields bytes_read the number of bytes read from a network port see meter bytes_read_size the size of packets read or received from a network port in bytes see histogram bytes_sent the number of bytes send over the network see meter bytes_sent_size the size of packets sent over a network port in bytes see histogram Examples some metric example datapoints in json-format { \"ts\" : 1592386096330 , \"id\" : \"00000\" , \"df\" : \"92.001\" , \"data\" :{ \"type\" : \"counter\" , \"node_id\" : \"default3\" , \"metric_name\" : \"processing_errors\" , \"flow_id\" : \"41ef642f-e5ee-4c48-8bd9-565de810f242\" , \"counter\" : 0 }} { \"ts\" : 1592386096330 , \"id\" : \"00000\" , \"df\" : \"92.001\" , \"data\" :{ \"type\" : \"histogram\" , \"node_id\" : \"default3\" , \"n\" : 8 , \"min\" : 0.011 , \"metric_name\" : \"processing_time\" , \"mean\" : 0.016625 , \"max\" : 0.028 , \"flow_id\" : \"41ef642f-e5ee-4c48-8bd9-565de810f242\" }} { \"ts\" : 1592386076289 , \"id\" : \"00000\" , \"df\" : \"92.001\" , \"data\" :{ \"type\" : \"gauge\" , \"node_id\" : \"default3\" , \"metric_name\" : \"msg_q_size\" , \"gauge\" : 0 , \"flow_id\" : \"41ef642f-e5ee-4c48-8bd9-565de810f242\" }} { \"ts\" : 1592386076289 , \"id\" : \"00000\" , \"df\" : \"92.001\" , \"data\" :{ \"type\" : \"meter\" , \"one\" : 0.2 , \"node_id\" : \"default3\" , \"metric_name\" : \"items_out\" , \"instant\" : 0.2 , \"five\" : 0.2 , \"fifteen\" : 0.2 , \"count\" : 2 , \"flow_id\" : \"41ef642f-e5ee-4c48-8bd9-565de810f242\" }} Common fields field name meaning data.type the metric type, see table below data.node_id the nodes id data.flow_id id of the flow, the node belongs to Fields by metrics-type metric-type field meaning counter counter total counted number meter instant number of occurrences in the last 5 sec meter one 1 min exponentially weighted moving average meter five 5 min exponentially weighted moving average meter fifteen 15 min exponentially weighted moving average meter count total number gauge gauge point-in-time single value histogram mean mean value histogram min minimum value histogram max maximum value histogram n number of values Flow Metrics For every task there is a summary of the node metrics: { \"ts\" : 1592393302700 , \"id\" : \"00000\" , \"df\" : \"92.002\" , \"data\" :{ \"ts\" : 1631557230000 , \"processing_time\" : 0.088 , \"processing_errors\" : 0 , \"msg_q_size\" : 0 , \"mem_used\" : 17768 , \"items_out\" : 60 , \"items_in\" : 60 , \"bytes_sent_avg\" : 123 , \"bytes_sent\" : 24.6 , \"bytes_read_avg\" : 104 , \"bytes_read\" : 20.8 , \"flow_id\" : \"http_get_test\" } } Fields in flow metrics metric name description note processing_time average processing time for the flow in milliseconds processing_errors sum of errors for all flow nodes msg_q_size total number of items currently in process-queues for all flow nodes mem_used total number of bytes the flow and all of its nodes are currently using items_out maximum of all nodes' items_out value items_in maximum of all nodes' items_in value bytes_sent_avg sum of bytes sent over the network for all nodes in the flow bytes_sent sum of the 5-minute exponential moving averages from all nodes' bytes_sent metrics bytes_read_avg sum of bytes received from the network for all nodes in the flow bytes_read sum of the 5-minute exponential moving averages from all nodes' bytes_read metrics Configuration With the configuration setting metrics.handler.mqtt.enable you can turn on/off publishing of metrics for a faxe instance. If this setting is set to on flow metrics get published with the interval set with metrics.publish_interval . To additionally enable publishing of single node metrics for a faxe flow, use the RestAPI endpoint /v1/task/start_metrics_trace/:task_id/[:duration_minutes] . duration_minutes defaults to the config setting debug.time . Publish metrics events Faxe has 2 different metrics-handlers that can be configured. MQTT and AMQP metrics emitter. See config section for details. Use metrics in tasks Faxe's internal metrics can be used in tasks(flows) with the metrics node. Connection status Faxe tracks the status of every external connection it opens and exposes events. These events can be used in tasks with the conn_status node. They can also be sent to a mqtt and/or amqp broker. Examples connecting to an mqtt-broker on ip 10.14.204.3 and port 2883 ... { \"ts\" : 1592386056299 , \"id\" : \"00000\" , \"df\" : \"92.003\" , \"data\" :{ \"status\" : 2 , \"port\" : 2883 , \"peer\" : \"10.14.204.3\" , \"node_id\" : \"sys\" , \"flow_id\" : \"sys\" , \"connected\" : false , \"conn_type\" : \"mqtt\" }} connected to the mqtt-broker { \"ts\" : 1592386056319 , \"id\" : \"00000\" , \"df\" : \"92.003\" , \"data\" :{ \"status\" : 1 , \"port\" : 1883 , \"peer\" : \"10.14.204.3\" , \"node_id\" : \"sys\" , \"flow_id\" : \"sys\" , \"connected\" : true , \"conn_type\" : \"mqtt\" }} The connection status is represented by the boolean value connected and an enum status . connected status meaning false 0 not connected false 2 connecting true 1 connected Configuration Publish connection status events As stated above, FAXE has 2 different conn_status-handlers that can be configured : See config section for details. Debug and Logs For debugging purposes faxe flows can expose events on items going in and out of every node in a flow. Like with metrics and conn_status events, these events can be published to a mqtt/amqp broker. Debug and Log events must be started explicitly, and they will be published for a certain configurable amount of time. To enable publishing of debug events for a faxe flow, use the RestAPI endpoint /v1/task/start_debug/:task_id/[:duration_minutes] . duration_minutes defaults to the config setting debug.time . (This is for debugging purposes). See config section for details. Example debug data item: { \"ts\" : 1594627419206 , \"id\" : \"00000\" , \"df\" : \"00.000\" , \"data\" : { \"meta\" : { \"type\" : \"item_in\" , \"port\" : 1 , \"node_id\" : \"win_time6\" , \"flow_id\" : \"trace_test\" }, \"data_item\" : \"{\\\"ts\\\":1594627419205,\\\"id\\\":\\\"00000\\\",\\\"df\\\":\\\"00.000\\\",\\\"data\\\":{\\\"val\\\":4.770044683775623}}\" }} Example log { \"ts\" : 1595317825817 , \"id\" : \"00000\" , \"df\" : \"00.000\" , \"data\" : { \"node_id\" : \"eval26\" , \"meta\" : { \"pid\" : \"<0.1750.0>\" , \"node\" : \"faxe@ubuntu\" , \"module\" : \"df_component\" , \"line\" : 316 , \"function\" : \"handle_info\" , \"application\" : \"faxe\" }, \"message\" : \"'error' in component esp_eval caught when processing item: {1,{data_point,1595317823813,#{<<\\\"esp_avg\\\">> => 6.0685635225505425, <<\\\"factored\\\">> => 3.0342817612752713},#{},<<>>}} -- \\\"\\\\n gen_server:try_dispatch/4 line 637\\\\n df_component:handle_info/2 line 314\\\\n esp_eval:process/3 line 39\\\\n esp_eval:eval/4 line 44\\\\n lists:foldl/3 line 1263\\\\n esp_eval:'-eval/4-fun-0-'/4 line 47\\\\n faxe_lambda:execute/3 line 34\\\\n erlang:'/'(undefined, 2)\\\\nEXIT:badarith\\\"\" , \"level\" : \"error\" , \"flow_id\" : \"script5\" }} A data_point caused an error in an eval node.","title":"Metrics, Connection status, Debug events and Logs"},{"location":"metrics.html#metrics-connection-status-debug-events-and-logs","text":"For debugging and observability Faxe exposes internal metric as well as connection status events. Furthermore, every running flow can emit debugging and log events. All these events can be published to a mqtt broker.","title":"Metrics, Connection status, Debug events and Logs"},{"location":"metrics.html#topics-and-routing-keys","text":"Topics for the mqtt emitters can be prefixed with the config-value base_topic (see config ). Note: MQTT topics should not start with a / character. type topic base_topic default metrics per node { base_topic }/metrics/{flow_id}/{node_id}/{metric_name} sys/ metrics per flow { base_topic }/metrics/{flow_id} sys/ conn_status { base_topic }/conn_status/{flow_id}/{node_id} sys/ debug per node { base_topic }/debug/{flow_id}/{node_id}/{debug_type} sys/ logs per node { base_topic }/log/{flow_id}/{node_id} sys/","title":"Topics and routing keys"},{"location":"metrics.html#node-metrics","text":"Faxe will collect and periodically emit various metrics to configurable endpoints. Metrics are collected for each individual node and a summery for whole tasks. These are the metrics that will be collected for every node running in a task: metric name description metric fields items_in number of items a node received from other nodes or over the network items_out number of items a node emitted to other nodes or over some network connection processing_errors the number of errors that occurred during processing mem_used memory usage in bytes msg_q_size number of items currently in the node-process' message-queue processing_time time in milliseconds it took the node to process 1 item Nodes that start a network connection have additional metrics (such as the modbus, s7read, mqtt, ... - nodes): metric name description metric fields bytes_read the number of bytes read from a network port see meter bytes_read_size the size of packets read or received from a network port in bytes see histogram bytes_sent the number of bytes send over the network see meter bytes_sent_size the size of packets sent over a network port in bytes see histogram","title":"Node Metrics"},{"location":"metrics.html#examples","text":"some metric example datapoints in json-format { \"ts\" : 1592386096330 , \"id\" : \"00000\" , \"df\" : \"92.001\" , \"data\" :{ \"type\" : \"counter\" , \"node_id\" : \"default3\" , \"metric_name\" : \"processing_errors\" , \"flow_id\" : \"41ef642f-e5ee-4c48-8bd9-565de810f242\" , \"counter\" : 0 }} { \"ts\" : 1592386096330 , \"id\" : \"00000\" , \"df\" : \"92.001\" , \"data\" :{ \"type\" : \"histogram\" , \"node_id\" : \"default3\" , \"n\" : 8 , \"min\" : 0.011 , \"metric_name\" : \"processing_time\" , \"mean\" : 0.016625 , \"max\" : 0.028 , \"flow_id\" : \"41ef642f-e5ee-4c48-8bd9-565de810f242\" }} { \"ts\" : 1592386076289 , \"id\" : \"00000\" , \"df\" : \"92.001\" , \"data\" :{ \"type\" : \"gauge\" , \"node_id\" : \"default3\" , \"metric_name\" : \"msg_q_size\" , \"gauge\" : 0 , \"flow_id\" : \"41ef642f-e5ee-4c48-8bd9-565de810f242\" }} { \"ts\" : 1592386076289 , \"id\" : \"00000\" , \"df\" : \"92.001\" , \"data\" :{ \"type\" : \"meter\" , \"one\" : 0.2 , \"node_id\" : \"default3\" , \"metric_name\" : \"items_out\" , \"instant\" : 0.2 , \"five\" : 0.2 , \"fifteen\" : 0.2 , \"count\" : 2 , \"flow_id\" : \"41ef642f-e5ee-4c48-8bd9-565de810f242\" }}","title":"Examples"},{"location":"metrics.html#common-fields","text":"field name meaning data.type the metric type, see table below data.node_id the nodes id data.flow_id id of the flow, the node belongs to","title":"Common fields"},{"location":"metrics.html#fields-by-metrics-type","text":"metric-type field meaning counter counter total counted number meter instant number of occurrences in the last 5 sec meter one 1 min exponentially weighted moving average meter five 5 min exponentially weighted moving average meter fifteen 15 min exponentially weighted moving average meter count total number gauge gauge point-in-time single value histogram mean mean value histogram min minimum value histogram max maximum value histogram n number of values","title":"Fields by metrics-type"},{"location":"metrics.html#flow-metrics","text":"For every task there is a summary of the node metrics: { \"ts\" : 1592393302700 , \"id\" : \"00000\" , \"df\" : \"92.002\" , \"data\" :{ \"ts\" : 1631557230000 , \"processing_time\" : 0.088 , \"processing_errors\" : 0 , \"msg_q_size\" : 0 , \"mem_used\" : 17768 , \"items_out\" : 60 , \"items_in\" : 60 , \"bytes_sent_avg\" : 123 , \"bytes_sent\" : 24.6 , \"bytes_read_avg\" : 104 , \"bytes_read\" : 20.8 , \"flow_id\" : \"http_get_test\" } }","title":"Flow Metrics"},{"location":"metrics.html#fields-in-flow-metrics","text":"metric name description note processing_time average processing time for the flow in milliseconds processing_errors sum of errors for all flow nodes msg_q_size total number of items currently in process-queues for all flow nodes mem_used total number of bytes the flow and all of its nodes are currently using items_out maximum of all nodes' items_out value items_in maximum of all nodes' items_in value bytes_sent_avg sum of bytes sent over the network for all nodes in the flow bytes_sent sum of the 5-minute exponential moving averages from all nodes' bytes_sent metrics bytes_read_avg sum of bytes received from the network for all nodes in the flow bytes_read sum of the 5-minute exponential moving averages from all nodes' bytes_read metrics","title":"Fields in flow metrics"},{"location":"metrics.html#configuration","text":"With the configuration setting metrics.handler.mqtt.enable you can turn on/off publishing of metrics for a faxe instance. If this setting is set to on flow metrics get published with the interval set with metrics.publish_interval . To additionally enable publishing of single node metrics for a faxe flow, use the RestAPI endpoint /v1/task/start_metrics_trace/:task_id/[:duration_minutes] . duration_minutes defaults to the config setting debug.time .","title":"Configuration"},{"location":"metrics.html#publish-metrics-events","text":"Faxe has 2 different metrics-handlers that can be configured. MQTT and AMQP metrics emitter. See config section for details.","title":"Publish metrics events"},{"location":"metrics.html#use-metrics-in-tasks","text":"Faxe's internal metrics can be used in tasks(flows) with the metrics node.","title":"Use metrics in tasks"},{"location":"metrics.html#connection-status","text":"Faxe tracks the status of every external connection it opens and exposes events. These events can be used in tasks with the conn_status node. They can also be sent to a mqtt and/or amqp broker.","title":"Connection status"},{"location":"metrics.html#examples_1","text":"connecting to an mqtt-broker on ip 10.14.204.3 and port 2883 ... { \"ts\" : 1592386056299 , \"id\" : \"00000\" , \"df\" : \"92.003\" , \"data\" :{ \"status\" : 2 , \"port\" : 2883 , \"peer\" : \"10.14.204.3\" , \"node_id\" : \"sys\" , \"flow_id\" : \"sys\" , \"connected\" : false , \"conn_type\" : \"mqtt\" }} connected to the mqtt-broker { \"ts\" : 1592386056319 , \"id\" : \"00000\" , \"df\" : \"92.003\" , \"data\" :{ \"status\" : 1 , \"port\" : 1883 , \"peer\" : \"10.14.204.3\" , \"node_id\" : \"sys\" , \"flow_id\" : \"sys\" , \"connected\" : true , \"conn_type\" : \"mqtt\" }} The connection status is represented by the boolean value connected and an enum status . connected status meaning false 0 not connected false 2 connecting true 1 connected","title":"Examples"},{"location":"metrics.html#configuration_1","text":"","title":"Configuration"},{"location":"metrics.html#publish-connection-status-events","text":"As stated above, FAXE has 2 different conn_status-handlers that can be configured : See config section for details.","title":"Publish connection status events"},{"location":"metrics.html#debug-and-logs","text":"For debugging purposes faxe flows can expose events on items going in and out of every node in a flow. Like with metrics and conn_status events, these events can be published to a mqtt/amqp broker. Debug and Log events must be started explicitly, and they will be published for a certain configurable amount of time. To enable publishing of debug events for a faxe flow, use the RestAPI endpoint /v1/task/start_debug/:task_id/[:duration_minutes] . duration_minutes defaults to the config setting debug.time . (This is for debugging purposes). See config section for details. Example debug data item: { \"ts\" : 1594627419206 , \"id\" : \"00000\" , \"df\" : \"00.000\" , \"data\" : { \"meta\" : { \"type\" : \"item_in\" , \"port\" : 1 , \"node_id\" : \"win_time6\" , \"flow_id\" : \"trace_test\" }, \"data_item\" : \"{\\\"ts\\\":1594627419205,\\\"id\\\":\\\"00000\\\",\\\"df\\\":\\\"00.000\\\",\\\"data\\\":{\\\"val\\\":4.770044683775623}}\" }} Example log { \"ts\" : 1595317825817 , \"id\" : \"00000\" , \"df\" : \"00.000\" , \"data\" : { \"node_id\" : \"eval26\" , \"meta\" : { \"pid\" : \"<0.1750.0>\" , \"node\" : \"faxe@ubuntu\" , \"module\" : \"df_component\" , \"line\" : 316 , \"function\" : \"handle_info\" , \"application\" : \"faxe\" }, \"message\" : \"'error' in component esp_eval caught when processing item: {1,{data_point,1595317823813,#{<<\\\"esp_avg\\\">> => 6.0685635225505425, <<\\\"factored\\\">> => 3.0342817612752713},#{},<<>>}} -- \\\"\\\\n gen_server:try_dispatch/4 line 637\\\\n df_component:handle_info/2 line 314\\\\n esp_eval:process/3 line 39\\\\n esp_eval:eval/4 line 44\\\\n lists:foldl/3 line 1263\\\\n esp_eval:'-eval/4-fun-0-'/4 line 47\\\\n faxe_lambda:execute/3 line 34\\\\n erlang:'/'(undefined, 2)\\\\nEXIT:badarith\\\"\" , \"level\" : \"error\" , \"flow_id\" : \"script5\" }} A data_point caused an error in an eval node.","title":"Debug and Logs"},{"location":"os_tuning.html","text":"Tune OS Limits Number of file desciptors https://www.rabbitmq.com/install-debian.html#kernel-resource-limits https://github.com/basho/basho_docs/blob/master/content/riak/kv/2.2.3/using/performance/open-files-limit.md#debian--ubuntu https://docs.riak.com/riak/kv/2.2.3/using/performance/open-files-limit/#debian-ubuntu For current session $ ulimit -n 100000 Permanent settings Linux: In /etc/security/limit.conf set these two lines: * soft nofile 65536 * hard nofile 100000 Docker https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-configuration-file erlang Q flag is used in vm.args +Q Number check actual number used erlang:system_info(port_limit).","title":"Os tuning"},{"location":"os_tuning.html#tune-os-limits","text":"","title":"Tune OS Limits"},{"location":"os_tuning.html#number-of-file-desciptors","text":"https://www.rabbitmq.com/install-debian.html#kernel-resource-limits https://github.com/basho/basho_docs/blob/master/content/riak/kv/2.2.3/using/performance/open-files-limit.md#debian--ubuntu https://docs.riak.com/riak/kv/2.2.3/using/performance/open-files-limit/#debian-ubuntu","title":"Number of file desciptors"},{"location":"os_tuning.html#for-current-session","text":"$ ulimit -n 100000","title":"For current session"},{"location":"os_tuning.html#permanent-settings","text":"Linux: In /etc/security/limit.conf set these two lines: * soft nofile 65536 * hard nofile 100000","title":"Permanent settings"},{"location":"os_tuning.html#docker","text":"https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-configuration-file","title":"Docker"},{"location":"os_tuning.html#erlang","text":"Q flag is used in vm.args +Q Number","title":"erlang"},{"location":"os_tuning.html#check-actual-number-used","text":"erlang:system_info(port_limit).","title":"check actual number used"},{"location":"dfs_examples/consume_data_rewrite_republish.html","text":"Data cleaning and publishing Consume data from an MQTT-Broker and do some cleaning, at the end republish this data. def topic_in = 'my/topic/in' def topic_out = 'my/topic/out' def host = '192.168.1.2' | mqtt_subscribe() . host ( host ) . topic ( topic_in ) . dt_field ( 'UTC-Time' ) . dt_format ( 'float_micro' ) %% here is were we clean data | eval( lambda : int ( \"u_length\" * 1000 ), lambda : int ( \"u_width\" * 1000 ), lambda : int ( \"lc_quantity\" ), lambda : int ( \"pcs_lost\" ) ) %% overwrite the original fields . as ( 'u_length' , 'u_width' , 'lc_quantity' , 'pcs_lost' ) | delete( 'UTC-Time' ) % publish the resulting message | mqtt_publish() . host ( host ) . qos ( 1 ) . topic ( topic_out ) . retained () Note: If topic_in = topic_out we create a loop using the mqtt broker, something we do not want normally.","title":"Consume data rewrite republish"},{"location":"dfs_examples/consume_data_rewrite_republish.html#data-cleaning-and-publishing","text":"Consume data from an MQTT-Broker and do some cleaning, at the end republish this data. def topic_in = 'my/topic/in' def topic_out = 'my/topic/out' def host = '192.168.1.2' | mqtt_subscribe() . host ( host ) . topic ( topic_in ) . dt_field ( 'UTC-Time' ) . dt_format ( 'float_micro' ) %% here is were we clean data | eval( lambda : int ( \"u_length\" * 1000 ), lambda : int ( \"u_width\" * 1000 ), lambda : int ( \"lc_quantity\" ), lambda : int ( \"pcs_lost\" ) ) %% overwrite the original fields . as ( 'u_length' , 'u_width' , 'lc_quantity' , 'pcs_lost' ) | delete( 'UTC-Time' ) % publish the resulting message | mqtt_publish() . host ( host ) . qos ( 1 ) . topic ( topic_out ) . retained () Note: If topic_in = topic_out we create a loop using the mqtt broker, something we do not want normally.","title":"Data cleaning and publishing"},{"location":"dfs_examples/fun_with_http.html","text":"fun with http Using faxe's http nodes. Http GET and POST from and to itself. def path2 = '/faxe_stats' %% first set up a listen node to receive data via http | http_listen () . path ( path2 ) . port ( 8899 ) . payload_type ( 'json' ) . as ( 'recv' ) | debug() %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% def path = '/v1/stats/faxe' def host = '127.0.0.1' def port = 8081 %% call faxe's own rest api, and get some stats fields | http_get () . host ( host ) . port ( port ) . path ( path ) . every ( 3s ) %% post this data to the http_listen node setup at the beginning of the script | http_post() . host ( host ) . port ( 8899 ) . path ( path2 )","title":"Fun with http"},{"location":"dfs_examples/fun_with_http.html#fun-with-http","text":"Using faxe's http nodes. Http GET and POST from and to itself. def path2 = '/faxe_stats' %% first set up a listen node to receive data via http | http_listen () . path ( path2 ) . port ( 8899 ) . payload_type ( 'json' ) . as ( 'recv' ) | debug() %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% def path = '/v1/stats/faxe' def host = '127.0.0.1' def port = 8081 %% call faxe's own rest api, and get some stats fields | http_get () . host ( host ) . port ( port ) . path ( path ) . every ( 3s ) %% post this data to the http_listen node setup at the beginning of the script | http_post() . host ( host ) . port ( 8899 ) . path ( path2 )","title":"fun with http"},{"location":"dfs_examples/message_re-modeling.html","text":"This example uses different faxe nodes to remodel and extend a json message that is received via mqtt. Input message { \"ts\" : 1634657531710 , \"data\" : { \"availablitiy\" : \"NRDY\" , \"operatingState\" : \"ON\" , \"operatingMode\" : \"AUTO\" , \"alarmState\" : \"NOAL\" , \"errorStates\" : [], \"Typ\" : \"ATS\" , \"Id\" : 3179 } } Desired message { \"ts\" : 1634657531710 , \"data\" : { \"robot_state\" : \"BUSY\" , \"order_state\" : { \"name\" : \"NRDY\" , \"id\" : 1 }, \"operating_state\" : { \"name\" : \"ON\" , \"id\" : 0 }, \"operating_mode\" : { \"name\" : \"AUTO\" , \"id\" : 1 }, \"alarm_state\" : { \"name\" : \"NOAL\" , \"id\" : 0 , \"errors\" : [] }, \"Typ\" : \"ATS\" , \"Id\" : 3179 } } Example %% enum mappings def alarm_state_map = '{\"NOAL\":0,\"ERR\":1}' def opmode_map = '{\"NOMO\":0,\"AUTO\":1,\"MANU\":2}' def opstate_map = '{\"ON\":0,\"OFF\":1}' def or der_state_map = '{\"RDY\":0,\"NRDY\":1,\"OFF\":2}' def topic_out = 'msm/r1/grp/condition/robot_state' def republish_timeout = 15s def topic_in = 'msm/r1/grp/wms/RMST/v1' def out = | mqtt_subscribe() . host ( '10.10.1.102' ) . topic ( topic_in ) . include_topic ( false ) | rename() . fields ( 'data.alarmState' , 'data.operatingMode' , 'data.operatingState' , 'data.errorStates' , 'data.availablitiy' ) . as_fields ( 'data.alarm_state.name' , 'data.operating_mode.name' , 'data.operating_state.name' , 'data.alarm_state.errors' , 'data.order_state.name' ) | eval( lambda : map_get ( \"data.alarm_state.name\" , alarm_state_map ), lambda : map_get ( \"data.operating_mode.name\" , opmode_map ), lambda : map_get ( \"data.operating_state.name\" , opstate_map ), lambda : map_get ( \"data.order_state.name\" , or der_state_map ) ) . as ( 'data.alarm_state.id' , 'data.operating_mode.id' , 'data.operating_state.id' , 'data.order_state.id' ) | case( lambda : \"data.alarm_state.name\" == 'ERR' , lambda : \"data.operating_state.name\" == 'ON' AND \"data.operating_mode.name\" == 'AUTO' AND \"data.order_state.name\" == 'NRDY' , lambda : \"data.operating_state.name\" == 'ON' AND \"data.operating_mode.name\" == 'AUTO' AND \"data.order_state.name\" == 'IDLE' ) . values ( 'ERROR' , 'BUSY' , 'IDLE' ) .default( 'OFF' ) . as ( 'data.robot_state' ) out | mqtt_publish() . topic ( topic_out )","title":"Message re modeling"},{"location":"dfs_examples/message_re-modeling.html#input-message","text":"{ \"ts\" : 1634657531710 , \"data\" : { \"availablitiy\" : \"NRDY\" , \"operatingState\" : \"ON\" , \"operatingMode\" : \"AUTO\" , \"alarmState\" : \"NOAL\" , \"errorStates\" : [], \"Typ\" : \"ATS\" , \"Id\" : 3179 } }","title":"Input message"},{"location":"dfs_examples/message_re-modeling.html#desired-message","text":"{ \"ts\" : 1634657531710 , \"data\" : { \"robot_state\" : \"BUSY\" , \"order_state\" : { \"name\" : \"NRDY\" , \"id\" : 1 }, \"operating_state\" : { \"name\" : \"ON\" , \"id\" : 0 }, \"operating_mode\" : { \"name\" : \"AUTO\" , \"id\" : 1 }, \"alarm_state\" : { \"name\" : \"NOAL\" , \"id\" : 0 , \"errors\" : [] }, \"Typ\" : \"ATS\" , \"Id\" : 3179 } }","title":"Desired message"},{"location":"dfs_examples/message_re-modeling.html#example","text":"%% enum mappings def alarm_state_map = '{\"NOAL\":0,\"ERR\":1}' def opmode_map = '{\"NOMO\":0,\"AUTO\":1,\"MANU\":2}' def opstate_map = '{\"ON\":0,\"OFF\":1}' def or der_state_map = '{\"RDY\":0,\"NRDY\":1,\"OFF\":2}' def topic_out = 'msm/r1/grp/condition/robot_state' def republish_timeout = 15s def topic_in = 'msm/r1/grp/wms/RMST/v1' def out = | mqtt_subscribe() . host ( '10.10.1.102' ) . topic ( topic_in ) . include_topic ( false ) | rename() . fields ( 'data.alarmState' , 'data.operatingMode' , 'data.operatingState' , 'data.errorStates' , 'data.availablitiy' ) . as_fields ( 'data.alarm_state.name' , 'data.operating_mode.name' , 'data.operating_state.name' , 'data.alarm_state.errors' , 'data.order_state.name' ) | eval( lambda : map_get ( \"data.alarm_state.name\" , alarm_state_map ), lambda : map_get ( \"data.operating_mode.name\" , opmode_map ), lambda : map_get ( \"data.operating_state.name\" , opstate_map ), lambda : map_get ( \"data.order_state.name\" , or der_state_map ) ) . as ( 'data.alarm_state.id' , 'data.operating_mode.id' , 'data.operating_state.id' , 'data.order_state.id' ) | case( lambda : \"data.alarm_state.name\" == 'ERR' , lambda : \"data.operating_state.name\" == 'ON' AND \"data.operating_mode.name\" == 'AUTO' AND \"data.order_state.name\" == 'NRDY' , lambda : \"data.operating_state.name\" == 'ON' AND \"data.operating_mode.name\" == 'AUTO' AND \"data.order_state.name\" == 'IDLE' ) . values ( 'ERROR' , 'BUSY' , 'IDLE' ) .default( 'OFF' ) . as ( 'data.robot_state' ) out | mqtt_publish() . topic ( topic_out )","title":"Example"},{"location":"dfs_examples/python_double.html","text":"Custom python node Using a custom python node called double to double values of a data_batch. Here data is produced every 2s, then accumulated to a data_batch of length 6, the result gets then forwarded to our custom python node, which doubles all values of the field val . dfs | value_emitter() . every ( 2s ) . type ( point ) | win_event() . every ( 6 ) @ double () . field ( 'val' ) . as ( 'double_val' ) | debug() python from faxe import Faxe class Double (Faxe): @staticmethod def options (): opts = [ ( b'field' , b'string' ), ( b'as' , b'string' ) ] return opts def init (self, args): self . fieldname = args[ b'field' ] self . asfieldname = args[ b'as' ] def handle_point (self, point_data): self . emit(self . calc(point_data)) def handle_batch (self, batch_data): out_list = list() for point in batch_data: out_list . append(self . calc(point)) self . emit(out_list) def calc (self, point_dict): point_dict[self . asfieldname] = point_dict[self . fieldname] * 2 return point_dict","title":"Python double"},{"location":"dfs_examples/python_double.html#custom-python-node","text":"Using a custom python node called double to double values of a data_batch. Here data is produced every 2s, then accumulated to a data_batch of length 6, the result gets then forwarded to our custom python node, which doubles all values of the field val .","title":"Custom python node"},{"location":"dfs_examples/python_double.html#dfs","text":"| value_emitter() . every ( 2s ) . type ( point ) | win_event() . every ( 6 ) @ double () . field ( 'val' ) . as ( 'double_val' ) | debug()","title":"dfs"},{"location":"dfs_examples/python_double.html#python","text":"from faxe import Faxe class Double (Faxe): @staticmethod def options (): opts = [ ( b'field' , b'string' ), ( b'as' , b'string' ) ] return opts def init (self, args): self . fieldname = args[ b'field' ] self . asfieldname = args[ b'as' ] def handle_point (self, point_data): self . emit(self . calc(point_data)) def handle_batch (self, batch_data): out_list = list() for point in batch_data: out_list . append(self . calc(point)) self . emit(out_list) def calc (self, point_dict): point_dict[self . asfieldname] = point_dict[self . fieldname] * 2 return point_dict","title":"python"},{"location":"dfs_examples/read_modbus_publish_mqtt.html","text":"Modbus to MQTT In this example we write a simple DFS which reads energy data from a modbus-tcp device periodically and publishes to an mqtt broker. def device_id = 255 def modbus_ip = '127.0.0.1' def mqtt_broker = '10.14.204.3' | modbus() . ip ( '127.0.0.1' ) %% not the default port here . port ( 8899 ) . device ( device_id ) %% we read the values every second . every ( 1s ) %% we read 3 values . function ( 'coils' , 'hregs' , 'iregs' ) %% start addresses . from ( 2127 , 3008 , 104 ) %% amount of data for each value . count ( 1 , 2 , 2 ) %% we want these resulting fieldnames . as ( 'ActiveEnergyConsumption' , 'MaximalCurrentValue' , 'BlindEnergyDelivered' ) %% add some default values to each message | default() . fields ( 'id' , 'vs' , 'df' ) . field_values ( 'my_id_string' , 1 , '01.010' ) %% publish to mqtt broker | mqtt_publish() . host ( mqtt_broker ) . port ( 1883 ) . qos ( 1 ) . topic ( 'ttopic/energy' ) . retained ()","title":"Read modbus publish mqtt"},{"location":"dfs_examples/read_modbus_publish_mqtt.html#modbus-to-mqtt","text":"In this example we write a simple DFS which reads energy data from a modbus-tcp device periodically and publishes to an mqtt broker. def device_id = 255 def modbus_ip = '127.0.0.1' def mqtt_broker = '10.14.204.3' | modbus() . ip ( '127.0.0.1' ) %% not the default port here . port ( 8899 ) . device ( device_id ) %% we read the values every second . every ( 1s ) %% we read 3 values . function ( 'coils' , 'hregs' , 'iregs' ) %% start addresses . from ( 2127 , 3008 , 104 ) %% amount of data for each value . count ( 1 , 2 , 2 ) %% we want these resulting fieldnames . as ( 'ActiveEnergyConsumption' , 'MaximalCurrentValue' , 'BlindEnergyDelivered' ) %% add some default values to each message | default() . fields ( 'id' , 'vs' , 'df' ) . field_values ( 'my_id_string' , 1 , '01.010' ) %% publish to mqtt broker | mqtt_publish() . host ( mqtt_broker ) . port ( 1883 ) . qos ( 1 ) . topic ( 'ttopic/energy' ) . retained ()","title":"Modbus to MQTT"},{"location":"dfs_examples/using_mem_node.html","text":"mem node Using the mem node standalone with predefined data as a lookup table. def default_map = '{\"key1\":\"topic/1/new\", \"key2\":\"topic/2/new\", \"key3\":\"topic/3/new\"}' %% setup the mem node with the json-map, we can reference this map later in lambda expressions with %% the key `topic_lookup` | mem() . key ( 'topic_lookup' ) .default( default_map ) . default_json ()","title":"Using mem node"},{"location":"dfs_examples/using_mem_node.html#mem-node","text":"Using the mem node standalone with predefined data as a lookup table. def default_map = '{\"key1\":\"topic/1/new\", \"key2\":\"topic/2/new\", \"key3\":\"topic/3/new\"}' %% setup the mem node with the json-map, we can reference this map later in lambda expressions with %% the key `topic_lookup` | mem() . key ( 'topic_lookup' ) .default( default_map ) . default_json ()","title":"mem node"},{"location":"dfs_script_language/index.html","text":"Introducing the DFS Script Language Faxe uses a Domain Specific Language(DSL) named dfs (Dataflow Scripting Language) to define dataflow tasks involving the extraction, collection, transformation and loading and writing of data and involving, moreover, the tracking of arbitrary changes and the detection of events within data. Dfs is heavily influenced by InfluxData's TICKScript . Dfs is used in .dfs files or via API to define pipelines and graphs for processing data. The Dfs language is designed to chain together the invocation of data processing operations defined in nodes. At the heart of its's engine, faxe will run an acyclic graph of computing nodes. Every node runs in its own erlang process. DFS Definitions Keywords Word Usage true boolean true false boolean false TRUE boolean true FALSE boolean false lambda: used to denote lambda expression e: used to denote script expressions def starts a variable declaration Operators Operator Usage + addition operator - substraction operator / division operator * multiplication operator AND and OR or < less than > greater than =< less than or equal <= less than or equal => greater or equal >= greater or equal == equal != Not equal /= Not equal ! Logical Not rem remainder div integer division These operators are mainly used in Lambda expressions. Chaining operators Operator Usage Example | Used to declare a new node instance and chains it to the node above it (if any) |some_node() |debug() || Used to reference a macro script ||some_macro().some_param(3) . Declares a property (or parameter) call, setting or changing an internal param in the node to which it belongs |log() .file('log1.txt') @ Declares a user defined node written in python . Same as | , but for user defined nodes |some_node() ... @mynode() Variables and literals Variables are declared using the keyword def at the start of a declaration. Variables are immutable and cannot be reassigned new values later on in the script, though they can be used in other declarations and can be passed into functions, property calls and text-templates. Variable declarations def string = 'this is a string !' def text = ' this is a text with some weird chars :// %& ' %% escape single quotes in strings with a second single quote: def string_with_single quotes = 'my string has ''single quotes'' in it' def func = lambda : \"value\" / 3 def expr = e : str_replace ( string , ' ' , '_' ) def meas = 4.44 % A lambda expression as literal def func2 = lambda : int ( meas / 13 ) def an_int = 32342 def a_float = 2131.342 % a chain can also be bound to a declaration def in1 = | mqtt_subscribe() . host ( '127.0.0.1' ) . topic ( 'some/topic' ) % it can then be used like so in1 | debug() Datatypes DFS recognizes a view basic data types, the type of the literal will be interpreted from its declaration. Type name Description Examples string String/text type. Single quotes are used for strings, strings can also be multiline. To use single quotes in your string, simple use 2 single quotes (since 0.19.0) 'this_is_a_string' since 0.19.0 : 'SELECT MEAN(obj[''current'']) FROM mytable' integer Integer type. Arbitrarily big ints are allowed 123456789987654321, 55 float Floating point number. May be arbitrarily big 12.343422023, 5.6 double Same as float 12.343422023, 5.6 duration A duration literal. See section below. 34s, 500ms, 2d lambda A lambda expression. See extra section in this documentation. lambda: str_downcase('BIG') list A list of above simple types. ['a', 'b'] [1, 456, 4536] Duration literals Duration literals define a span of time. A duration literal is comprised of two parts: an integer and a duration unit. It is essentially an integer terminated by one or a pair of reserved characters, which represent a unit of time. The following table presents the time units used in declaring duration types. Unit Meaning ms millisecond s second m minute h hour d day w week Internally all time and duration related values are converted to milliseconds. Examples def span = 10s def frequency = 10m def short = 50ms | win_time() . period ( 1h ) . every ( 30m ) Text templates Embedding literal values in a string, using double curly braces: {{ variable_name }} Use def this_portion = 'it' def text_template = 'Some string/text where {{this_portion}} will get replaced' In the above example, after compilation of the dfs script the variable text_template will hold the following value: Some string/text where it will get replaced Text templates can be used in variable declarations like in the above example, they can be used in node-parameter and option-parameter calls. When used in template scripts string/text templates can be very powerful. The variable this_portion could be overwritten with a new value for every instantiation of a template script. More examples def an_integer = 33 def a_float = 345.78 def a_string = 'Embedding an integer: {{an_integer}} and a floating point number: {{a_float}}.' % results in: 'Embedding an integer: 33 and a floating point number: 345.78.' %% list: def fruits = '[ {\"color\": \"orange\", \"name\": \"orange\", \"peel\": true}, {\"color\": \"orange\", \"name\": \"mandarin\", \"peel\": true}, {\"color\": \"orange\", \"name\": \"peach\", \"peel\": false}, {\"color\": \"orange\", \"name\": \"navel-orange\", \"peel\": true}, {\"color\": \"yellow\", \"name\": \"lemon\", \"peel\": true} ]' def selected_fruits = e : select ( 'name' , [{ 'peel' , true }], fruits ) def citric_fruits = 'citrus fruits are: {{selected_fruits}}.' % results in: 'citrus fruits are: orange,mandarin,navel-orange,lemon.' There is another version of text-templating which uses a value inside the current data_point, that can be used with some nodes in faxe: {% raw %} {{ \"field_name\" }} | email() . body ( ' No data since {{\"datetime\"}} on topic ''ttgw/energy'', last value was {{\"val\"}}. ' ) Note: We use double quotes to reference a field in the current data_item. Here the values for datetime and val will be taken from the current data_point in the email node. If a field used in a text_template is not present in the current data_point, the string 'undefined' will be used.","title":"Introducing the DFS Script Language"},{"location":"dfs_script_language/index.html#introducing-the-dfs-script-language","text":"Faxe uses a Domain Specific Language(DSL) named dfs (Dataflow Scripting Language) to define dataflow tasks involving the extraction, collection, transformation and loading and writing of data and involving, moreover, the tracking of arbitrary changes and the detection of events within data. Dfs is heavily influenced by InfluxData's TICKScript . Dfs is used in .dfs files or via API to define pipelines and graphs for processing data. The Dfs language is designed to chain together the invocation of data processing operations defined in nodes. At the heart of its's engine, faxe will run an acyclic graph of computing nodes. Every node runs in its own erlang process.","title":"Introducing the DFS Script Language"},{"location":"dfs_script_language/index.html#dfs-definitions","text":"","title":"DFS Definitions"},{"location":"dfs_script_language/index.html#keywords","text":"Word Usage true boolean true false boolean false TRUE boolean true FALSE boolean false lambda: used to denote lambda expression e: used to denote script expressions def starts a variable declaration","title":"Keywords"},{"location":"dfs_script_language/index.html#operators","text":"Operator Usage + addition operator - substraction operator / division operator * multiplication operator AND and OR or < less than > greater than =< less than or equal <= less than or equal => greater or equal >= greater or equal == equal != Not equal /= Not equal ! Logical Not rem remainder div integer division These operators are mainly used in Lambda expressions.","title":"Operators"},{"location":"dfs_script_language/index.html#chaining-operators","text":"Operator Usage Example | Used to declare a new node instance and chains it to the node above it (if any) |some_node() |debug() || Used to reference a macro script ||some_macro().some_param(3) . Declares a property (or parameter) call, setting or changing an internal param in the node to which it belongs |log() .file('log1.txt') @ Declares a user defined node written in python . Same as | , but for user defined nodes |some_node() ... @mynode()","title":"Chaining operators"},{"location":"dfs_script_language/index.html#variables-and-literals","text":"Variables are declared using the keyword def at the start of a declaration. Variables are immutable and cannot be reassigned new values later on in the script, though they can be used in other declarations and can be passed into functions, property calls and text-templates.","title":"Variables and literals"},{"location":"dfs_script_language/index.html#variable-declarations","text":"def string = 'this is a string !' def text = ' this is a text with some weird chars :// %& ' %% escape single quotes in strings with a second single quote: def string_with_single quotes = 'my string has ''single quotes'' in it' def func = lambda : \"value\" / 3 def expr = e : str_replace ( string , ' ' , '_' ) def meas = 4.44 % A lambda expression as literal def func2 = lambda : int ( meas / 13 ) def an_int = 32342 def a_float = 2131.342 % a chain can also be bound to a declaration def in1 = | mqtt_subscribe() . host ( '127.0.0.1' ) . topic ( 'some/topic' ) % it can then be used like so in1 | debug()","title":"Variable declarations"},{"location":"dfs_script_language/index.html#datatypes","text":"DFS recognizes a view basic data types, the type of the literal will be interpreted from its declaration. Type name Description Examples string String/text type. Single quotes are used for strings, strings can also be multiline. To use single quotes in your string, simple use 2 single quotes (since 0.19.0) 'this_is_a_string' since 0.19.0 : 'SELECT MEAN(obj[''current'']) FROM mytable' integer Integer type. Arbitrarily big ints are allowed 123456789987654321, 55 float Floating point number. May be arbitrarily big 12.343422023, 5.6 double Same as float 12.343422023, 5.6 duration A duration literal. See section below. 34s, 500ms, 2d lambda A lambda expression. See extra section in this documentation. lambda: str_downcase('BIG') list A list of above simple types. ['a', 'b'] [1, 456, 4536]","title":"Datatypes"},{"location":"dfs_script_language/index.html#duration-literals","text":"Duration literals define a span of time. A duration literal is comprised of two parts: an integer and a duration unit. It is essentially an integer terminated by one or a pair of reserved characters, which represent a unit of time. The following table presents the time units used in declaring duration types. Unit Meaning ms millisecond s second m minute h hour d day w week Internally all time and duration related values are converted to milliseconds.","title":"Duration literals"},{"location":"dfs_script_language/index.html#examples","text":"def span = 10s def frequency = 10m def short = 50ms | win_time() . period ( 1h ) . every ( 30m )","title":"Examples"},{"location":"dfs_script_language/index.html#text-templates","text":"Embedding literal values in a string, using double curly braces: {{ variable_name }}","title":"Text templates"},{"location":"dfs_script_language/index.html#use","text":"def this_portion = 'it' def text_template = 'Some string/text where {{this_portion}} will get replaced' In the above example, after compilation of the dfs script the variable text_template will hold the following value: Some string/text where it will get replaced Text templates can be used in variable declarations like in the above example, they can be used in node-parameter and option-parameter calls. When used in template scripts string/text templates can be very powerful. The variable this_portion could be overwritten with a new value for every instantiation of a template script.","title":"Use"},{"location":"dfs_script_language/index.html#more-examples","text":"def an_integer = 33 def a_float = 345.78 def a_string = 'Embedding an integer: {{an_integer}} and a floating point number: {{a_float}}.' % results in: 'Embedding an integer: 33 and a floating point number: 345.78.' %% list: def fruits = '[ {\"color\": \"orange\", \"name\": \"orange\", \"peel\": true}, {\"color\": \"orange\", \"name\": \"mandarin\", \"peel\": true}, {\"color\": \"orange\", \"name\": \"peach\", \"peel\": false}, {\"color\": \"orange\", \"name\": \"navel-orange\", \"peel\": true}, {\"color\": \"yellow\", \"name\": \"lemon\", \"peel\": true} ]' def selected_fruits = e : select ( 'name' , [{ 'peel' , true }], fruits ) def citric_fruits = 'citrus fruits are: {{selected_fruits}}.' % results in: 'citrus fruits are: orange,mandarin,navel-orange,lemon.' There is another version of text-templating which uses a value inside the current data_point, that can be used with some nodes in faxe: {% raw %} {{ \"field_name\" }} | email() . body ( ' No data since {{\"datetime\"}} on topic ''ttgw/energy'', last value was {{\"val\"}}. ' ) Note: We use double quotes to reference a field in the current data_item. Here the values for datetime and val will be taken from the current data_point in the email node. If a field used in a text_template is not present in the current data_point, the string 'undefined' will be used.","title":"More examples"},{"location":"dfs_script_language/built-in_functions.html","text":"A complete list of currently supported functions. These function can be used in lambda and script expressions Type conversion With a few exceptions every type can be converted to every other type. Bool bool(a_value) -> true|false Integer int(value) -> integer If value is a float, then int(value) is equivalent to trunc(value) Float float(value) -> float String string(val) -> string If val is a list: String string(list) -> string Converting a list into a string with string is equivalent to list_join ( ',' , list_of_strings ( List )) Converts every entry of List to a string first and then joins the resulting list with a comma. [ 1 , 2 , 3 ] => [ '1' , '2' , '3' ] => '1,2,3' Type checks Function is_string(Data) -> bool is_integer(Data) -> bool is_int(Data) -> bool is_float(Data) -> bool is_number(Data) -> bool is_list(Data) -> bool is_duration(Data) -> bool is_boolean(Data) -> bool is_bool(Data) -> bool Time functions Every data_point in faxe contains a field called ts , which holds a UTC timestamp in milliseconds. Function Description now() -> integer returns an utc timestamp in milliseconds dt_parse(ts, formatstring) -> integer used to parse a datetime string to the internal format, see datetime-parsing for details to_iso8601(ts) -> string converts the timestamp to an ISO8601 datetime string to_rfc3339(ts) -> string converts the timestamp to an RFC3339 datetime string from_duration(ts) -> integer converts a duration to it's millisecond equivalent millisecond(ts) -> integer milliseconds within the second [0, 999] second(ts) -> integer second within the minute [0, 59] minute(ts) -> integer minute within the hour [0, 59] hour(ts) -> integer hour within the day [0, 23] day(ts) -> integer day within the month [1, 31] day_of_week(ts) -> integer the weekday with week [1, 7] 1 is monday week(ts) -> integer isoweek-number within year [1, 53] month(ts) -> integer month within the year [1, 12] time_align(ts, duration) -> integer align the given timestamp to duration Examples: lambda : hour ( \"ts\" ) >= 8 AND hour ( \"ts\" ) < 19 The above expression evaluates to true if the hour of the day for the data point falls between 08:00 and 19:00. lambda : time_align ( \"ts\" , 3m ) Will align every timestamp (ts) to a multiple of the 3 minutes within an hour (ie: 00:00, 00:03, 00:06, ...) Math functions Function Description abs(x) -> number acos(x) -> float acosh(x) -> float asin(x) -> float asinh(x) -> float atan(x) -> float atan2(y, x) -> float atanh(x) -> float ceil(x) -> float cos(x) -> float cosh(x) -> float exp(x) -> float floor(x) -> float fmod(x, y) -> float log(x) -> float log10(x) -> float log2(x) -> float max(x, y) -> number max(list) -> number min(x, y) -> number min(list) -> number pi() -> float gives pi pow(x, y) -> float round(x) -> integer round a number to an integer round_float(x, precision) -> float round a float (x) with the given precision trunc(x) -> integer Returns an integer by truncating a number sin(x) -> float sinh(x) -> float sqrt(x) -> float tan(x) -> float tanh(x) -> float String functions String positions start with index 0. Function Description str_at(x, pos) -> string/undefined Returns the grapheme in the position of the given utf8 string. If position is greater than string length, then it returns undefined. Negative offsets count back from the end of the string. str_capitalize(x) -> string Converts the first character in the given string to uppercase and the remaining to lowercase str_contains(x, contents) -> bool Check if string contains any of the given contents str_downcase(x) -> string Convert all characters on the given string to lowercase str_enclose(Wrapper, StringOrList) -> string or list Prepend and append Wrapper to a string or each entry of a list of strings str_ends_with(x, suffix) -> string Returns true if string ends with suffix, otherwise false. str_ends_with_any(x, suffixes) -> string Returns true if string ends with any of the suffixes given, otherwise false. str_eqi(x,y) -> bool Compares strings case insensitively str_first(x) -> string/undefined Returns the first grapheme from an utf8 string, undefined if the string is empty str_last(x) -> string/undefined Returns the last grapheme from an utf8 string, undefined if the string is empty str_length(x) -> int Returns the number of unicode graphemes in an utf8 string str_lstrip(x) -> string Returns a string where leading Unicode whitespace has been removed str_lstrip(x, char) -> string Returns a string where leading char have been removed str_pad_leading/2 str_pad_leading/3 str_pad_trailing/2 str_pad_trailing/3 str_replace(x, patt, repl) -> string Returns a new string based on subject by replacing the parts matching pattern by replacement. str_replace_leading/3 Replaces all leading occurrences of match by replacement of match in string. str_replace_trailing/3 Replaces all trailing occurrences of match by replacement of match in string. str_replace_prefix(x, match, repl) -> string Replaces prefix in string by replacement if it matches match. Returns the string untouched if there is no match. If match is an empty string (\"\"), replacement is just prepended to string. str_replace_suffix(x, match, repl) -> string Replaces suffix in string by replacement if it matches match. Returns the string untouched if there is no match. If match is an empty string (\"\"), replacement is just appended to string. str_reverse(x) -> string Reverses the given string. str_rstrip(x) -> string Returns a string where trailing Unicode whitespace has been removed str_rstrip(x, char) -> string Returns a string where trailing char have been removed str_slice(x, start, len) -> string Returns a substring starting at the offset given by the first, and a length given by the second param, if offset is negative, count back from end of string. str_split/1 str_split/2 str_split/3 str_split_at/2 str_split_by_any/2 str_split_by_any/3 str_split_by_re/2 str_split_by_re/3 str_starts_with(x, pre) -> bool Returns true if string starts with Prefix str_starts_with_any(x, prefixes) -> bool Returns true if string starts with any of the prefixes given, otherwise false. str_strip(x) -> string Returns a string where leading/trailing Unicode whitespace has been removed str_strip(x, char) -> string Returns a string where leading/trailing char have been removed str_upcase(x) -> string Convert all characters on the given string to uppercase Lists and Maps (jsn) Function Description Example from_json_string(String) -> faxe map or list convert a valid json string into faxe's internal data-structure from_json_string('{\"a\":\"b\"}') to_json_string(String) -> string convert faxe's internal data-structure into a json string to_json_string(\"data.struct\") size(ListOrMap) -> integer get the number of entries in a list or map size(\"data.lines\") head(List) -> any returns the head, or the first element, of list List head(\"data.lines\") nth(Integer, List) -> any returns the nth element from list List, list indices start with 1 nth(N, \"data.lines\") list_unique(List) -> any returns a new list with only unique values from List list_unique(\"data.lines\") list_join(List) -> any equivalent to list_join(',', List) list_join(\"data.lines\") list_join(Separator, List) -> any join a list of strings with a separator string list_join('-', \"data.lines\") list_of_strings(List) -> any convert every entry of a list to a string and return the list of strings list_of_strings(\"data.lines\") member(Ele, ListOrMap) -> bool check for list/set membership of a value, or when used with a map, check if Ele is a key in the map not_member(Ele, ListOrMap) -> bool map_get(Key, Map) -> any get a value from a map, equivalent to map_get(Key, Map, 'undefined') map_get(\"topic\", '{\"data/temp/val1\": \"temp\"}') map_get(Key, Map, Default) -> any get a value from a map, Default is returned, if the key is not present in map map_get(\"topic\", '{\"data/temp/val1\": \"temp\"}', 'no_match') select(Key, Jsn) -> list Select every value with the path Key from a json-array See description below. select(Key, Where, Jsn) -> list Select every value with the path Key and conditions Where from a json-array. Elements that do not meet the given conditions are filtered out. See description below. select_all(Key, Where, Jsn) -> list Select every value with the path Key and conditions Where from a json-array. The list of Where conditions are OR ed in this case, so there is a select operation for every entry in Where See description below. select_first(Key, Jsn) -> any Select the first value with the path Key from a json-array. See description below. select_first(Key, Where, Jsn) -> any Select the first value with the path Key and conditions Where from a json-array. See description below. select_any(Key, Where, Jsn) -> any Select the first value with the path Key that is found with any of the Where conditions from a json-array. See description below. select, select_first, select_any With these functions we can select values from a json-array like datastructure, meaning an array of - possibly nested - json objects. def fruits = '[ {\"color\": \"orange\", \"name\": \"orange\", \"peel\": true}, {\"color\": \"orange\", \"name\": \"mandarin\", \"peel\": true}, {\"color\": \"orange\", \"name\": \"peach\", \"peel\": false}, {\"color\": \"orange\", \"name\": \"navel-orange\", \"peel\": true}, {\"color\": \"yellow\", \"name\": \"lemon\", \"peel\": true} ]' The Where parameter in select functions is a list of tuples. e : select ( 'name' , [{ 'peel' , true }], fruits ) e : select_first ( 'name' , [{ 'color' , 'orange' }, { 'peel' , false }], fruits ) The first example above would select the names of all fruits, that have a 'peel' value of true: ['orange', 'mandarin', 'navel-orange', 'lemon'] . The second example selects the first name with 'color' orange and the 'peel' value false : 'peach' . Besides an exact match in the Where parameters, we can also use a regular expression: e : select ( 'name' , [{ 'regex' , 'name' , 'orange$' }], fruits ) Results in ['orange', 'navel-orange'] . Misc Function Description Example defined(Key) -> bool whether the given Key is defined in the current data-item undefined(Key) -> bool whether the given Key is NOT defined in the current data-item empty(Key) -> bool whether the given Key points to an empty or not defined value, empty means an empty string or an empty list or tuple empty(\"data.topic\") -> false topic_part(TopicString, PartIndex, [Seperator]) -> string extract a part from a topic string, Separator defaults to '/', the index of the first part is 1 not 0 topic_part('this/is/mytopic', 2) -> 'is' random(N) -> integer generate a random integer between 1 and N random_real(N) -> float generate a random float between 0.0 and 1.0, that gets multiplied by N crc32(String) -> string Computes the crc32 (IEEE 802.3 style) checksum for the given string. phash(Any) -> integer Portable hash function, that outputs an integer in the range 0..2^27-1 mem values are set with the mem node ls_mem(Key) -> any get the single value associated with Key from the flow-memory ls_mem_list(Key) -> any get the list value associated with Key from the flow-memory ls_mem_set(Key) -> any get the set value associated with Key from the flow-memory Conditional functions Special if function If In DFS if is not a language construct, but a function with 3 parameters. The if function\u2019s return type is the same type as its second and third arguments. if ( condition , true expression , false expression ) Returns the result of its operands depending on the value of the first argument. Examples: | eval( lambda : if ( \"field.val1\" > threshold AND \"field.val1\" != 0 , 'true' , 'false' )) . as ( 'value' ) The value of the field value in the above example will be the string true or false , depending on the condition passed as the first argument. | eval( lambda : if ( is_float ( \"data.duration_ms\" ), trunc ( \"data.duration_ms\" * 1000 ), \"data.duration_ms\" )) . as ( 'value' ) Both expressions (2nd and 3rd parameter) may also be arbitrarily complex. In this example, if the condition returns true, data.duration_ms will be mulitplied be 1000 and then truncated to an integer. If the condition returns false, just the value of data.duration_ms will be returned.","title":"Built in functions"},{"location":"dfs_script_language/built-in_functions.html#type-conversion","text":"With a few exceptions every type can be converted to every other type. Bool bool(a_value) -> true|false Integer int(value) -> integer If value is a float, then int(value) is equivalent to trunc(value) Float float(value) -> float String string(val) -> string If val is a list: String string(list) -> string Converting a list into a string with string is equivalent to list_join ( ',' , list_of_strings ( List )) Converts every entry of List to a string first and then joins the resulting list with a comma. [ 1 , 2 , 3 ] => [ '1' , '2' , '3' ] => '1,2,3'","title":"Type conversion"},{"location":"dfs_script_language/built-in_functions.html#type-checks","text":"Function is_string(Data) -> bool is_integer(Data) -> bool is_int(Data) -> bool is_float(Data) -> bool is_number(Data) -> bool is_list(Data) -> bool is_duration(Data) -> bool is_boolean(Data) -> bool is_bool(Data) -> bool","title":"Type checks"},{"location":"dfs_script_language/built-in_functions.html#time-functions","text":"Every data_point in faxe contains a field called ts , which holds a UTC timestamp in milliseconds. Function Description now() -> integer returns an utc timestamp in milliseconds dt_parse(ts, formatstring) -> integer used to parse a datetime string to the internal format, see datetime-parsing for details to_iso8601(ts) -> string converts the timestamp to an ISO8601 datetime string to_rfc3339(ts) -> string converts the timestamp to an RFC3339 datetime string from_duration(ts) -> integer converts a duration to it's millisecond equivalent millisecond(ts) -> integer milliseconds within the second [0, 999] second(ts) -> integer second within the minute [0, 59] minute(ts) -> integer minute within the hour [0, 59] hour(ts) -> integer hour within the day [0, 23] day(ts) -> integer day within the month [1, 31] day_of_week(ts) -> integer the weekday with week [1, 7] 1 is monday week(ts) -> integer isoweek-number within year [1, 53] month(ts) -> integer month within the year [1, 12] time_align(ts, duration) -> integer align the given timestamp to duration Examples: lambda : hour ( \"ts\" ) >= 8 AND hour ( \"ts\" ) < 19 The above expression evaluates to true if the hour of the day for the data point falls between 08:00 and 19:00. lambda : time_align ( \"ts\" , 3m ) Will align every timestamp (ts) to a multiple of the 3 minutes within an hour (ie: 00:00, 00:03, 00:06, ...)","title":"Time functions"},{"location":"dfs_script_language/built-in_functions.html#math-functions","text":"Function Description abs(x) -> number acos(x) -> float acosh(x) -> float asin(x) -> float asinh(x) -> float atan(x) -> float atan2(y, x) -> float atanh(x) -> float ceil(x) -> float cos(x) -> float cosh(x) -> float exp(x) -> float floor(x) -> float fmod(x, y) -> float log(x) -> float log10(x) -> float log2(x) -> float max(x, y) -> number max(list) -> number min(x, y) -> number min(list) -> number pi() -> float gives pi pow(x, y) -> float round(x) -> integer round a number to an integer round_float(x, precision) -> float round a float (x) with the given precision trunc(x) -> integer Returns an integer by truncating a number sin(x) -> float sinh(x) -> float sqrt(x) -> float tan(x) -> float tanh(x) -> float","title":"Math functions"},{"location":"dfs_script_language/built-in_functions.html#string-functions","text":"String positions start with index 0. Function Description str_at(x, pos) -> string/undefined Returns the grapheme in the position of the given utf8 string. If position is greater than string length, then it returns undefined. Negative offsets count back from the end of the string. str_capitalize(x) -> string Converts the first character in the given string to uppercase and the remaining to lowercase str_contains(x, contents) -> bool Check if string contains any of the given contents str_downcase(x) -> string Convert all characters on the given string to lowercase str_enclose(Wrapper, StringOrList) -> string or list Prepend and append Wrapper to a string or each entry of a list of strings str_ends_with(x, suffix) -> string Returns true if string ends with suffix, otherwise false. str_ends_with_any(x, suffixes) -> string Returns true if string ends with any of the suffixes given, otherwise false. str_eqi(x,y) -> bool Compares strings case insensitively str_first(x) -> string/undefined Returns the first grapheme from an utf8 string, undefined if the string is empty str_last(x) -> string/undefined Returns the last grapheme from an utf8 string, undefined if the string is empty str_length(x) -> int Returns the number of unicode graphemes in an utf8 string str_lstrip(x) -> string Returns a string where leading Unicode whitespace has been removed str_lstrip(x, char) -> string Returns a string where leading char have been removed str_pad_leading/2 str_pad_leading/3 str_pad_trailing/2 str_pad_trailing/3 str_replace(x, patt, repl) -> string Returns a new string based on subject by replacing the parts matching pattern by replacement. str_replace_leading/3 Replaces all leading occurrences of match by replacement of match in string. str_replace_trailing/3 Replaces all trailing occurrences of match by replacement of match in string. str_replace_prefix(x, match, repl) -> string Replaces prefix in string by replacement if it matches match. Returns the string untouched if there is no match. If match is an empty string (\"\"), replacement is just prepended to string. str_replace_suffix(x, match, repl) -> string Replaces suffix in string by replacement if it matches match. Returns the string untouched if there is no match. If match is an empty string (\"\"), replacement is just appended to string. str_reverse(x) -> string Reverses the given string. str_rstrip(x) -> string Returns a string where trailing Unicode whitespace has been removed str_rstrip(x, char) -> string Returns a string where trailing char have been removed str_slice(x, start, len) -> string Returns a substring starting at the offset given by the first, and a length given by the second param, if offset is negative, count back from end of string. str_split/1 str_split/2 str_split/3 str_split_at/2 str_split_by_any/2 str_split_by_any/3 str_split_by_re/2 str_split_by_re/3 str_starts_with(x, pre) -> bool Returns true if string starts with Prefix str_starts_with_any(x, prefixes) -> bool Returns true if string starts with any of the prefixes given, otherwise false. str_strip(x) -> string Returns a string where leading/trailing Unicode whitespace has been removed str_strip(x, char) -> string Returns a string where leading/trailing char have been removed str_upcase(x) -> string Convert all characters on the given string to uppercase","title":"String functions"},{"location":"dfs_script_language/built-in_functions.html#lists-and-maps-jsn","text":"Function Description Example from_json_string(String) -> faxe map or list convert a valid json string into faxe's internal data-structure from_json_string('{\"a\":\"b\"}') to_json_string(String) -> string convert faxe's internal data-structure into a json string to_json_string(\"data.struct\") size(ListOrMap) -> integer get the number of entries in a list or map size(\"data.lines\") head(List) -> any returns the head, or the first element, of list List head(\"data.lines\") nth(Integer, List) -> any returns the nth element from list List, list indices start with 1 nth(N, \"data.lines\") list_unique(List) -> any returns a new list with only unique values from List list_unique(\"data.lines\") list_join(List) -> any equivalent to list_join(',', List) list_join(\"data.lines\") list_join(Separator, List) -> any join a list of strings with a separator string list_join('-', \"data.lines\") list_of_strings(List) -> any convert every entry of a list to a string and return the list of strings list_of_strings(\"data.lines\") member(Ele, ListOrMap) -> bool check for list/set membership of a value, or when used with a map, check if Ele is a key in the map not_member(Ele, ListOrMap) -> bool map_get(Key, Map) -> any get a value from a map, equivalent to map_get(Key, Map, 'undefined') map_get(\"topic\", '{\"data/temp/val1\": \"temp\"}') map_get(Key, Map, Default) -> any get a value from a map, Default is returned, if the key is not present in map map_get(\"topic\", '{\"data/temp/val1\": \"temp\"}', 'no_match') select(Key, Jsn) -> list Select every value with the path Key from a json-array See description below. select(Key, Where, Jsn) -> list Select every value with the path Key and conditions Where from a json-array. Elements that do not meet the given conditions are filtered out. See description below. select_all(Key, Where, Jsn) -> list Select every value with the path Key and conditions Where from a json-array. The list of Where conditions are OR ed in this case, so there is a select operation for every entry in Where See description below. select_first(Key, Jsn) -> any Select the first value with the path Key from a json-array. See description below. select_first(Key, Where, Jsn) -> any Select the first value with the path Key and conditions Where from a json-array. See description below. select_any(Key, Where, Jsn) -> any Select the first value with the path Key that is found with any of the Where conditions from a json-array. See description below.","title":"Lists and Maps (jsn)"},{"location":"dfs_script_language/built-in_functions.html#select-select_first-select_any","text":"With these functions we can select values from a json-array like datastructure, meaning an array of - possibly nested - json objects. def fruits = '[ {\"color\": \"orange\", \"name\": \"orange\", \"peel\": true}, {\"color\": \"orange\", \"name\": \"mandarin\", \"peel\": true}, {\"color\": \"orange\", \"name\": \"peach\", \"peel\": false}, {\"color\": \"orange\", \"name\": \"navel-orange\", \"peel\": true}, {\"color\": \"yellow\", \"name\": \"lemon\", \"peel\": true} ]' The Where parameter in select functions is a list of tuples. e : select ( 'name' , [{ 'peel' , true }], fruits ) e : select_first ( 'name' , [{ 'color' , 'orange' }, { 'peel' , false }], fruits ) The first example above would select the names of all fruits, that have a 'peel' value of true: ['orange', 'mandarin', 'navel-orange', 'lemon'] . The second example selects the first name with 'color' orange and the 'peel' value false : 'peach' . Besides an exact match in the Where parameters, we can also use a regular expression: e : select ( 'name' , [{ 'regex' , 'name' , 'orange$' }], fruits ) Results in ['orange', 'navel-orange'] .","title":"select, select_first, select_any"},{"location":"dfs_script_language/built-in_functions.html#misc","text":"Function Description Example defined(Key) -> bool whether the given Key is defined in the current data-item undefined(Key) -> bool whether the given Key is NOT defined in the current data-item empty(Key) -> bool whether the given Key points to an empty or not defined value, empty means an empty string or an empty list or tuple empty(\"data.topic\") -> false topic_part(TopicString, PartIndex, [Seperator]) -> string extract a part from a topic string, Separator defaults to '/', the index of the first part is 1 not 0 topic_part('this/is/mytopic', 2) -> 'is' random(N) -> integer generate a random integer between 1 and N random_real(N) -> float generate a random float between 0.0 and 1.0, that gets multiplied by N crc32(String) -> string Computes the crc32 (IEEE 802.3 style) checksum for the given string. phash(Any) -> integer Portable hash function, that outputs an integer in the range 0..2^27-1 mem values are set with the mem node ls_mem(Key) -> any get the single value associated with Key from the flow-memory ls_mem_list(Key) -> any get the list value associated with Key from the flow-memory ls_mem_set(Key) -> any get the set value associated with Key from the flow-memory","title":"Misc"},{"location":"dfs_script_language/built-in_functions.html#conditional-functions","text":"","title":"Conditional functions"},{"location":"dfs_script_language/built-in_functions.html#special-if-function","text":"If In DFS if is not a language construct, but a function with 3 parameters. The if function\u2019s return type is the same type as its second and third arguments. if ( condition , true expression , false expression ) Returns the result of its operands depending on the value of the first argument. Examples: | eval( lambda : if ( \"field.val1\" > threshold AND \"field.val1\" != 0 , 'true' , 'false' )) . as ( 'value' ) The value of the field value in the above example will be the string true or false , depending on the condition passed as the first argument. | eval( lambda : if ( is_float ( \"data.duration_ms\" ), trunc ( \"data.duration_ms\" * 1000 ), \"data.duration_ms\" )) . as ( 'value' ) Both expressions (2nd and 3rd parameter) may also be arbitrarily complex. In this example, if the condition returns true, data.duration_ms will be mulitplied be 1000 and then truncated to an integer. If the condition returns false, just the value of data.duration_ms will be returned.","title":"Special if function"},{"location":"dfs_script_language/lambda_expressions.html","text":"Lambda expressions Overview DFS uses lambda expressions to define transformations on data points as well as define Boolean conditions that act as filters. Lambda expressions wrap mathematical operations, Boolean operations, internal function calls or a combination of all three. All lambda expressions in DFS begin with the lambda: keyword. | where( lambda : \"topic\" == 'ttop/grap/prec' ) In the above example \"topic\" is used to access the value of a field called topic from the current data_point and compared against the string 'ttop/grap/prec' . Note here that literal string values are declared using single quotes, while double quotes are used to access the values of tags and fields within the current data_item. Field paths As field and tag values can be deeply nested maps and lists, it is possible to use a JSON-path like syntax to reference them: Valid examples: \"averages\" \"axis.z.cur\" \"value.sub[2].data\" \"averages.emitted[5]\" Built-in functions Faxe features a lot of built-in functions, that can be used in lambda and script expressions. See here for a full list of them. Special if function If In DFS if is not a language construct, but a function with 3 parameters. The if function\u2019s return type is the same type as its second and third arguments. if ( condition , true expression , false expression ) Returns the result of its operands depending on the value of the first argument. Examples: | eval( lambda : if ( \"field.val1\" > threshold AND \"field.val1\" != 0 , 'true' , 'false' )) . as ( 'value' ) The value of the field value in the above example will be the string true or false , depending on the condition passed as the first argument. | eval( lambda : if ( is_float ( \"data.duration_ms\" ), trunc ( \"data.duration_ms\" * 1000 ), \"data.duration_ms\" )) . as ( 'value' ) Both expressions (2nd and 3rd parameter) may also be arbitrarily complex. In this example, if the condition returns true, data.duration_ms will be mulitplied be 1000 and then truncated to an integer. If the condition returns false, just the value of data.duration_ms will be returned.","title":"Lambda expressions"},{"location":"dfs_script_language/lambda_expressions.html#lambda-expressions","text":"","title":"Lambda expressions"},{"location":"dfs_script_language/lambda_expressions.html#overview","text":"DFS uses lambda expressions to define transformations on data points as well as define Boolean conditions that act as filters. Lambda expressions wrap mathematical operations, Boolean operations, internal function calls or a combination of all three. All lambda expressions in DFS begin with the lambda: keyword. | where( lambda : \"topic\" == 'ttop/grap/prec' ) In the above example \"topic\" is used to access the value of a field called topic from the current data_point and compared against the string 'ttop/grap/prec' . Note here that literal string values are declared using single quotes, while double quotes are used to access the values of tags and fields within the current data_item.","title":"Overview"},{"location":"dfs_script_language/lambda_expressions.html#field-paths","text":"As field and tag values can be deeply nested maps and lists, it is possible to use a JSON-path like syntax to reference them: Valid examples: \"averages\" \"axis.z.cur\" \"value.sub[2].data\" \"averages.emitted[5]\"","title":"Field paths"},{"location":"dfs_script_language/lambda_expressions.html#built-in-functions","text":"Faxe features a lot of built-in functions, that can be used in lambda and script expressions. See here for a full list of them.","title":"Built-in functions"},{"location":"dfs_script_language/lambda_expressions.html#special-if-function","text":"If In DFS if is not a language construct, but a function with 3 parameters. The if function\u2019s return type is the same type as its second and third arguments. if ( condition , true expression , false expression ) Returns the result of its operands depending on the value of the first argument. Examples: | eval( lambda : if ( \"field.val1\" > threshold AND \"field.val1\" != 0 , 'true' , 'false' )) . as ( 'value' ) The value of the field value in the above example will be the string true or false , depending on the condition passed as the first argument. | eval( lambda : if ( is_float ( \"data.duration_ms\" ), trunc ( \"data.duration_ms\" * 1000 ), \"data.duration_ms\" )) . as ( 'value' ) Both expressions (2nd and 3rd parameter) may also be arbitrarily complex. In this example, if the condition returns true, data.duration_ms will be mulitplied be 1000 and then truncated to an integer. If the condition returns false, just the value of data.duration_ms will be returned.","title":"Special if function"},{"location":"dfs_script_language/macros.html","text":"Macros are a way to reuse often used parts and to build more complex scripts more easy. Every task that is registered with faxe can be used as a macro-script. Note: Macros are implemented on the script-level, so faxe's internal engine does not know anything about macros. %%% MACRO called 'multiply_above_threshold' def threshold = 30 def factor = 2 | where( lambda : \"value\" > threshold ) | eval( lambda : \"value\" * factor ) . as ( 'multiple' ) Once the above dfs script is registered with faxe, we can use it as a macro: | value_emitter() . every ( 500ms ) || multiply_above_threshold () . threshold ( 2.7 ) A macro is referenced with a double pipe symbol || , followed by the name of the task which we want to use as macro. The resulting script will look like this: | value_emitter() . every ( 500ms ) def threshold = 2.7 def factor = 2 | where( lambda : \"value\" > threshold ) | eval( lambda : \"value\" * factor ) . as ( 'multiple' ) We can override every literal declaration within the macro-script by simply using them as node-parameters. Ie: here the declaration 'threshold' is overriden, we could also override 'factor'. Theoretically any number of macro-references ||dfs_script_name can be used in a single dfs-script. Furthermore every macro used in a script can itself reference any number of macros, as every macro-script is an ordinary dfs-script registered in faxe .","title":"Macros"},{"location":"dfs_script_language/node_connection.html","text":"Nodes are normally connected by their occurence in the dfs script. | node1 () | node2 () Node1 is connected to node2. The above could also be written as: def n1 = | node1 () n1 | node2 () ... which results in the same computing flow. Here we see that we can actively manipulate the connections in a flow by binding a node to a declaration with the def keyword. Whole chains of nodes ie: sub-graphs can be bound to a variable. This is called a chain-declaration . def chain1 = | node1 () | node1_1 () | node1_2 chain1 | node2 () Now node2 is connected to node1_2 With the above example we can connect another node to chain1: def chain1 = | node1 () | node1_1 () | node1_2 chain1 | node2 () chain1 | node3 () Here both nodes node2 and node3 are connected to node1_2. Note: Every use of the def keyword interrupts the auto chaining of nodes, ie: | node1 () | node2 () def n3 = | node3 () | node4 () In the above example, node3 and node4 are not connected to node2, as a consequence of using the def keyword. Instead we have 2 chains in this flow: 1. Node1 connected to node2 and 2. node3 connected node4. If we'd like to union these 2 node chains: def in1 = | node1 () | node2 () def in2 = | node3 () | node4 () in1 | union( in2 ) There are several node-types in faxe that deal with more than one input node, for example the combine node. Here the use of chain-declarations is necessary: def s1 = | node1 () | node1_1 def s2 = | node2 () | node2_1 () s1 | combine( s2 )","title":"Node connection"},{"location":"dfs_script_language/node_connection.html#note","text":"Every use of the def keyword interrupts the auto chaining of nodes, ie: | node1 () | node2 () def n3 = | node3 () | node4 () In the above example, node3 and node4 are not connected to node2, as a consequence of using the def keyword. Instead we have 2 chains in this flow: 1. Node1 connected to node2 and 2. node3 connected node4. If we'd like to union these 2 node chains: def in1 = | node1 () | node2 () def in2 = | node3 () | node4 () in1 | union( in2 ) There are several node-types in faxe that deal with more than one input node, for example the combine node. Here the use of chain-declarations is necessary: def s1 = | node1 () | node1_1 def s2 = | node2 () | node2_1 () s1 | combine( s2 )","title":"Note:"},{"location":"dfs_script_language/script_expressions.html","text":"Script expressions Script expressions look the same as lambda expressions , but start with e: instead of lambda: . There is a main difference between script expressions and lambda expressions: Script expressions are evaluated during dfs compilation. Therefore, script expressions can not use \"references\" to access fields in data-items. Example def topic = 'this/is/my/topic' %% script expression, which uses the topic definition def routing_key = e : str_replace ( topic , '/' , '.' ) The above example will resolve to the following equivalent dfs script: def topic = 'this/is/my/topic' %% script expression, which uses the topic definition def routing_key = 'this.is.my.topic' References References are used in lambda expression to access fields in data-items (data_points and data_batches). They cannot be used in script expressions, so the next example will throw an error during script compilation: def base_topic = 'this/is/my/base/' def topic = e : str_concat ( base_topic , \"data.postfix\" ) The above example will fail with the message: \"Reference(s) used in inline-expression: data.postfix\" Note: We use double quotes to reference a field in the current data_item. More examples def new_id = 3 def def_inline = e : string ( 3 + 5 + sqrt ( new_id )) def i_string = 'this is my string' def def_inline_string = e : str_replace ( i_string , 'i' , 'a' ) def batch_size = 50 | amqp_consume() . bindings ( 'this.is.my.binding.#' ) %% use of script expression for node option, %% we have to round to an integer with the `round` function . prefetch ( e : round ( batch_size * 1.25 ) ) . exchange ( 'x_xchange' ) . queue ( 'q_test' ) | batch( batch_size ) . timeout ( 10s ) Script expression can be used for node parameters (except where a lambda expression is required of course), as long as the resulting value is of the required data-type.","title":"Script expressions"},{"location":"dfs_script_language/script_expressions.html#script-expressions","text":"Script expressions look the same as lambda expressions , but start with e: instead of lambda: . There is a main difference between script expressions and lambda expressions: Script expressions are evaluated during dfs compilation. Therefore, script expressions can not use \"references\" to access fields in data-items.","title":"Script expressions"},{"location":"dfs_script_language/script_expressions.html#example","text":"def topic = 'this/is/my/topic' %% script expression, which uses the topic definition def routing_key = e : str_replace ( topic , '/' , '.' ) The above example will resolve to the following equivalent dfs script: def topic = 'this/is/my/topic' %% script expression, which uses the topic definition def routing_key = 'this.is.my.topic'","title":"Example"},{"location":"dfs_script_language/script_expressions.html#references","text":"References are used in lambda expression to access fields in data-items (data_points and data_batches). They cannot be used in script expressions, so the next example will throw an error during script compilation: def base_topic = 'this/is/my/base/' def topic = e : str_concat ( base_topic , \"data.postfix\" ) The above example will fail with the message: \"Reference(s) used in inline-expression: data.postfix\" Note: We use double quotes to reference a field in the current data_item.","title":"References"},{"location":"dfs_script_language/script_expressions.html#more-examples","text":"def new_id = 3 def def_inline = e : string ( 3 + 5 + sqrt ( new_id )) def i_string = 'this is my string' def def_inline_string = e : str_replace ( i_string , 'i' , 'a' ) def batch_size = 50 | amqp_consume() . bindings ( 'this.is.my.binding.#' ) %% use of script expression for node option, %% we have to round to an integer with the `round` function . prefetch ( e : round ( batch_size * 1.25 ) ) . exchange ( 'x_xchange' ) . queue ( 'q_test' ) | batch( batch_size ) . timeout ( 10s ) Script expression can be used for node parameters (except where a lambda expression is required of course), as long as the resulting value is of the required data-type.","title":"More examples"},{"location":"dfs_script_language/tips_and_best_pratices.html","text":"DFS Script Language tips and best pratices Variables Ok, they are unmutable really, so we call them declarations. def stream_id = 'ee29e1f8552e4a2561329fec59688e60' When using declarations in DFS script you should stick to one type of naming schema. Be it CamelCase naming or all lowercase with underscores like in the example above. Lambda expressions As a rule we say: Avoid lambda expressions, if you can achieve your goal without them. Of course you cannot do everything without lambda expression, as many nodes require them to work.","title":"DFS Script Language tips and best pratices"},{"location":"dfs_script_language/tips_and_best_pratices.html#dfs-script-language-tips-and-best-pratices","text":"","title":"DFS Script Language tips and best pratices"},{"location":"dfs_script_language/tips_and_best_pratices.html#variables","text":"Ok, they are unmutable really, so we call them declarations. def stream_id = 'ee29e1f8552e4a2561329fec59688e60' When using declarations in DFS script you should stick to one type of naming schema. Be it CamelCase naming or all lowercase with underscores like in the example above.","title":"Variables"},{"location":"dfs_script_language/tips_and_best_pratices.html#lambda-expressions","text":"As a rule we say: Avoid lambda expressions, if you can achieve your goal without them. Of course you cannot do everything without lambda expression, as many nodes require them to work.","title":"Lambda expressions"},{"location":"dfs_script_language/tips_and_best_pratices.html#_1","text":"","title":""},{"location":"nodes/index.html","text":"Faxe nodes Parameters Faxe nodes can have 2 types of parameters : Node parameters provided to the node declaration function % the level parameter is given to the node declaration function | debug( 'notice' ) Option parameters provided to an option call % the level parameter is given as an extra option function | debug() . level ( 'notice' ) Some parameters are required and others are optional. Every parameter with no default value is mandatory ! The following is a list of all possible parameter types faxe supports based on the basic data-types: Name Description Example is_set Special parameter type that evaluates to true if called (even with no value) .use_ssl() number Integer or float value 324 or 4.3424325 integer Integer value float Floating point value double Floating point value string String value .topic('home/alex/garage') binary atom used internally only list any kind of list lambda a lambda expression bool number_list a list of numbers .values(3, 44, 34.5) integer_list a list of integers .ints(2, 3, 4, 5) float_list a list of floats .floats(43.4, 12.2, 545.009832) string_list a list of strings .strings('alex1', 'alex2', 'flo', 'markus') binary_list atom_list internally only lambda_list a list of lambda expressions .functions(lambda: \"val\" * 2, lambda: \"val\" * 3, lambda: \"val\" / 4) What is important to note: If a node requires a _list type for any parameter, we just provide 1 or more of the same data-type separated be commas. For example the eval node requires the lambdas parameter to be of type lambda_list , the following calls would be valid: | eval( lambda : str_concat ( \"strval\" , postfix )) | eval( lambda : str_starts_with ( \"strval\" , 'pre' ), lambda : 3 * ( \"val1\" + \"val2\" )) | eval( lambda : sqrt ( \"base\" ) + const , lambda : if ( hour ( \"ts\" ) > 18 AND day_of_week ( \"ts\" ) < 6 , 'late_for_work' , 'ok' ), lambda : abs ( \"ts\" - \"ts_previous\" ) )","title":"Faxe nodes"},{"location":"nodes/index.html#faxe-nodes","text":"","title":"Faxe nodes"},{"location":"nodes/index.html#parameters","text":"Faxe nodes can have 2 types of parameters : Node parameters provided to the node declaration function % the level parameter is given to the node declaration function | debug( 'notice' ) Option parameters provided to an option call % the level parameter is given as an extra option function | debug() . level ( 'notice' ) Some parameters are required and others are optional. Every parameter with no default value is mandatory ! The following is a list of all possible parameter types faxe supports based on the basic data-types: Name Description Example is_set Special parameter type that evaluates to true if called (even with no value) .use_ssl() number Integer or float value 324 or 4.3424325 integer Integer value float Floating point value double Floating point value string String value .topic('home/alex/garage') binary atom used internally only list any kind of list lambda a lambda expression bool number_list a list of numbers .values(3, 44, 34.5) integer_list a list of integers .ints(2, 3, 4, 5) float_list a list of floats .floats(43.4, 12.2, 545.009832) string_list a list of strings .strings('alex1', 'alex2', 'flo', 'markus') binary_list atom_list internally only lambda_list a list of lambda expressions .functions(lambda: \"val\" * 2, lambda: \"val\" * 3, lambda: \"val\" / 4) What is important to note: If a node requires a _list type for any parameter, we just provide 1 or more of the same data-type separated be commas. For example the eval node requires the lambdas parameter to be of type lambda_list , the following calls would be valid: | eval( lambda : str_concat ( \"strval\" , postfix )) | eval( lambda : str_starts_with ( \"strval\" , 'pre' ), lambda : 3 * ( \"val1\" + \"val2\" )) | eval( lambda : sqrt ( \"base\" ) + const , lambda : if ( hour ( \"ts\" ) > 18 AND day_of_week ( \"ts\" ) < 6 , 'late_for_work' , 'ok' ), lambda : abs ( \"ts\" - \"ts_previous\" ) )","title":"Parameters"},{"location":"nodes/aggregate.html","text":"The aggregate node The aggregate node lets you compute statistical functions on data_batches. This node can compute multiple statistic functions on multiple fields. See nodes under /nodes/statistics for details (Not all of these are available here). This node can only take data_batch items, so in most cases you will need some kind of batch or window node to collect batches. The aggregate node produces a new stream of data consisting of data_point items. Example | value_emitter() . every ( 1s ) . type ( 'point' ) | win_time() . every ( 15s ) | aggregate () . fields ( 'val' , 'val' , 'val' , 'val' ) . functions ( 'variance' , 'sum' , 'avg' , 'count_distinct' ) . as ( 'variance' , 'sum' , 'average' , 'count_distinct' ) | debug() Parameters Parameter Description Default fields( string_list ) names of the fields used for each computation functions( string_list ) list of function names (see 'Functions' table below for possible values) as( string_list ) names for the fields for output values defaults to the name of the compute function keep( string_list ) fields that should be kept from the input data-items [] Functions Name Description Note sum computes the sum of all values numerical values only avg computes the average of all values numerical values only min computes the minimum value max computes the maximum value mean computes the mean value numerical values only median computes the median value numerical values only first outputs the first (oldest) value last outputs the last (newest) value count outputs the total number of values count_distinct outputs the number of unique values count_change outputs the number of value changes for the given field range computes the range between minimum and maximum variance computes the variance numerical values only stddev computes the standard deviation of the values numerical values only skew computes the skew of all values numerical values only","title":"Aggregate"},{"location":"nodes/aggregate.html#the-aggregate-node","text":"The aggregate node lets you compute statistical functions on data_batches. This node can compute multiple statistic functions on multiple fields. See nodes under /nodes/statistics for details (Not all of these are available here). This node can only take data_batch items, so in most cases you will need some kind of batch or window node to collect batches. The aggregate node produces a new stream of data consisting of data_point items.","title":"The aggregate node"},{"location":"nodes/aggregate.html#example","text":"| value_emitter() . every ( 1s ) . type ( 'point' ) | win_time() . every ( 15s ) | aggregate () . fields ( 'val' , 'val' , 'val' , 'val' ) . functions ( 'variance' , 'sum' , 'avg' , 'count_distinct' ) . as ( 'variance' , 'sum' , 'average' , 'count_distinct' ) | debug()","title":"Example"},{"location":"nodes/aggregate.html#parameters","text":"Parameter Description Default fields( string_list ) names of the fields used for each computation functions( string_list ) list of function names (see 'Functions' table below for possible values) as( string_list ) names for the fields for output values defaults to the name of the compute function keep( string_list ) fields that should be kept from the input data-items []","title":"Parameters"},{"location":"nodes/aggregate.html#functions","text":"Name Description Note sum computes the sum of all values numerical values only avg computes the average of all values numerical values only min computes the minimum value max computes the maximum value mean computes the mean value numerical values only median computes the median value numerical values only first outputs the first (oldest) value last outputs the last (newest) value count outputs the total number of values count_distinct outputs the number of unique values count_change outputs the number of value changes for the given field range computes the range between minimum and maximum variance computes the variance numerical values only stddev computes the standard deviation of the values numerical values only skew computes the skew of all values numerical values only","title":"Functions"},{"location":"nodes/collect.html","text":"The collect node Experimental . Since 0.15.2 The collect node will maintain a set of data-points based on some criteria given as lambda-expressions. It will normally output a data_batch item regularily (when emit_every is given) or based on every incoming item. The internal collection is a key-value set with unqiue values for the keys, taken from the key_fields . [{Key, DataPoint}] Adding, updating and removing a value With every incoming data-item the node will first check, if there is already an item with the same key-field value in the collection. If this is not the case, the node will evaluate the add function, if given. Data_points that do not have (any of) the key-field(s) present, will be ignored. An item will be added to the collection if the key is new to the collection and if the add -function returns true is not given If there is already an item with the same value(s) for the key-field(s), the node would check, if there is an update function and evaluate it and if no update happend, it will try to evaluate the remove function. If an update happened, the node will skip evaluating the remove function. If no remove function is given, data-items will not be removed, but only evicted by the max_age option. Update and remove expression In the update or remove lambda-expressions, the datapoint, that is already in the collection, can be used, for comparing for example. That is to say: When the update or remove function is evaluated, it gets injected the data-point for the same key-field value, that is found in the collection. There is a root-object used for the fields in this datapoint: __state . What this means is, that you can do something like this in the update expression (see also Example 3 below) | collect () . key_fields ( 'data.id' ) %% comparing the current value of 'val' to the value from the data-point currently in the collection %% here we use the value from the internal buffer that is found in the '__state' object. . update ( lambda : \"data.val\" > \"__state.data.val\" ) Output When no emit_every is given, the node will output data with every incoming data-item. With emit_unchanged set to false, output will only happen after processing a data-item that has changed the internal set. data_batch items are processed as a whole first and then may trigger an emit operation. Since 0.19.4 : The output batch can be conflated into a single data_point item, when the merge option is set to true. Note: Produced data may become very large, if the value of key_fields is ever-changing, so that the node will cache a lot of data and therefore may use a lot of memory, be aware of that ! Example 1 | collect () . key_field ( 'data.code' ) . max_age ( 2m ) Collect by the field data.code keeping every data-item for 2 minutes. No update or remove will occur otherwise. Example 2 | collect () . key_field ( 'data.code' ) . update ( lambda : \"data.mode\" == 1 ) . remove ( lambda : str_length ( \"data.message\" ) > 7 ) Collect by the field data.code , update an item when the data.mode field is 1. Items get removed, if they have a value for data.message , that is more than 7 chars long. Example 3 %% input data -> | json_emitter() . every ( 500ms ) . json ( '{\"id\": 224, \"name\" : \"twotwofour\", \"val\": 12, \"mode\": 1}' , '{\"id\": 112, \"name\" : \"oneonetow\", \"val\": 11, \"mode\": 1}' , '{\"id\": 358, \"name\" : \"threefiveeigth\", \"val\": 7, \"mode\": 1}' , '{\"id\": 102, \"name\" : \"onezerotwo\", \"val\": 12, \"mode\": 1}' , '{\"id\": 224, \"name\" : \"twotwofour\", \"val\": 13, \"mode\": 1}' , '{\"id\": 112, \"name\" : \"oneonetow\", \"val\": 9, \"mode\": 1}' , '{\"id\": 358, \"name\" : \"threefiveeigth\", \"val\": 4, \"mode\": 1}' , '{\"id\": 102, \"name\" : \"onezerotow\", \"val\": 2, \"mode\": 1}' , '{\"id\": 224, \"name\" : \"twotwofour\", \"val\": 3, \"mode\": 1}' , '{\"id\": 112, \"name\" : \"oneonetow\", \"val\": 15, \"mode\": 1}' , '{\"id\": 358, \"name\" : \"threefiveeigth\", \"val\": 22, \"mode\": 1}' , '{\"id\": 102, \"name\" : \"onezerotwo\", \"val\": 9, \"mode\": 1}' , '{\"id\": 224, \"name\" : \"twotwofour\", \"val\": 0, \"mode\": 0}' , '{\"id\": 112, \"name\" : \"oneonetow\", \"val\": 0, \"mode\": 0}' , '{\"id\": 358, \"name\" : \"threefiveeigth\", \"val\": 0, \"mode\": 0}' , '{\"id\": 102, \"name\" : \"onezerotow\", \"val\": 0, \"mode\": 0}' ) . as ( 'data' ) . select ( 'rand' ) | collect () . key_fields ( 'data.id' ) .keep( 'data.id' , 'data.val' ) . update ( lambda : \"data.val\" > \"__state.data.val\" ) . remove ( lambda : \"data.mode\" == 0 ) Collect the max value of data.val for every different data.id value. Note the use of the '__state' prefix for previous value, that can be used in the update expression. Example 4 %% input data -> | json_emitter() . every ( 500ms ) . json ( '{\"code\" : {\"id\": 224, \"name\" : \"224\"}, \"message\": \" a test\", \"mode\": 1}' , '{\"code\" : {\"id\": 334, \"name\" : \"334\"}, \"message\": \" another test\", \"mode\": 1}' , '{\"code\" : {\"id\": 114, \"name\" : \"114\"}, \"message\": \" another test\", \"mode\": 2}' , '{\"code\" : {\"id\": 443, \"name\" : \"443\"}, \"message\": \" another test\", \"mode\": 1}' , '{\"code\" : {\"id\": 111, \"name\" : \"111\"}, \"message\": \" another test\", \"mode\": 1}' , '{\"code\" : {\"id\": 443, \"name\" : \"443\"}, \"message\": \" another test\", \"mode\": 0}' , '{\"code\" : {\"id\": 224, \"name\" : \"224\"}, \"message\": \" another test\", \"mode\": 0}' , '{\"code\" : {\"id\": 111, \"name\" : \"111\"}, \"message\": \" another test\", \"mode\": 0}' , '{\"code\" : {\"id\": 334, \"name\" : \"334\"}, \"message\": \" another test\", \"mode\": 0}' , '{\"code\" : {\"id\": 551, \"name\" : \"551\"}, \"message\": \" another test\", \"mode\": 2}' , '{\"code\" : {\"id\": 551, \"name\" : \"551\"}, \"message\": \" another test\", \"mode\": 0}' ) . as ( 'data' ) %% collect 2 fields ('data.code' and 'data.message') %% with the key-field 'data.code'. %% this node will output a data_batch item with a list of data-points %% where the original timestamp and meta-data is preserved and %% containing the fields mentioned before every 10 secondes | collect () %% the collect node will build an internal buffer %% with the value of the 'key_field' as index . key_field ( 'data.code' ) %% criterion for adding a data-point to the internal collection buffer . add ( lambda : \"data.mode\" > 0 ) %% criterion for removal of values . remove ( lambda : \"data.mode\" == 0 ) %% we keep these fields in the resulting data_batch .keep( 'data.code' , 'data.message' ) . emit_every ( 10s ) %% make sense of the data-collection in counting the 'data.code' values | aggregate () . fields ( 'data.code' ) . functions ( 'count' ) . as ( 'code_count' ) | debug() Parameters Parameter Description Default key_fields( string ) The value of the key-field will be used as an index for the collection. add( lambda ) Criterion for adding an incoming point to the collection, must return a boolean value. undefined remove( lambda ) Criterion for removing a point from the collection, must return a boolen value. undefined update( lambda ) Criterion for updating a data_point in the collection, must return a boolen value. If not given, no updating will occur. Reference the current value with __state undefined update_mode( string ) replace , merge , merge_reverse . When updating, an existing point in the collection can be replaced by or merged with the new one. With merge_reverse data_point positions for the merge operation get flipped, so that the existing point is merged onto the new data_point. 'replace' tag_added( boolean ) When set to true, emitted data_points that have been added since the last emit will have a field called added with the value of tag_value false tag_value( any ) Value to be use for tag fields ( added , removed ) 1 include_removed( boolean ) When set to true, data_points that would normally be removed from the collection will get a field called removed with the value of tag_value and are included in the next data-batch emit false keep( string_list ) If given, these field will be kept from every data-point, if not given, the whole item will be kept. undefined keep_as( string_list ) Rename fields that are kept (must have the same number of entries as keep ). undefined emit_unchanged ( boolean ) When set to false, processing of a data-item that does not result in a change to the collection, will not trigger an output of data. true emit_every ( duration ) Interval at which to emit the current collection as a data_batch item. If not given, every data-item (point or batch) will trigger an output (based on the value of emit_unchanged ). undefined max_age( duration ) Maximum age for any data-item in the collection, before it gets removed. Reference time for item age is the time the item entered the collection. 3h merge( boolean ) Whether to condense the output data_batch into a single data_point item by merging them. false","title":"Collect"},{"location":"nodes/collect.html#the-collect-node","text":"Experimental . Since 0.15.2 The collect node will maintain a set of data-points based on some criteria given as lambda-expressions. It will normally output a data_batch item regularily (when emit_every is given) or based on every incoming item. The internal collection is a key-value set with unqiue values for the keys, taken from the key_fields . [{Key, DataPoint}]","title":"The collect node"},{"location":"nodes/collect.html#adding-updating-and-removing-a-value","text":"With every incoming data-item the node will first check, if there is already an item with the same key-field value in the collection. If this is not the case, the node will evaluate the add function, if given. Data_points that do not have (any of) the key-field(s) present, will be ignored. An item will be added to the collection if the key is new to the collection and if the add -function returns true is not given If there is already an item with the same value(s) for the key-field(s), the node would check, if there is an update function and evaluate it and if no update happend, it will try to evaluate the remove function. If an update happened, the node will skip evaluating the remove function. If no remove function is given, data-items will not be removed, but only evicted by the max_age option.","title":"Adding, updating and removing a value"},{"location":"nodes/collect.html#update-and-remove-expression","text":"In the update or remove lambda-expressions, the datapoint, that is already in the collection, can be used, for comparing for example. That is to say: When the update or remove function is evaluated, it gets injected the data-point for the same key-field value, that is found in the collection. There is a root-object used for the fields in this datapoint: __state . What this means is, that you can do something like this in the update expression (see also Example 3 below) | collect () . key_fields ( 'data.id' ) %% comparing the current value of 'val' to the value from the data-point currently in the collection %% here we use the value from the internal buffer that is found in the '__state' object. . update ( lambda : \"data.val\" > \"__state.data.val\" )","title":"Update and remove expression"},{"location":"nodes/collect.html#output","text":"When no emit_every is given, the node will output data with every incoming data-item. With emit_unchanged set to false, output will only happen after processing a data-item that has changed the internal set. data_batch items are processed as a whole first and then may trigger an emit operation. Since 0.19.4 : The output batch can be conflated into a single data_point item, when the merge option is set to true. Note: Produced data may become very large, if the value of key_fields is ever-changing, so that the node will cache a lot of data and therefore may use a lot of memory, be aware of that !","title":"Output"},{"location":"nodes/collect.html#example-1","text":"| collect () . key_field ( 'data.code' ) . max_age ( 2m ) Collect by the field data.code keeping every data-item for 2 minutes. No update or remove will occur otherwise.","title":"Example 1"},{"location":"nodes/collect.html#example-2","text":"| collect () . key_field ( 'data.code' ) . update ( lambda : \"data.mode\" == 1 ) . remove ( lambda : str_length ( \"data.message\" ) > 7 ) Collect by the field data.code , update an item when the data.mode field is 1. Items get removed, if they have a value for data.message , that is more than 7 chars long.","title":"Example 2"},{"location":"nodes/collect.html#example-3","text":"%% input data -> | json_emitter() . every ( 500ms ) . json ( '{\"id\": 224, \"name\" : \"twotwofour\", \"val\": 12, \"mode\": 1}' , '{\"id\": 112, \"name\" : \"oneonetow\", \"val\": 11, \"mode\": 1}' , '{\"id\": 358, \"name\" : \"threefiveeigth\", \"val\": 7, \"mode\": 1}' , '{\"id\": 102, \"name\" : \"onezerotwo\", \"val\": 12, \"mode\": 1}' , '{\"id\": 224, \"name\" : \"twotwofour\", \"val\": 13, \"mode\": 1}' , '{\"id\": 112, \"name\" : \"oneonetow\", \"val\": 9, \"mode\": 1}' , '{\"id\": 358, \"name\" : \"threefiveeigth\", \"val\": 4, \"mode\": 1}' , '{\"id\": 102, \"name\" : \"onezerotow\", \"val\": 2, \"mode\": 1}' , '{\"id\": 224, \"name\" : \"twotwofour\", \"val\": 3, \"mode\": 1}' , '{\"id\": 112, \"name\" : \"oneonetow\", \"val\": 15, \"mode\": 1}' , '{\"id\": 358, \"name\" : \"threefiveeigth\", \"val\": 22, \"mode\": 1}' , '{\"id\": 102, \"name\" : \"onezerotwo\", \"val\": 9, \"mode\": 1}' , '{\"id\": 224, \"name\" : \"twotwofour\", \"val\": 0, \"mode\": 0}' , '{\"id\": 112, \"name\" : \"oneonetow\", \"val\": 0, \"mode\": 0}' , '{\"id\": 358, \"name\" : \"threefiveeigth\", \"val\": 0, \"mode\": 0}' , '{\"id\": 102, \"name\" : \"onezerotow\", \"val\": 0, \"mode\": 0}' ) . as ( 'data' ) . select ( 'rand' ) | collect () . key_fields ( 'data.id' ) .keep( 'data.id' , 'data.val' ) . update ( lambda : \"data.val\" > \"__state.data.val\" ) . remove ( lambda : \"data.mode\" == 0 ) Collect the max value of data.val for every different data.id value. Note the use of the '__state' prefix for previous value, that can be used in the update expression.","title":"Example 3"},{"location":"nodes/collect.html#example-4","text":"%% input data -> | json_emitter() . every ( 500ms ) . json ( '{\"code\" : {\"id\": 224, \"name\" : \"224\"}, \"message\": \" a test\", \"mode\": 1}' , '{\"code\" : {\"id\": 334, \"name\" : \"334\"}, \"message\": \" another test\", \"mode\": 1}' , '{\"code\" : {\"id\": 114, \"name\" : \"114\"}, \"message\": \" another test\", \"mode\": 2}' , '{\"code\" : {\"id\": 443, \"name\" : \"443\"}, \"message\": \" another test\", \"mode\": 1}' , '{\"code\" : {\"id\": 111, \"name\" : \"111\"}, \"message\": \" another test\", \"mode\": 1}' , '{\"code\" : {\"id\": 443, \"name\" : \"443\"}, \"message\": \" another test\", \"mode\": 0}' , '{\"code\" : {\"id\": 224, \"name\" : \"224\"}, \"message\": \" another test\", \"mode\": 0}' , '{\"code\" : {\"id\": 111, \"name\" : \"111\"}, \"message\": \" another test\", \"mode\": 0}' , '{\"code\" : {\"id\": 334, \"name\" : \"334\"}, \"message\": \" another test\", \"mode\": 0}' , '{\"code\" : {\"id\": 551, \"name\" : \"551\"}, \"message\": \" another test\", \"mode\": 2}' , '{\"code\" : {\"id\": 551, \"name\" : \"551\"}, \"message\": \" another test\", \"mode\": 0}' ) . as ( 'data' ) %% collect 2 fields ('data.code' and 'data.message') %% with the key-field 'data.code'. %% this node will output a data_batch item with a list of data-points %% where the original timestamp and meta-data is preserved and %% containing the fields mentioned before every 10 secondes | collect () %% the collect node will build an internal buffer %% with the value of the 'key_field' as index . key_field ( 'data.code' ) %% criterion for adding a data-point to the internal collection buffer . add ( lambda : \"data.mode\" > 0 ) %% criterion for removal of values . remove ( lambda : \"data.mode\" == 0 ) %% we keep these fields in the resulting data_batch .keep( 'data.code' , 'data.message' ) . emit_every ( 10s ) %% make sense of the data-collection in counting the 'data.code' values | aggregate () . fields ( 'data.code' ) . functions ( 'count' ) . as ( 'code_count' ) | debug()","title":"Example 4"},{"location":"nodes/collect.html#parameters","text":"Parameter Description Default key_fields( string ) The value of the key-field will be used as an index for the collection. add( lambda ) Criterion for adding an incoming point to the collection, must return a boolean value. undefined remove( lambda ) Criterion for removing a point from the collection, must return a boolen value. undefined update( lambda ) Criterion for updating a data_point in the collection, must return a boolen value. If not given, no updating will occur. Reference the current value with __state undefined update_mode( string ) replace , merge , merge_reverse . When updating, an existing point in the collection can be replaced by or merged with the new one. With merge_reverse data_point positions for the merge operation get flipped, so that the existing point is merged onto the new data_point. 'replace' tag_added( boolean ) When set to true, emitted data_points that have been added since the last emit will have a field called added with the value of tag_value false tag_value( any ) Value to be use for tag fields ( added , removed ) 1 include_removed( boolean ) When set to true, data_points that would normally be removed from the collection will get a field called removed with the value of tag_value and are included in the next data-batch emit false keep( string_list ) If given, these field will be kept from every data-point, if not given, the whole item will be kept. undefined keep_as( string_list ) Rename fields that are kept (must have the same number of entries as keep ). undefined emit_unchanged ( boolean ) When set to false, processing of a data-item that does not result in a change to the collection, will not trigger an output of data. true emit_every ( duration ) Interval at which to emit the current collection as a data_batch item. If not given, every data-item (point or batch) will trigger an output (based on the value of emit_unchanged ). undefined max_age( duration ) Maximum age for any data-item in the collection, before it gets removed. Reference time for item age is the time the item entered the collection. 3h merge( boolean ) Whether to condense the output data_batch into a single data_point item by merging them. false","title":"Parameters"},{"location":"nodes/email.html","text":"The email node Send an email to one or more recipients Note: This node is a sink node and does not output any flow-data, therefore any node connected to this node will not get any data. Example | email() . to ( 'name@email.com' , 'another@email.com' ) . subject ( 'Alert #ex3 EnergyData' ) . body ( ' No data since {{\"datetime\"}} on topic ''home/garage/energy'', last value was {{\"val\"}}. ' ) Sends an email with the subject 'Alert #ex3 EnergyData' to 2 recipients. The body will be rendered into an html template (see parameters). Body is a text_template parameter with two template-values: datetime and val , these two fields must be present in the data_point last received in the email node. If a field used in a text_template is not found in the current data_point, the string 'undefined' will be used. Parameters Parameter Description Default to( string_list ) the recipient email addresses subject( string ) body( text_template ) body_field( string ) field_path used to get the body string template ( string ) html email template to use from config file from_address ( string ) from config file smtp_relay( string ) from config file smtp_user ( string ) from config file smtp_pass ( string ) from config file body or body_field must be provided.","title":"Email"},{"location":"nodes/email.html#the-email-node","text":"Send an email to one or more recipients Note: This node is a sink node and does not output any flow-data, therefore any node connected to this node will not get any data.","title":"The email node"},{"location":"nodes/email.html#example","text":"| email() . to ( 'name@email.com' , 'another@email.com' ) . subject ( 'Alert #ex3 EnergyData' ) . body ( ' No data since {{\"datetime\"}} on topic ''home/garage/energy'', last value was {{\"val\"}}. ' ) Sends an email with the subject 'Alert #ex3 EnergyData' to 2 recipients. The body will be rendered into an html template (see parameters). Body is a text_template parameter with two template-values: datetime and val , these two fields must be present in the data_point last received in the email node. If a field used in a text_template is not found in the current data_point, the string 'undefined' will be used.","title":"Example"},{"location":"nodes/email.html#parameters","text":"Parameter Description Default to( string_list ) the recipient email addresses subject( string ) body( text_template ) body_field( string ) field_path used to get the body string template ( string ) html email template to use from config file from_address ( string ) from config file smtp_relay( string ) from config file smtp_user ( string ) from config file smtp_pass ( string ) from config file body or body_field must be provided.","title":"Parameters"},{"location":"nodes/mem.html","text":"The mem node Flow wide value storage. mem values are available to any other node (in lambda expressions) within a flow. There are 3 types of memories: 'single' holds a single value 'list' holds a list of values, value order is preserved within the list 'set' holds a list of values, where values have no duplicates Here value can be any valid datatype that is supported in faxe, from a single scalar to a nested map and/or list. The values will be held in a non persistent ets term storage. Example 1 | mem() . type ( 'set' ) . field ( 'topic' ) . key ( 'topics_seen' ) Holds a set of values from the field named topic . The set of values is available in lambda expression (within the same flow) with the key topics_seen . The above set can be used in lambda expressions with the functions: ls_mem , ls_mem_list , ls_mem_set . | where( lambda : member ( \"topic\" , ls_mem_set ( 'topics_seen' )) ) This will filter out all points that have a topic field, which has already been stored in the mem set. Thus the where node will only output points with a unique topic value. --------------------------------------------------------------------------------------------------- Example 2 def default_map = '{\"key1\":\"topic/1/new\", \"key2\":\"topic/2/new\", \"key3\":\"topic/3/new\"}' | mem() . key ( 'topic_lookup' ) .default( default_map ) . default_json () In the above example the mem node has no field parameter, but it is prepopulated with a json structure. The mem node is used as a lookup table here. The default value will stay in the storage as long as the node is running. With no field parameter given, a data_item coming in to the node will not overwrite the stored value. The internal representation of the given json object is a map in faxe. The stored map could then be use in a lambda expression: | eval( lambda : map_get ( \"some_field_name\" , ls_mem ( 'topic_lookup' ))). as ( 'topic' ) So based on the value of the field \"some_field_name\", the field topic will get the value of the corresponding map-key stored in the mem node. For a list of lambda_library functions see lambda_functions . Parameters Parameter Description Default field( string ) field-path undefined key( string ) name of the value storage type( string ) Type of mem storage, one of 'single', 'list' or 'set' 'single' default( string | number ) Prefill the storage with this value undefined default_json( is_set) When set, the default value will be interpreted as a json string false (not set) At least one of the parameters field and/or default has to be defined, otherwise this node will not be able to do anything useful.","title":"Mem"},{"location":"nodes/mem.html#the-mem-node","text":"Flow wide value storage. mem values are available to any other node (in lambda expressions) within a flow. There are 3 types of memories: 'single' holds a single value 'list' holds a list of values, value order is preserved within the list 'set' holds a list of values, where values have no duplicates Here value can be any valid datatype that is supported in faxe, from a single scalar to a nested map and/or list. The values will be held in a non persistent ets term storage.","title":"The mem node"},{"location":"nodes/mem.html#example-1","text":"| mem() . type ( 'set' ) . field ( 'topic' ) . key ( 'topics_seen' ) Holds a set of values from the field named topic . The set of values is available in lambda expression (within the same flow) with the key topics_seen . The above set can be used in lambda expressions with the functions: ls_mem , ls_mem_list , ls_mem_set . | where( lambda : member ( \"topic\" , ls_mem_set ( 'topics_seen' )) ) This will filter out all points that have a topic field, which has already been stored in the mem set. Thus the where node will only output points with a unique topic value.","title":"Example 1"},{"location":"nodes/mem.html#-","text":"","title":"---------------------------------------------------------------------------------------------------"},{"location":"nodes/mem.html#example-2","text":"def default_map = '{\"key1\":\"topic/1/new\", \"key2\":\"topic/2/new\", \"key3\":\"topic/3/new\"}' | mem() . key ( 'topic_lookup' ) .default( default_map ) . default_json () In the above example the mem node has no field parameter, but it is prepopulated with a json structure. The mem node is used as a lookup table here. The default value will stay in the storage as long as the node is running. With no field parameter given, a data_item coming in to the node will not overwrite the stored value. The internal representation of the given json object is a map in faxe. The stored map could then be use in a lambda expression: | eval( lambda : map_get ( \"some_field_name\" , ls_mem ( 'topic_lookup' ))). as ( 'topic' ) So based on the value of the field \"some_field_name\", the field topic will get the value of the corresponding map-key stored in the mem node. For a list of lambda_library functions see lambda_functions .","title":"Example 2"},{"location":"nodes/mem.html#parameters","text":"Parameter Description Default field( string ) field-path undefined key( string ) name of the value storage type( string ) Type of mem storage, one of 'single', 'list' or 'set' 'single' default( string | number ) Prefill the storage with this value undefined default_json( is_set) When set, the default value will be interpreted as a json string false (not set) At least one of the parameters field and/or default has to be defined, otherwise this node will not be able to do anything useful.","title":"Parameters"},{"location":"nodes/python.html","text":"The python node deprecated since vsn 1.0.0, see custom_nodes . Rules for python callback classes: Callback class must be in a module with the lowercase name of the class ie: module: \"double\", class: \"Double\" python callback class must be a subclass of the class Faxe from module faxe 'abstract' methods to implement are (note: they are all optional ): options() -> return a list of tuples // static init(self, args ) -> gets the object and a dict with args from options() handle_point(self, point_data) -> point_data is a dict handle_batch(self, batch_data ) -> batch_data is a list of dicts (points) the callbacks need not return anything except for the options method to emit data the method self.emit(data) has to be used, where data is a dict or a list of dicts All fields of data-item going in and out of a custom python node are placed under the root-object data . A custom python node is used with an @ as node sign instead of | in dfs! @ my_custom_python_node () Parameters Parameters can be freely defined by the python callback class via the static options() method (See example below). Note that parameter definition must be in python's bytes type. Example Callback The example python callback class below defined 2 Parameters: field must be a string and has no default value (so it must be given) as must be a string and has the default value 'double' from faxe import Faxe class Double (Faxe): @staticmethod def options (): \"\"\" overwrite this method to request options you would like to use return value is a list of tuples: (option_name, option_type, (optional: default type)) a two tuple: (b\"foo\", b\"string\") with no default value is mandatory in the dfs script a three tuple: (b\"foo\", b\"string\", b\"mystring\") may be overwritten in a dfs script :return: list of tuples \"\"\" opts = [ ( b'field' , b'string' ), ( b'as' , b'string' , b'double' ) ] return opts def init (self, args): \"\"\" will be called on startup with args requested with options() :param args: dict \"\"\" self . fieldname = args[ b'field' ] self . asfieldname = args[ b'as' ] print( \"my args: \" , args) def handle_point (self, point_data): \"\"\" called when a data_point comes in to this node :param point_data: dict \"\"\" self . emit(self . calc(point_data[ \"data\" ])) def handle_batch (self, batch_data): \"\"\" called when a data_batch comes in :param batch_data: list of dicts \"\"\" out_list = list() for point in batch_data: out_list . append(self . calc(point[ \"data\" ])) self . emit(out_list) def calc (self, point_dict): point_dict[self . asfieldname] = point_dict[self . fieldname] * 2 return point_dict Use in a dfs script: @ double () . field ( 'val' ) . as ( 'double_val' )","title":"Python"},{"location":"nodes/python.html#the-python-node","text":"deprecated since vsn 1.0.0, see custom_nodes . Rules for python callback classes: Callback class must be in a module with the lowercase name of the class ie: module: \"double\", class: \"Double\" python callback class must be a subclass of the class Faxe from module faxe 'abstract' methods to implement are (note: they are all optional ): options() -> return a list of tuples // static init(self, args ) -> gets the object and a dict with args from options() handle_point(self, point_data) -> point_data is a dict handle_batch(self, batch_data ) -> batch_data is a list of dicts (points) the callbacks need not return anything except for the options method to emit data the method self.emit(data) has to be used, where data is a dict or a list of dicts All fields of data-item going in and out of a custom python node are placed under the root-object data . A custom python node is used with an @ as node sign instead of | in dfs! @ my_custom_python_node ()","title":"The python node"},{"location":"nodes/python.html#parameters","text":"Parameters can be freely defined by the python callback class via the static options() method (See example below). Note that parameter definition must be in python's bytes type.","title":"Parameters"},{"location":"nodes/python.html#example-callback","text":"The example python callback class below defined 2 Parameters: field must be a string and has no default value (so it must be given) as must be a string and has the default value 'double' from faxe import Faxe class Double (Faxe): @staticmethod def options (): \"\"\" overwrite this method to request options you would like to use return value is a list of tuples: (option_name, option_type, (optional: default type)) a two tuple: (b\"foo\", b\"string\") with no default value is mandatory in the dfs script a three tuple: (b\"foo\", b\"string\", b\"mystring\") may be overwritten in a dfs script :return: list of tuples \"\"\" opts = [ ( b'field' , b'string' ), ( b'as' , b'string' , b'double' ) ] return opts def init (self, args): \"\"\" will be called on startup with args requested with options() :param args: dict \"\"\" self . fieldname = args[ b'field' ] self . asfieldname = args[ b'as' ] print( \"my args: \" , args) def handle_point (self, point_data): \"\"\" called when a data_point comes in to this node :param point_data: dict \"\"\" self . emit(self . calc(point_data[ \"data\" ])) def handle_batch (self, batch_data): \"\"\" called when a data_batch comes in :param batch_data: list of dicts \"\"\" out_list = list() for point in batch_data: out_list . append(self . calc(point[ \"data\" ])) self . emit(out_list) def calc (self, point_dict): point_dict[self . asfieldname] = point_dict[self . fieldname] * 2 return point_dict Use in a dfs script: @ double () . field ( 'val' ) . as ( 'double_val' )","title":"Example Callback"},{"location":"nodes/stats.html","text":"The stats node The stats node lets you compute statistical functions on data_points and data_batches. See nodes under statistics for details. Stats nodes produce a new stream, the incoming stream is not outputted. Parameters All statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Stats"},{"location":"nodes/stats.html#the-stats-node","text":"The stats node lets you compute statistical functions on data_points and data_batches. See nodes under statistics for details. Stats nodes produce a new stream, the incoming stream is not outputted.","title":"The stats node"},{"location":"nodes/stats.html#parameters","text":"All statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/tcp_send.html","text":"The tcp_send node This node connects to a tcp endpoint and sends data with a defined packet size. Data to be sent can be: + a predefined string + a predefined json string + data that comes in to this node from downstream nodes Sending can be done either periodically (if every is given) or triggered by an incoming data-item. The node can also handle \"responses\" from the peer endpoint (timeout can be given). After the node has sent a tcp message, it starts the timeout to wait for the response. Any data that comes in after this timeout will be ignored. But note: There is no guarantee, that a message that is received by this node on its tcp socket is in any way related to the message it just sent out. In other words: unless every message (and its response) is marked with some unique id, there is no means of ensuring that any incoming message is the \"response\" to a previous send operation done by this node. A tcp endpoint can send a message to another endpoint whenever is wants to do so. If you rely on a strict request-response paradigm, consider using http, as this is made for such operations. Packet Data packet can be: 1 | 2 | 4 and defaults to 2. Packets consist of a header specifying the number of bytes in the packet, followed by that number of bytes. The header length can be one, two, or four bytes, and containing an unsigned integer in big-endian byte order. `Length_Header:16/integer, Data:{Length_Header}/binary` Experimental since 0.19.19 : packet now can also have the value 'line', if this is given, data from the socket will be treated as separated by newline characters (\\n, \\r\\n). The tcp listener is protected against flooding with the {active, once} inet option. Note: This node is a sink node and does not output any flow-data, therefore any node connected to this node will not get any data. Example | tcp_send () . ip ( '127.0.0.1' ) . port ( 5555 ) . every ( 3s ) . msg_text ( 'hello tcp!' ) tcp_send will send the string \"hello tcp!\" every 3 seconds ignoring any incoming tcp data. It will also send the same string, if any data-item is received from parent nodes. | tcp_send () . ip ( '127.0.0.1' ) . port ( 5555 ) . response_as ( 'data.tcp.response[1]' ) . response_timeout ( 2s ) . response_json () tcp_send will send all data coming in from its downstream nodes, after sending it will wait for data with a 2 second timeout. Data received will be interpreted as a json-string and injected into the current data-item with the path: data.tcp.response[1] (first array entry of the data-tcp-response subobject) Parameters Parameter Description Default ip( string ) ip or hostname for the tcp peer port( integer ) port number packet( integer ) packet length 2 every( string ) send interval undefined msg_text( string ) predefined string to send to the peer endpoint undefined msg_json( string ) predefined json-string to send to the peer endpoint undefined response_as( string ) name of the field for parsed data undefined response_json( is_set ) interprets a response as a json-string false (not set) response_timeout( duration ) timeout for a \"response\" after a message has been sent 5s","title":"Tcp send"},{"location":"nodes/tcp_send.html#the-tcp_send-node","text":"This node connects to a tcp endpoint and sends data with a defined packet size. Data to be sent can be: + a predefined string + a predefined json string + data that comes in to this node from downstream nodes Sending can be done either periodically (if every is given) or triggered by an incoming data-item. The node can also handle \"responses\" from the peer endpoint (timeout can be given). After the node has sent a tcp message, it starts the timeout to wait for the response. Any data that comes in after this timeout will be ignored. But note: There is no guarantee, that a message that is received by this node on its tcp socket is in any way related to the message it just sent out. In other words: unless every message (and its response) is marked with some unique id, there is no means of ensuring that any incoming message is the \"response\" to a previous send operation done by this node. A tcp endpoint can send a message to another endpoint whenever is wants to do so. If you rely on a strict request-response paradigm, consider using http, as this is made for such operations.","title":"The tcp_send node"},{"location":"nodes/tcp_send.html#packet-data","text":"packet can be: 1 | 2 | 4 and defaults to 2. Packets consist of a header specifying the number of bytes in the packet, followed by that number of bytes. The header length can be one, two, or four bytes, and containing an unsigned integer in big-endian byte order. `Length_Header:16/integer, Data:{Length_Header}/binary` Experimental since 0.19.19 : packet now can also have the value 'line', if this is given, data from the socket will be treated as separated by newline characters (\\n, \\r\\n). The tcp listener is protected against flooding with the {active, once} inet option. Note: This node is a sink node and does not output any flow-data, therefore any node connected to this node will not get any data.","title":"Packet Data"},{"location":"nodes/tcp_send.html#example","text":"| tcp_send () . ip ( '127.0.0.1' ) . port ( 5555 ) . every ( 3s ) . msg_text ( 'hello tcp!' ) tcp_send will send the string \"hello tcp!\" every 3 seconds ignoring any incoming tcp data. It will also send the same string, if any data-item is received from parent nodes. | tcp_send () . ip ( '127.0.0.1' ) . port ( 5555 ) . response_as ( 'data.tcp.response[1]' ) . response_timeout ( 2s ) . response_json () tcp_send will send all data coming in from its downstream nodes, after sending it will wait for data with a 2 second timeout. Data received will be interpreted as a json-string and injected into the current data-item with the path: data.tcp.response[1] (first array entry of the data-tcp-response subobject)","title":"Example"},{"location":"nodes/tcp_send.html#parameters","text":"Parameter Description Default ip( string ) ip or hostname for the tcp peer port( integer ) port number packet( integer ) packet length 2 every( string ) send interval undefined msg_text( string ) predefined string to send to the peer endpoint undefined msg_json( string ) predefined json-string to send to the peer endpoint undefined response_as( string ) name of the field for parsed data undefined response_json( is_set ) interprets a response as a json-string false (not set) response_timeout( duration ) timeout for a \"response\" after a message has been sent 5s","title":"Parameters"},{"location":"nodes/udp_send.html","text":"The udp_send node This node sets up an udp socket and sends data to a specified endpoint (host, port). Before sending, data-items will be converted to json format. Note: Broadcast messages are not possible with this node. Note: This node is a sink node and does not output any flow-data, therefore any node connected to this node will not get any data. Example | udp_send () . host ( '127.0.0.1' ) . port ( 5555 ) Will send incoming data-items json-encoded to the specified endpoint. Parameters Parameter Description Default host( string ) ip or hostname for the udp peer port( integer ) port number","title":"Udp send"},{"location":"nodes/udp_send.html#the-udp_send-node","text":"This node sets up an udp socket and sends data to a specified endpoint (host, port). Before sending, data-items will be converted to json format. Note: Broadcast messages are not possible with this node. Note: This node is a sink node and does not output any flow-data, therefore any node connected to this node will not get any data.","title":"The udp_send node"},{"location":"nodes/udp_send.html#example","text":"| udp_send () . host ( '127.0.0.1' ) . port ( 5555 ) Will send incoming data-items json-encoded to the specified endpoint.","title":"Example"},{"location":"nodes/udp_send.html#parameters","text":"Parameter Description Default host( string ) ip or hostname for the udp peer port( integer ) port number","title":"Parameters"},{"location":"nodes/data_collection/modbus.html","text":"The modbus node Deprecated since version 0.19.7, use the modbus_read instead. Read data via modbus tcp , supported read functions are : ['coils', 'hregs', 'iregs', 'inputs', 'memory'] Reading can be done periodically (if every is given) and/or via a trigger (incoming value). Read multiple values with possibly different functions at once. The node will optimize reading by treating contiguous values as one reading var. Normally the node will open one connection for every variable (after read optimization is applied). The maximum number of connections can be set with the max_connections option. If the align property is set, the nodes's read times will be truncated to the every property (For example, if the node is started at 12:06 and the every property is 5m then the next read will occur at 12:10, then the next at 12:15 and so on, instead of 12:06, 12:11 and so on). Examples | modbus() . ip ( '127.0.0.1' ) . device ( 255 ) . every ( 1s ) . function ( 'coils' , 'hregs' , 'iregs' , 'hregs' ) . from ( 2127 , 3008 , 104 , 30306 ) . count ( 1 , 2 , 2 , 4 ) . as ( 'data.EnergyConsumption' , 'data.CurrentValue' , 'data.EnergyDelivered' ) . output ( 'int16' , 'float32' , 'float32' , 'double' ) . signed ( true , true , false , false ) | modbus() . ip ( '127.0.0.1' ) . every ( 2m ) . function ( 'hregs' , 'hregs' , 'hregs' ) . from ( 2127 , 2125 , 104 ) . count ( 2 , 2 , 2 ) . as ( 'data.EnergyConsumption' , 'data.CurrentValue' , 'data.EnergyDelivered' ) . output ( 'float32' , 'float32' , 'float32' ) The above modbus node will open 2 connections to the given modbus device, because the start values 2125 (2 bytes) and 2127 (2 bytes) can be treated as one single value. Parameters Parameter Description Default ip( string ) ip address of modbus device port( integer ) port of modbus device 502 every( duration ) time between reads undefined align( is_set ) align read intervals according to every false (not set) device( integer ) modbus device id (0-255) 255 function( string_list ) list of read functions, one of ['coils', 'hregs', 'iregs', 'inputs', 'memory'] from( integer_list ) list of start values count( integer_list ) list of count values, how much data to read for every function given as( string_list ) output names for the read values output( string_list ) list of output formats one of ['int16', 'int32', 'float32', 'double', 'coils', 'ascii', 'binary'] undefined signed( atom_list true/false) list of values indicating if values are signed undefined round( integer ) Round all float32 and double values with the given precision. If a value has less than the given decimal places, it will be left untouched undefined (no rounding) max_connections( integer ) number of connections to the modbus device auto , meaning 1 connection for every variable (after read optimization) Note that, if given, all read parameters( function, from, count, as, output, signed ) must have the same length, this means if you have two values you want to read : . function ( 'coils' , 'hregs' ) ` all corresponding read params (if given) must have the same length: . as ( 'val1' , 'val2' ) . output ( int16 , float32 ) . from ( 1 , 2 ) . count ( 2 , 4 ) . signed ( true , true )","title":"Modbus"},{"location":"nodes/data_collection/modbus.html#the-modbus-node","text":"Deprecated since version 0.19.7, use the modbus_read instead. Read data via modbus tcp , supported read functions are : ['coils', 'hregs', 'iregs', 'inputs', 'memory'] Reading can be done periodically (if every is given) and/or via a trigger (incoming value). Read multiple values with possibly different functions at once. The node will optimize reading by treating contiguous values as one reading var. Normally the node will open one connection for every variable (after read optimization is applied). The maximum number of connections can be set with the max_connections option. If the align property is set, the nodes's read times will be truncated to the every property (For example, if the node is started at 12:06 and the every property is 5m then the next read will occur at 12:10, then the next at 12:15 and so on, instead of 12:06, 12:11 and so on).","title":"The modbus node"},{"location":"nodes/data_collection/modbus.html#examples","text":"| modbus() . ip ( '127.0.0.1' ) . device ( 255 ) . every ( 1s ) . function ( 'coils' , 'hregs' , 'iregs' , 'hregs' ) . from ( 2127 , 3008 , 104 , 30306 ) . count ( 1 , 2 , 2 , 4 ) . as ( 'data.EnergyConsumption' , 'data.CurrentValue' , 'data.EnergyDelivered' ) . output ( 'int16' , 'float32' , 'float32' , 'double' ) . signed ( true , true , false , false ) | modbus() . ip ( '127.0.0.1' ) . every ( 2m ) . function ( 'hregs' , 'hregs' , 'hregs' ) . from ( 2127 , 2125 , 104 ) . count ( 2 , 2 , 2 ) . as ( 'data.EnergyConsumption' , 'data.CurrentValue' , 'data.EnergyDelivered' ) . output ( 'float32' , 'float32' , 'float32' ) The above modbus node will open 2 connections to the given modbus device, because the start values 2125 (2 bytes) and 2127 (2 bytes) can be treated as one single value.","title":"Examples"},{"location":"nodes/data_collection/modbus.html#parameters","text":"Parameter Description Default ip( string ) ip address of modbus device port( integer ) port of modbus device 502 every( duration ) time between reads undefined align( is_set ) align read intervals according to every false (not set) device( integer ) modbus device id (0-255) 255 function( string_list ) list of read functions, one of ['coils', 'hregs', 'iregs', 'inputs', 'memory'] from( integer_list ) list of start values count( integer_list ) list of count values, how much data to read for every function given as( string_list ) output names for the read values output( string_list ) list of output formats one of ['int16', 'int32', 'float32', 'double', 'coils', 'ascii', 'binary'] undefined signed( atom_list true/false) list of values indicating if values are signed undefined round( integer ) Round all float32 and double values with the given precision. If a value has less than the given decimal places, it will be left untouched undefined (no rounding) max_connections( integer ) number of connections to the modbus device auto , meaning 1 connection for every variable (after read optimization) Note that, if given, all read parameters( function, from, count, as, output, signed ) must have the same length, this means if you have two values you want to read : . function ( 'coils' , 'hregs' ) ` all corresponding read params (if given) must have the same length: . as ( 'val1' , 'val2' ) . output ( int16 , float32 ) . from ( 1 , 2 ) . count ( 2 , 4 ) . signed ( true , true )","title":"Parameters"},{"location":"nodes/data_collection/modbus_read.html","text":"The modbus_read node Read data from a modbus slave device via modbus tcp , supported read functions are : ['coils', 'hregs', 'iregs', 'inputs', 'memory'] Reading can be done periodically (if every is given) and/or via a trigger (incoming value). Read multiple values with possibly different functions at once. The node will optimize reading by treating contiguous values as one reading var. If the align property is set to true (default), the nodes's read times will be truncated to the every property (For example, if the node is started at 12:06 and the every property is 5m then the next read will occur at 12:10, then the next at 12:15 and so on, instead of 12:06, 12:11 and so on). Examples | modbus_read () . ip ( '127.0.0.1' ) . device ( 255 ) . every ( 1s ) . function ( 'coils' , 'hregs' , 'iregs' , 'hregs' ) . from ( 2127 , 3008 , 104 , 30306 ) . count ( 1 , 2 , 2 , 4 ) . as ( 'data.EnergyConsumption' , 'data.CurrentValue' , 'data.EnergyDelivered' ) . output ( 'int16' , 'float32' , 'float32' , 'double' ) . signed ( true , true , false , false ) | modbus_read () . ip ( '127.0.0.1' ) . every ( 2m ) . function ( 'hregs' , 'hregs' , 'hregs' ) . from ( 2127 , 2125 , 104 ) . count ( 2 , 2 , 2 ) . as ( 'data.EnergyConsumption' , 'data.CurrentValue' , 'data.EnergyDelivered' ) . output ( 'float32' , 'float32' , 'float32' ) Parameters Parameter Description Default ip( string ) ip address of modbus device port( integer ) port of modbus device 502 every( duration ) time between reads undefined align( boolean ) align read intervals according to every true device( integer ) modbus device id (0-255) 255 function( string_list ) list of read functions, one of ['coils', 'hregs', 'iregs', 'inputs', 'memory'] from( integer_list ) list of start values count( integer_list ) list of count values, how much data to read for every function given as( string_list ) output names for the read values output( string_list ) list of output formats one of ['int16', 'int32', 'float32', 'double', 'coils', 'ascii', 'binary'] undefined signed( atom_list true/false) list of values indicating if values are signed undefined round( integer ) Round all float32 and double values with the given precision. If a value has less than the given decimal places, it will be left untouched undefined (no rounding) timeout( duration ) Read timeout for one request 5s Note that, if given, all read parameters( function, from, count, as, output, signed ) must have the same length, this means if you have two values you want to read : . function ( 'coils' , 'hregs' ) ` all corresponding read params (if given) must have the same length: . as ( 'val1' , 'val2' ) . output ( int16 , float32 ) . from ( 1 , 2 ) . count ( 2 , 4 ) . signed ( true , true )","title":"Modbus read"},{"location":"nodes/data_collection/modbus_read.html#the-modbus_read-node","text":"Read data from a modbus slave device via modbus tcp , supported read functions are : ['coils', 'hregs', 'iregs', 'inputs', 'memory'] Reading can be done periodically (if every is given) and/or via a trigger (incoming value). Read multiple values with possibly different functions at once. The node will optimize reading by treating contiguous values as one reading var. If the align property is set to true (default), the nodes's read times will be truncated to the every property (For example, if the node is started at 12:06 and the every property is 5m then the next read will occur at 12:10, then the next at 12:15 and so on, instead of 12:06, 12:11 and so on).","title":"The modbus_read node"},{"location":"nodes/data_collection/modbus_read.html#examples","text":"| modbus_read () . ip ( '127.0.0.1' ) . device ( 255 ) . every ( 1s ) . function ( 'coils' , 'hregs' , 'iregs' , 'hregs' ) . from ( 2127 , 3008 , 104 , 30306 ) . count ( 1 , 2 , 2 , 4 ) . as ( 'data.EnergyConsumption' , 'data.CurrentValue' , 'data.EnergyDelivered' ) . output ( 'int16' , 'float32' , 'float32' , 'double' ) . signed ( true , true , false , false ) | modbus_read () . ip ( '127.0.0.1' ) . every ( 2m ) . function ( 'hregs' , 'hregs' , 'hregs' ) . from ( 2127 , 2125 , 104 ) . count ( 2 , 2 , 2 ) . as ( 'data.EnergyConsumption' , 'data.CurrentValue' , 'data.EnergyDelivered' ) . output ( 'float32' , 'float32' , 'float32' )","title":"Examples"},{"location":"nodes/data_collection/modbus_read.html#parameters","text":"Parameter Description Default ip( string ) ip address of modbus device port( integer ) port of modbus device 502 every( duration ) time between reads undefined align( boolean ) align read intervals according to every true device( integer ) modbus device id (0-255) 255 function( string_list ) list of read functions, one of ['coils', 'hregs', 'iregs', 'inputs', 'memory'] from( integer_list ) list of start values count( integer_list ) list of count values, how much data to read for every function given as( string_list ) output names for the read values output( string_list ) list of output formats one of ['int16', 'int32', 'float32', 'double', 'coils', 'ascii', 'binary'] undefined signed( atom_list true/false) list of values indicating if values are signed undefined round( integer ) Round all float32 and double values with the given precision. If a value has less than the given decimal places, it will be left untouched undefined (no rounding) timeout( duration ) Read timeout for one request 5s Note that, if given, all read parameters( function, from, count, as, output, signed ) must have the same length, this means if you have two values you want to read : . function ( 'coils' , 'hregs' ) ` all corresponding read params (if given) must have the same length: . as ( 'val1' , 'val2' ) . output ( int16 , float32 ) . from ( 1 , 2 ) . count ( 2 , 4 ) . signed ( true , true )","title":"Parameters"},{"location":"nodes/data_collection/s7read.html","text":"The s7read node Read data from a siemens s7 plc via the snap7 library using the iso on tcp protocol . The node is tested to read data from S7 300 and S7 1500 CPUs, but should be able to read data from any S7 PLC that supports the S7 comm protocol. Reading can be done periodically and/or triggered via incoming data-items. If the every parameter is not given, reading will be done only with trigger data-items. Data addressing can be done in a Step7 schema or with a slightly different schema used in node-red (although the step7 variant is preferred). See table below for more information about addressing. Optimzation when reading data The node will optimize reading by treating contiguous values as one reading var. Thus more data can be read in one go. since 0.20.0: when optimized is set to true (default), then optimization and reading will be done outside of the node. All variables from all s7read nodes, that target the same PLC are optimized as a whole, this makes it possible to take advantage of the maximum PDU size. This will possibly lead to more than one request for a specific time slot, if there are many vars to read. In optimized mode, it does not make a difference, if we use many s7read nodes, where every node just reads a view vars, or if we have only one s7read node, that reads a lot of vars (or anything in between) Connections to a PLC are handled by a connection pool, if use_pool is set to true, which can grow and shrink as needed. (This is especially useful, when connecting to a PLC, that has limited connection resources) The number of Min and Max connection count can be set in faxe configuration . Defaults to 2 and 16. Strings A String is defined as a sequence of contiguous char (byte) addresses. For strings faxe uses a special syntax not found in step7 addressing: DB5.DBS7.4 would read 4 bytes starting at byte 7 of DB 5 and output a string value. Note : Characters with an ascii value below 32 will be stripped from the char-sequence. Also for strings the above mentioned read-optimization will not be used. Examples | s7read () . ip ( '10.1.1.5' ) . rack ( 0 ) . slot ( 2 ) . every ( 300ms ) . vars ( 'DB1140.DBX4.0' , 'DB1140.DBX4.1' , 'DB1140.DBX4.4' , 'DB1140.DBX4.5' ) . as ( 'data.tbo[1].ix_OcM1' , 'data.tbo[1].ix_OcM2' , 'data.tbo[1].ix_Lift_PosTop' , 'data.tbo[1].ix_Lift_PosBo' ) Read 4 values (BOOL in this case) from a plc every 300 milliseconds and name them with a deep json path. def db_number = 1140 def db = 'DB{{db_number}}.DB' | s7read () . ip ( '10.1.1.5' ) . as_prefix ( 'data.tbo.' ) . vars_prefix ( db ) . vars ( 'X4.0' , 'X4.1' , 'X4.4' , 'X4.5' ) . as ( 'ix_OcM1' , 'ix_OcM2' , 'ix_Lift_PosTop' , 'ix_Lift_PosBo' ) Use of as_prefix and vars_prefix . The node will not read data on its own, because it has no every parameter. instead reading is done on data input from another node. | s7read () . ip ( '10.1.1.5' ) . rack ( 0 ) . slot ( 2 ) . every ( 300ms ) . vars ( 'DB12004.DBS36.30' ) . as ( 'data.LcBc' ) Read a sequence of 30 bytes as a string. def db_number = 1140 def db = 'DB{{db_number}}.DB' | s7read () . ip ( '10.1.1.5' ) . as_prefix ( 'data.tbo.' ) . vars_prefix ( db ) . byte_offset ( 4 ) . vars ( 'X0.0' , 'X0.1' , 'X0.4' , 'X0.5' ) . as ( 'ix_OcM1' , 'ix_OcM2' , 'ix_Lift_PosTop' , 'ix_Lift_PosBo' ) In the last example byte_offset of 4 is used, so effectively the following addresses will be used: X4.0, X4.1, X4.4, X4.5 since v0.19.5 When as is not given, every second entry in vars is used as a field-name instead. def db_number = 1140 def db = 'DB{{db_number}}.DB' | s7read () . ip ( '10.1.1.5' ) . as_prefix ( 'data.tbo.' ) . vars_prefix ( db ) . byte_offset ( 4 ) . vars ( 'X0.0' , 'ix_OcM1' , 'X0.1' , 'ix_OcM2' , 'X0.4' , 'ix_Lift_PosTop' , 'X0.5' , 'ix_Lift_PosBo' ) Parameters Parameter Description Default ip( string ) ip address of plc port( integer ) network port 102 (standard s7 port) every( duration ) time between reads undefined align( is_set ) align read intervals according to every false (not set) slot( integer ) plc slot number 1 rack( integer ) plc rack number 0 vars( string_list ) list of s7 addresses ie: 'DB3.DBX2.5' (see table below) as( string_list ) output names for the read values Since 0.19.5: if not given, every second entry in vars is used as a fieldname (prefixes can still be used) undefined vars_prefix( string ) vars will be prefixed with this value undefined as_prefix( string ) as values will be prefixed with this value undefined byte_offset( integer ) offset for addressing, every address in vars gets this offset added 0 diff( is_set ) when given, only output values different to previous values false (not set) use_pool ( bool ) whether to use the built-in connection pool from config value optimized ( bool ) whether to use the collected read optimization or do standalone reading for this node from config value Note that params vars and as must have the same length (if both are given). Data addressing Note: Step7 style preferred and should be used ! Address Step7 equivalent JS Data type Description DB5,X0.1 DB5.DBX0.1 Boolean Bit 1 of byte 0 of DB 5 DB23,B1 or DB23,BYTE1 DB23.DBB1 Number Byte 1 (0-255) of DB 23 DB100,C2 or DB100,CHAR2 DB100.DBB2 String Byte 2 of DB 100 as a Char DB42,I3 or DB42,INT3 DB42.DBW3 Number Signed 16-bit number at byte 3 of DB 42 DB57,WORD4 DB57.DBW4 Number Unsigned 16-bit number at byte 4 of DB 57 DB13,DI5 or DB13,DINT5 DB13.DBD5 Number Signed 32-bit number at byte 5 of DB 13 DB19,DW6 or DB19,DWORD6 DB19.DBD6 Number Unsigned 32-bit number at byte 6 of DB 19 DB21,DR7 or DB21,REAL7 DB19.DBD6 Number Floating point 32-bit number at byte 7 of DB 21 DB2,S7.10* DB2.DBS7.10 (faxe only) String String of length 10 starting at byte 7 of DB 2 I1.0 or E1.0 I1.0 or E1.0 Boolean Bit 0 of byte 1 of input area Q2.1 or A2.1 Q2.1 or A2.1 Boolean Bit 1 of byte 2 of output area M3.2 QM3.2 Boolean Bit 2 of byte 3 of memory area IB4 or EB4 IB4 or EB4 Number Byte 4 (0 -255) of input area QB5 or AB5 QB5 or AB5 Number Byte 5 (0 -255) of output area MB6 MB6 Number Byte 6 (0 -255) of memory area IC7 or EC7 IB7 or EB7 String Byte 7 of input area as a Char QC8 or AC8 QB8 or AB8 String Byte 8 of output area as a Char MC9 MB9 String Byte 9 of memory area as a Char II10 or EI10 IW10 or EW10 Number Signed 16-bit number at byte 10 of input area QI12 or AI12 QW12 or AW12 Number Signed 16-bit number at byte 12 of output area MI14 MW14 Number Signed 16-bit number at byte 14 of memory area IW16 or EW16 IW16 or EW16 Number Unsigned 16-bit number at byte 16 of input area QW18 or AW18 QW18 or AW18 Number Unsigned 16-bit number at byte 18 of output area MW20 MW20 Number Unsigned 16-bit number at byte 20 of memory area IDI22 or EDI22 ID22 or ED22 Number Signed 32-bit number at byte 22 of input area QDI24 or ADI24 QD24 or AD24 Number Signed 32-bit number at byte 24 of output area MDI26 MD26 Number Signed 32-bit number at byte 26 of memory area ID28 or ED28 ID28 or ED28 Number Unsigned 32-bit number at byte 28 of input area QD30 or AD30 QD30 or AD30 Number Unsigned 32-bit number at byte 30 of output area MD32 MD32 Number Unsigned 32-bit number at byte 32 of memory area IR34 or ER34 IR34 or ER34 Number Floating point 32-bit number at byte 34 of input area QR36 or AR36 QR36 or AR36 Number Floating point 32-bit number at byte 36 of output area MR38 MR38 Number Floating point 32-bit number at byte 38 of memory area","title":"S7read"},{"location":"nodes/data_collection/s7read.html#the-s7read-node","text":"Read data from a siemens s7 plc via the snap7 library using the iso on tcp protocol . The node is tested to read data from S7 300 and S7 1500 CPUs, but should be able to read data from any S7 PLC that supports the S7 comm protocol. Reading can be done periodically and/or triggered via incoming data-items. If the every parameter is not given, reading will be done only with trigger data-items. Data addressing can be done in a Step7 schema or with a slightly different schema used in node-red (although the step7 variant is preferred). See table below for more information about addressing.","title":"The s7read node"},{"location":"nodes/data_collection/s7read.html#optimzation-when-reading-data","text":"The node will optimize reading by treating contiguous values as one reading var. Thus more data can be read in one go. since 0.20.0: when optimized is set to true (default), then optimization and reading will be done outside of the node. All variables from all s7read nodes, that target the same PLC are optimized as a whole, this makes it possible to take advantage of the maximum PDU size. This will possibly lead to more than one request for a specific time slot, if there are many vars to read. In optimized mode, it does not make a difference, if we use many s7read nodes, where every node just reads a view vars, or if we have only one s7read node, that reads a lot of vars (or anything in between) Connections to a PLC are handled by a connection pool, if use_pool is set to true, which can grow and shrink as needed. (This is especially useful, when connecting to a PLC, that has limited connection resources) The number of Min and Max connection count can be set in faxe configuration . Defaults to 2 and 16.","title":"Optimzation when reading data"},{"location":"nodes/data_collection/s7read.html#strings","text":"A String is defined as a sequence of contiguous char (byte) addresses. For strings faxe uses a special syntax not found in step7 addressing: DB5.DBS7.4 would read 4 bytes starting at byte 7 of DB 5 and output a string value. Note : Characters with an ascii value below 32 will be stripped from the char-sequence. Also for strings the above mentioned read-optimization will not be used.","title":"Strings"},{"location":"nodes/data_collection/s7read.html#examples","text":"| s7read () . ip ( '10.1.1.5' ) . rack ( 0 ) . slot ( 2 ) . every ( 300ms ) . vars ( 'DB1140.DBX4.0' , 'DB1140.DBX4.1' , 'DB1140.DBX4.4' , 'DB1140.DBX4.5' ) . as ( 'data.tbo[1].ix_OcM1' , 'data.tbo[1].ix_OcM2' , 'data.tbo[1].ix_Lift_PosTop' , 'data.tbo[1].ix_Lift_PosBo' ) Read 4 values (BOOL in this case) from a plc every 300 milliseconds and name them with a deep json path. def db_number = 1140 def db = 'DB{{db_number}}.DB' | s7read () . ip ( '10.1.1.5' ) . as_prefix ( 'data.tbo.' ) . vars_prefix ( db ) . vars ( 'X4.0' , 'X4.1' , 'X4.4' , 'X4.5' ) . as ( 'ix_OcM1' , 'ix_OcM2' , 'ix_Lift_PosTop' , 'ix_Lift_PosBo' ) Use of as_prefix and vars_prefix . The node will not read data on its own, because it has no every parameter. instead reading is done on data input from another node. | s7read () . ip ( '10.1.1.5' ) . rack ( 0 ) . slot ( 2 ) . every ( 300ms ) . vars ( 'DB12004.DBS36.30' ) . as ( 'data.LcBc' ) Read a sequence of 30 bytes as a string. def db_number = 1140 def db = 'DB{{db_number}}.DB' | s7read () . ip ( '10.1.1.5' ) . as_prefix ( 'data.tbo.' ) . vars_prefix ( db ) . byte_offset ( 4 ) . vars ( 'X0.0' , 'X0.1' , 'X0.4' , 'X0.5' ) . as ( 'ix_OcM1' , 'ix_OcM2' , 'ix_Lift_PosTop' , 'ix_Lift_PosBo' ) In the last example byte_offset of 4 is used, so effectively the following addresses will be used: X4.0, X4.1, X4.4, X4.5 since v0.19.5 When as is not given, every second entry in vars is used as a field-name instead. def db_number = 1140 def db = 'DB{{db_number}}.DB' | s7read () . ip ( '10.1.1.5' ) . as_prefix ( 'data.tbo.' ) . vars_prefix ( db ) . byte_offset ( 4 ) . vars ( 'X0.0' , 'ix_OcM1' , 'X0.1' , 'ix_OcM2' , 'X0.4' , 'ix_Lift_PosTop' , 'X0.5' , 'ix_Lift_PosBo' )","title":"Examples"},{"location":"nodes/data_collection/s7read.html#parameters","text":"Parameter Description Default ip( string ) ip address of plc port( integer ) network port 102 (standard s7 port) every( duration ) time between reads undefined align( is_set ) align read intervals according to every false (not set) slot( integer ) plc slot number 1 rack( integer ) plc rack number 0 vars( string_list ) list of s7 addresses ie: 'DB3.DBX2.5' (see table below) as( string_list ) output names for the read values Since 0.19.5: if not given, every second entry in vars is used as a fieldname (prefixes can still be used) undefined vars_prefix( string ) vars will be prefixed with this value undefined as_prefix( string ) as values will be prefixed with this value undefined byte_offset( integer ) offset for addressing, every address in vars gets this offset added 0 diff( is_set ) when given, only output values different to previous values false (not set) use_pool ( bool ) whether to use the built-in connection pool from config value optimized ( bool ) whether to use the collected read optimization or do standalone reading for this node from config value Note that params vars and as must have the same length (if both are given).","title":"Parameters"},{"location":"nodes/data_collection/s7read.html#data-addressing","text":"Note: Step7 style preferred and should be used ! Address Step7 equivalent JS Data type Description DB5,X0.1 DB5.DBX0.1 Boolean Bit 1 of byte 0 of DB 5 DB23,B1 or DB23,BYTE1 DB23.DBB1 Number Byte 1 (0-255) of DB 23 DB100,C2 or DB100,CHAR2 DB100.DBB2 String Byte 2 of DB 100 as a Char DB42,I3 or DB42,INT3 DB42.DBW3 Number Signed 16-bit number at byte 3 of DB 42 DB57,WORD4 DB57.DBW4 Number Unsigned 16-bit number at byte 4 of DB 57 DB13,DI5 or DB13,DINT5 DB13.DBD5 Number Signed 32-bit number at byte 5 of DB 13 DB19,DW6 or DB19,DWORD6 DB19.DBD6 Number Unsigned 32-bit number at byte 6 of DB 19 DB21,DR7 or DB21,REAL7 DB19.DBD6 Number Floating point 32-bit number at byte 7 of DB 21 DB2,S7.10* DB2.DBS7.10 (faxe only) String String of length 10 starting at byte 7 of DB 2 I1.0 or E1.0 I1.0 or E1.0 Boolean Bit 0 of byte 1 of input area Q2.1 or A2.1 Q2.1 or A2.1 Boolean Bit 1 of byte 2 of output area M3.2 QM3.2 Boolean Bit 2 of byte 3 of memory area IB4 or EB4 IB4 or EB4 Number Byte 4 (0 -255) of input area QB5 or AB5 QB5 or AB5 Number Byte 5 (0 -255) of output area MB6 MB6 Number Byte 6 (0 -255) of memory area IC7 or EC7 IB7 or EB7 String Byte 7 of input area as a Char QC8 or AC8 QB8 or AB8 String Byte 8 of output area as a Char MC9 MB9 String Byte 9 of memory area as a Char II10 or EI10 IW10 or EW10 Number Signed 16-bit number at byte 10 of input area QI12 or AI12 QW12 or AW12 Number Signed 16-bit number at byte 12 of output area MI14 MW14 Number Signed 16-bit number at byte 14 of memory area IW16 or EW16 IW16 or EW16 Number Unsigned 16-bit number at byte 16 of input area QW18 or AW18 QW18 or AW18 Number Unsigned 16-bit number at byte 18 of output area MW20 MW20 Number Unsigned 16-bit number at byte 20 of memory area IDI22 or EDI22 ID22 or ED22 Number Signed 32-bit number at byte 22 of input area QDI24 or ADI24 QD24 or AD24 Number Signed 32-bit number at byte 24 of output area MDI26 MD26 Number Signed 32-bit number at byte 26 of memory area ID28 or ED28 ID28 or ED28 Number Unsigned 32-bit number at byte 28 of input area QD30 or AD30 QD30 or AD30 Number Unsigned 32-bit number at byte 30 of output area MD32 MD32 Number Unsigned 32-bit number at byte 32 of memory area IR34 or ER34 IR34 or ER34 Number Floating point 32-bit number at byte 34 of input area QR36 or AR36 QR36 or AR36 Number Floating point 32-bit number at byte 36 of output area MR38 MR38 Number Floating point 32-bit number at byte 38 of memory area","title":"Data addressing"},{"location":"nodes/data_collection/tcp_receive.html","text":"The tcp_receive node This node is used to receive data from a tcp socket. There are two modes it can operate in: Connect : If ip is set to a valid hostname or ip address, the node will connect to that peer and then just waits for incoming data. Listen : Without ip , the node will just open a tcp listen port and waits for a connection from outside to receive data. Data - Parser When we define a parser module, incoming data can be parsed to faxe's internal data-format with it. If no parser is defined, incoming data will be set as is to the field given with the as paramenter. If both parser and as are not given, the node assumes, that incoming data has JSON format and will try to decode it into faxe's internal format. Packet size Data is expected with a defined packet size. packet can be: 1 | 2 | 4 and defaults to 2. Packets consist of a header specifying the number of bytes in the packet, followed by that number of bytes. The header length can be one, two, or four bytes, and containing an unsigned integer in big-endian byte order. `Length_Header:16/integer, Data:{Length_Header}/binary` Experimental since 0.19.19 : packet now can also have the value 'line', if this is given, data from the socket will be treated as separated by newline characters (\\n, \\r\\n). If the changed option is given, the node will only emit on changed values (crc32 checksum comparison). The tcp listener is protected against flooding with the {active, once} inet option. Examples | tcp_recv () . port ( 9745 ) . packet ( 4 ) Sets up a tcp listen socket on port 9745 and awaits an incoming connection. It uses a 4-byte length header to determine packet boundaries and will try to json-decode incoming data. def parser = 'parser_robot_plc_v1' | tcp_recv () . ip ( '212.14.149.8' ) . port ( 9715 ) . parser ( parser ) . as ( 'data' ) | tcp_recv () . ip ( '212.14.149.8' ) . port ( 9715 ) . packet ( 4 ) . as ( 'data.raw' ) Parameters Parameter Description Default ip( string ) ip or hostname for the tcp peer undefined port( integer ) port number packet( integer ) packet length 2 parser( string ) name of parser to use for data conversion, see table below undefined as( string ) name of the field for parsed data undefined changed( is_set ) whether to check for changed data false (not set)","title":"Tcp receive"},{"location":"nodes/data_collection/tcp_receive.html#the-tcp_receive-node","text":"This node is used to receive data from a tcp socket. There are two modes it can operate in: Connect : If ip is set to a valid hostname or ip address, the node will connect to that peer and then just waits for incoming data. Listen : Without ip , the node will just open a tcp listen port and waits for a connection from outside to receive data.","title":"The tcp_receive node"},{"location":"nodes/data_collection/tcp_receive.html#data-parser","text":"When we define a parser module, incoming data can be parsed to faxe's internal data-format with it. If no parser is defined, incoming data will be set as is to the field given with the as paramenter. If both parser and as are not given, the node assumes, that incoming data has JSON format and will try to decode it into faxe's internal format.","title":"Data - Parser"},{"location":"nodes/data_collection/tcp_receive.html#packet-size","text":"Data is expected with a defined packet size. packet can be: 1 | 2 | 4 and defaults to 2. Packets consist of a header specifying the number of bytes in the packet, followed by that number of bytes. The header length can be one, two, or four bytes, and containing an unsigned integer in big-endian byte order. `Length_Header:16/integer, Data:{Length_Header}/binary` Experimental since 0.19.19 : packet now can also have the value 'line', if this is given, data from the socket will be treated as separated by newline characters (\\n, \\r\\n). If the changed option is given, the node will only emit on changed values (crc32 checksum comparison). The tcp listener is protected against flooding with the {active, once} inet option.","title":"Packet size"},{"location":"nodes/data_collection/tcp_receive.html#examples","text":"| tcp_recv () . port ( 9745 ) . packet ( 4 ) Sets up a tcp listen socket on port 9745 and awaits an incoming connection. It uses a 4-byte length header to determine packet boundaries and will try to json-decode incoming data. def parser = 'parser_robot_plc_v1' | tcp_recv () . ip ( '212.14.149.8' ) . port ( 9715 ) . parser ( parser ) . as ( 'data' ) | tcp_recv () . ip ( '212.14.149.8' ) . port ( 9715 ) . packet ( 4 ) . as ( 'data.raw' )","title":"Examples"},{"location":"nodes/data_collection/tcp_receive.html#parameters","text":"Parameter Description Default ip( string ) ip or hostname for the tcp peer undefined port( integer ) port number packet( integer ) packet length 2 parser( string ) name of parser to use for data conversion, see table below undefined as( string ) name of the field for parsed data undefined changed( is_set ) whether to check for changed data false (not set)","title":"Parameters"},{"location":"nodes/data_collection/tcp_receive_line.html","text":"The tcp_receive_line node This node connects to a tcp endpoint and awaits data in a line separated special format, which is defined by the parser parameter. The parser will then try to convert the data to faxe's internal format and emit the result. At the moment the line separator is fixed to \\n . If the changed option is given, the node will only emit on changed values (crc32 checksum comparison). The tcp listener is protected against flooding with the {active, once} inet option. Example def parser = 'parser_conv_tracking_v1' | tcp_recv_line () . ip ( '212.14.149.3' ) . port ( 2004 ) . parser ( parser ) . as ( 'data' ) Parameters Parameter Description Default ip( string ) ip or hostname for the tcp peer port( integer ) port number parser( string ) name of parser to use for data conversion, see table below as( string ) name of the field for parsed data changed( is_set ) whether to check for changed data false (not set) min_length( integer) lines shorter than min_length bytes will be ignored 61 Available Parsers","title":"Tcp receive line"},{"location":"nodes/data_collection/tcp_receive_line.html#the-tcp_receive_line-node","text":"This node connects to a tcp endpoint and awaits data in a line separated special format, which is defined by the parser parameter. The parser will then try to convert the data to faxe's internal format and emit the result. At the moment the line separator is fixed to \\n . If the changed option is given, the node will only emit on changed values (crc32 checksum comparison). The tcp listener is protected against flooding with the {active, once} inet option.","title":"The tcp_receive_line node"},{"location":"nodes/data_collection/tcp_receive_line.html#example","text":"def parser = 'parser_conv_tracking_v1' | tcp_recv_line () . ip ( '212.14.149.3' ) . port ( 2004 ) . parser ( parser ) . as ( 'data' )","title":"Example"},{"location":"nodes/data_collection/tcp_receive_line.html#parameters","text":"Parameter Description Default ip( string ) ip or hostname for the tcp peer port( integer ) port number parser( string ) name of parser to use for data conversion, see table below as( string ) name of the field for parsed data changed( is_set ) whether to check for changed data false (not set) min_length( integer) lines shorter than min_length bytes will be ignored 61","title":"Parameters"},{"location":"nodes/data_collection/tcp_receive_line.html#available-parsers","text":"","title":"Available Parsers"},{"location":"nodes/data_collection/udp_receive.html","text":"The udp_receive node This node listens on an udp socket and awaits data in a special format, which is defined by the parser parameter, the parser will then try to convert the data to faxe format and emit the result. If the changed option is given, the node will only emit on changed values (crc32 checksum comparison). The udp listener is protected against flooding with the {active, once} inet option. Example def parser = 'parser_robot_plc_v1' | udp_recv () . ip ( '212.14.149.8' ) . port ( 9715 ) . parser ( parser ) . as ( 'data' ) Parameters Parameter Description Default port( integer ) port number parser( string ) name of parser to use for data conversion, see table below as( string ) name of the field for parsed data changed( is_set ) whether to check for changed data false (not set) Available Parsers","title":"Udp receive"},{"location":"nodes/data_collection/udp_receive.html#the-udp_receive-node","text":"This node listens on an udp socket and awaits data in a special format, which is defined by the parser parameter, the parser will then try to convert the data to faxe format and emit the result. If the changed option is given, the node will only emit on changed values (crc32 checksum comparison). The udp listener is protected against flooding with the {active, once} inet option.","title":"The udp_receive node"},{"location":"nodes/data_collection/udp_receive.html#example","text":"def parser = 'parser_robot_plc_v1' | udp_recv () . ip ( '212.14.149.8' ) . port ( 9715 ) . parser ( parser ) . as ( 'data' )","title":"Example"},{"location":"nodes/data_collection/udp_receive.html#parameters","text":"Parameter Description Default port( integer ) port number parser( string ) name of parser to use for data conversion, see table below as( string ) name of the field for parsed data changed( is_set ) whether to check for changed data false (not set)","title":"Parameters"},{"location":"nodes/data_collection/udp_receive.html#available-parsers","text":"","title":"Available Parsers"},{"location":"nodes/database/crate_out.html","text":"The crate_out node Write data to CrateDB. Sends data to a CRATE DB HTTP endpoint using Crate's HTTP Api. If any errors occur during the request, the node will attempt to retry sending. Note: This node is a sink node and does not output any flow-data, therefore any node connected to this node will not get any data. Example def db_table = 'grip_log_fulltext3' def db_fields = [ 'id' , 'df' , 'vs' , 'topic' ] def faxe_fields = [ 'id' , 'df' , 'vs' , 'topic' ] | http_post_crate() . table ( db_table ) . db_fields ( db_fields ) . faxe_fields ( faxe_fields ) . remaining_fields_as ( 'data_obj' ) Inserts the faxe-fields id , df , vs , topic into the db-fields with the same names and all remaining fields into the db-field named data_obj (which is of type 'OBJECT') in the table grip_log_fulltext3 . Parameters Parameter Description Default host( string ) hostname or ip address of endpoint from config file port( integer ) port number from config file user( string ) username from config file pass( string ) password from config file tls( is_set ) whether to use tls ie. https false (not set) table( string ) database tablename db_fields( string_list ) db fieldnames (mapping for faxe fieldname to table field names) faxe_fields( string_list ) faxe fieldnames (mapping for faxe fieldname to table field names) remaining_fields_as( string ) if given inserts all fields not in faxe_fields into the given field, which must be of type 'object' undefined","title":"Crate out"},{"location":"nodes/database/crate_out.html#the-crate_out-node","text":"Write data to CrateDB. Sends data to a CRATE DB HTTP endpoint using Crate's HTTP Api. If any errors occur during the request, the node will attempt to retry sending. Note: This node is a sink node and does not output any flow-data, therefore any node connected to this node will not get any data.","title":"The crate_out node"},{"location":"nodes/database/crate_out.html#example","text":"def db_table = 'grip_log_fulltext3' def db_fields = [ 'id' , 'df' , 'vs' , 'topic' ] def faxe_fields = [ 'id' , 'df' , 'vs' , 'topic' ] | http_post_crate() . table ( db_table ) . db_fields ( db_fields ) . faxe_fields ( faxe_fields ) . remaining_fields_as ( 'data_obj' ) Inserts the faxe-fields id , df , vs , topic into the db-fields with the same names and all remaining fields into the db-field named data_obj (which is of type 'OBJECT') in the table grip_log_fulltext3 .","title":"Example"},{"location":"nodes/database/crate_out.html#parameters","text":"Parameter Description Default host( string ) hostname or ip address of endpoint from config file port( integer ) port number from config file user( string ) username from config file pass( string ) password from config file tls( is_set ) whether to use tls ie. https false (not set) table( string ) database tablename db_fields( string_list ) db fieldnames (mapping for faxe fieldname to table field names) faxe_fields( string_list ) faxe fieldnames (mapping for faxe fieldname to table field names) remaining_fields_as( string ) if given inserts all fields not in faxe_fields into the given field, which must be of type 'object' undefined","title":"Parameters"},{"location":"nodes/database/crate_query.html","text":"The crate_query node Query the CRATE database for time series data . This node is experimental . The select statement will be executed periodically according to the every parameter. Each time the database is queried, the timestamps will be set according to period . Example def host = '10.14.204.8' def port = 5433 %% to escape single quotes (') we use double single quotes ('') def query = ' SELECT avg(data_obj[''x''][''cur'']) AS x_cur, avg(data_obj[''y''][''cur'']) AS y_cur, avg(data_obj[''z''][''cur'']) AS z_cur, avg(data_obj[''yaw''][''cur'']) AS yaw_cur, avg(data_obj[''pitch''][''cur'']) AS pitch_cur FROM robotplc_parted; ' def s = | crate_query() . query ( query ) . group_by_time ( 3m ) . every ( 15s ) . period ( 30m ) . align () The above example will execute the query every 15 seconds. It gets data which is in the timerange now -30 minutes and now . Parameters Parameter Description Default host( string ) CrateDB host from config file port( integer ) CrateDB port from config file user( string ) username from config file pass( string ) password from config file database( string ) Database name from config file query( string text ) 'SELECT-FROM' query clause time_field( string ) name of the timefield to use 'ts' every( duration ) time between query execution 5s period( duration ) time span of data to query 1h align( is_set ) whether to align period to full every durations false (not set) group_by_time( duration ) group the aggregations into time buckets 2m group_by( string_list ) additional group by [] limit( string ) LIMIT statement '30'","title":"Crate query"},{"location":"nodes/database/crate_query.html#the-crate_query-node","text":"Query the CRATE database for time series data . This node is experimental . The select statement will be executed periodically according to the every parameter. Each time the database is queried, the timestamps will be set according to period .","title":"The crate_query node"},{"location":"nodes/database/crate_query.html#example","text":"def host = '10.14.204.8' def port = 5433 %% to escape single quotes (') we use double single quotes ('') def query = ' SELECT avg(data_obj[''x''][''cur'']) AS x_cur, avg(data_obj[''y''][''cur'']) AS y_cur, avg(data_obj[''z''][''cur'']) AS z_cur, avg(data_obj[''yaw''][''cur'']) AS yaw_cur, avg(data_obj[''pitch''][''cur'']) AS pitch_cur FROM robotplc_parted; ' def s = | crate_query() . query ( query ) . group_by_time ( 3m ) . every ( 15s ) . period ( 30m ) . align () The above example will execute the query every 15 seconds. It gets data which is in the timerange now -30 minutes and now .","title":"Example"},{"location":"nodes/database/crate_query.html#parameters","text":"Parameter Description Default host( string ) CrateDB host from config file port( integer ) CrateDB port from config file user( string ) username from config file pass( string ) password from config file database( string ) Database name from config file query( string text ) 'SELECT-FROM' query clause time_field( string ) name of the timefield to use 'ts' every( duration ) time between query execution 5s period( duration ) time span of data to query 1h align( is_set ) whether to align period to full every durations false (not set) group_by_time( duration ) group the aggregations into time buckets 2m group_by( string_list ) additional group by [] limit( string ) LIMIT statement '30'","title":"Parameters"},{"location":"nodes/database/crate_query_cont.html","text":"The crate_query_cont node [ experimental ] since 0.19.0 Query the CRATE database for time series data . This node is used for continous timeseries queries. Query A select statement will be executed periodically, on every iteration a timefilter gets adjusted according to the period parameter. For this to work, the query given must contain the $__timefilter placeholder in the query's where clause: def query = ' SELECT ts, id, temp1 FROM doc.table WHERE $__timefilter AND stream_id = ' dd419f94834a ' ORDER BY ts ASC ' The timefilter placeholder gets replaced by this statement: ts >= $ 1 AND ts < $ 2 Start The start parameter determines the query start time. It's value is a past point in time. There are two possible ways to provide this: provide an ISO8601 Datetimestamp, ie: '2021-11-16T17:15:00.000Z' provide a query that results in 1 row with exactly 1 column named 'ts' containing an ISO8601 Datetimestamp. SELECT DATE_FORMAT(ts) FROM table WHERE worked_on = false ORDER BY ts LIMIT 1 or with a fallback start-time SELECT COALESCE ( ( SELECT DATE_FORMAT(ts) FROM table WHERE worked_on = false ORDER BY ts LIMIT 1 ), '2021-11-16T16:20:00.000000Z' ) AS ts Historic and up-to-date data While reading data from the past, min_interval will be used to schedule the operation. Once the timefilter reaches present wall-clock time, the offset parameter will determine an amount of time to add to the scheduled time, that is now period . This is to account for late incoming data to the database. Example def period = 1m def sql = 'SELECT ts, id, temp1 FROM doc.table WHERE $__timefilter AND stream_id = ' dd419f94834a ' ORDER BY ts ASC ' | crate_query_cont () . query ( sql ) . period ( period ) . start ( '2021-11-16T16:03:42.040000Z' ) The above example will execute the query periodically, emitting data_batch items with data_points worth of 1 minute. start will be aligned to period , so that the timefilter will look like this for the first query: ts >= '2021-11-16T16:03:00.000Z' AND ts < '2021-11-16T16:04:00.000Z' Parameters Parameter Description Default host( string ) CrateDB host from config port( integer ) CrateDB port from config user( string ) username from config pass( string ) password from config database( string ) Database name from config query( string ) 'SELECT' query with $__timefilter placeholder start( string ) timefilter start point .ISO8601 datetime string or query that retrieves an ISO8601 datetime string from the database stop( string ) timefilter stop point .ISO8601 datetime string or query that retrieves an ISO8601 datetime string from the database undefined stop_flow( boolean ) Whether to stop the whole flow, this node runs in, when stop time is reached. If this is false, then the node will just stop querying the database. true filter_time_field( string ) name of timestamp db column, used for timefiler 'ts' result_time_field( string ) name of result column, used for retrieving timestamps defaults to filter_time_field period( duration ) timefilter timespan, query boundaries will be aligned to this value 1h offset( duration ) offset at which the database is queried when the timefilter reached 'now' time 20s min_interval( duration ) minimum query-interval when the timefilter is in the past 5s","title":"Crate query cont"},{"location":"nodes/database/crate_query_cont.html#the-crate_query_cont-node","text":"[ experimental ] since 0.19.0 Query the CRATE database for time series data . This node is used for continous timeseries queries.","title":"The crate_query_cont node"},{"location":"nodes/database/crate_query_cont.html#query","text":"A select statement will be executed periodically, on every iteration a timefilter gets adjusted according to the period parameter. For this to work, the query given must contain the $__timefilter placeholder in the query's where clause: def query = ' SELECT ts, id, temp1 FROM doc.table WHERE $__timefilter AND stream_id = ' dd419f94834a ' ORDER BY ts ASC ' The timefilter placeholder gets replaced by this statement: ts >= $ 1 AND ts < $ 2","title":"Query"},{"location":"nodes/database/crate_query_cont.html#start","text":"The start parameter determines the query start time. It's value is a past point in time. There are two possible ways to provide this: provide an ISO8601 Datetimestamp, ie: '2021-11-16T17:15:00.000Z' provide a query that results in 1 row with exactly 1 column named 'ts' containing an ISO8601 Datetimestamp. SELECT DATE_FORMAT(ts) FROM table WHERE worked_on = false ORDER BY ts LIMIT 1 or with a fallback start-time SELECT COALESCE ( ( SELECT DATE_FORMAT(ts) FROM table WHERE worked_on = false ORDER BY ts LIMIT 1 ), '2021-11-16T16:20:00.000000Z' ) AS ts","title":"Start"},{"location":"nodes/database/crate_query_cont.html#historic-and-up-to-date-data","text":"While reading data from the past, min_interval will be used to schedule the operation. Once the timefilter reaches present wall-clock time, the offset parameter will determine an amount of time to add to the scheduled time, that is now period . This is to account for late incoming data to the database.","title":"Historic and up-to-date data"},{"location":"nodes/database/crate_query_cont.html#example","text":"def period = 1m def sql = 'SELECT ts, id, temp1 FROM doc.table WHERE $__timefilter AND stream_id = ' dd419f94834a ' ORDER BY ts ASC ' | crate_query_cont () . query ( sql ) . period ( period ) . start ( '2021-11-16T16:03:42.040000Z' ) The above example will execute the query periodically, emitting data_batch items with data_points worth of 1 minute. start will be aligned to period , so that the timefilter will look like this for the first query: ts >= '2021-11-16T16:03:00.000Z' AND ts < '2021-11-16T16:04:00.000Z'","title":"Example"},{"location":"nodes/database/crate_query_cont.html#parameters","text":"Parameter Description Default host( string ) CrateDB host from config port( integer ) CrateDB port from config user( string ) username from config pass( string ) password from config database( string ) Database name from config query( string ) 'SELECT' query with $__timefilter placeholder start( string ) timefilter start point .ISO8601 datetime string or query that retrieves an ISO8601 datetime string from the database stop( string ) timefilter stop point .ISO8601 datetime string or query that retrieves an ISO8601 datetime string from the database undefined stop_flow( boolean ) Whether to stop the whole flow, this node runs in, when stop time is reached. If this is false, then the node will just stop querying the database. true filter_time_field( string ) name of timestamp db column, used for timefiler 'ts' result_time_field( string ) name of result column, used for retrieving timestamps defaults to filter_time_field period( duration ) timefilter timespan, query boundaries will be aligned to this value 1h offset( duration ) offset at which the database is queried when the timefilter reached 'now' time 20s min_interval( duration ) minimum query-interval when the timefilter is in the past 5s","title":"Parameters"},{"location":"nodes/database/http_post_crate.html","text":"The http_post_crate node renamed to crate_out .","title":"Http post crate"},{"location":"nodes/database/http_post_crate.html#the-http_post_crate-node","text":"renamed to crate_out .","title":"The http_post_crate node"},{"location":"nodes/database/influx_out.html","text":"The influx_out node Write data to InfluxDB via it's HTTP API. This node supports InfluxDB up to version 1.8. If any errors occur during the request, the node will attempt to retry sending. Since FAXE and InfluxDB share the notion of tags , this node will write all fields to InfluxDB fields and all tags as Influx tags. If you want to control which fields and tags get written to the database, use one of the flowdata-nodes, ie. use delete to delete some fields and/or tags before writing data with this node. Note: it is recommended to batch single data-points. Note: This node is a sink node and does not output any flow-data, therefore any node connected to this node will not get any data. Example Simple: | influx_out () . host ( '127.0.0.1' ) . port ( 8086 ) . measurement ( 'm1' ) . database ( 'mydb' ) Use delete and batch before writing to InfluxDB: | delete() . fields ( 'calc.avg_temp' ) . tags ( 'is_on' , 'color' ) | batch( 25 ) . timeout ( 3s ) | influx_out () . host ( '127.0.0.1' ) . port ( 8086 ) . measurement ( 'm1' ) . database ( 'mydb' ) Parameters Parameter Description Default host( string ) hostname or ip address of endpoint from config file port( integer ) port number from config file user( string ) username from config file pass( string ) password from config file tls( is_set ) whether to use tls ie. https false (not set) database( string ) database name measurement( string ) measurement name retpol( string ) retention policy to write to default","title":"Influx out"},{"location":"nodes/database/influx_out.html#the-influx_out-node","text":"Write data to InfluxDB via it's HTTP API. This node supports InfluxDB up to version 1.8. If any errors occur during the request, the node will attempt to retry sending. Since FAXE and InfluxDB share the notion of tags , this node will write all fields to InfluxDB fields and all tags as Influx tags. If you want to control which fields and tags get written to the database, use one of the flowdata-nodes, ie. use delete to delete some fields and/or tags before writing data with this node. Note: it is recommended to batch single data-points. Note: This node is a sink node and does not output any flow-data, therefore any node connected to this node will not get any data.","title":"The influx_out node"},{"location":"nodes/database/influx_out.html#example","text":"Simple: | influx_out () . host ( '127.0.0.1' ) . port ( 8086 ) . measurement ( 'm1' ) . database ( 'mydb' ) Use delete and batch before writing to InfluxDB: | delete() . fields ( 'calc.avg_temp' ) . tags ( 'is_on' , 'color' ) | batch( 25 ) . timeout ( 3s ) | influx_out () . host ( '127.0.0.1' ) . port ( 8086 ) . measurement ( 'm1' ) . database ( 'mydb' )","title":"Example"},{"location":"nodes/database/influx_out.html#parameters","text":"Parameter Description Default host( string ) hostname or ip address of endpoint from config file port( integer ) port number from config file user( string ) username from config file pass( string ) password from config file tls( is_set ) whether to use tls ie. https false (not set) database( string ) database name measurement( string ) measurement name retpol( string ) retention policy to write to default","title":"Parameters"},{"location":"nodes/database/mongo_query.html","text":"The mongo_query node Experimental . Since 0.15.2 Read data from MongoDB. Database , collection and selector ( query ) can be specified by the user. The mongo_query node will always output data_batch items at the moment, even if there is only 1 result document. Note: Faxe is not optimized for heavy batch processing, it is rather designed for massive concurrent stream processing. If you want to process more than a few thousand rows at a time, maybe faxe is not the right tool for your job. Example def host = 'localhost' | mongo_query () . host ( host ) . user ( 'root' ) . pass ( 'root' ) . database ( 'test' ) . collection ( 'inventory' ) . query ( ' {\"item\": \"canvas\"} ' ) . every ( 5s ) . as ( 'data' ) Every 5 seconds query mongo db on database test and collection inventory for documents, that have \"canvas\" as their values for the \"item\" field. Sets root field to 'data'. def host = 'localhost' | value_emitter() . every ( 7s ) | mongo_query () . host ( host ) . user ( 'root' ) . pass ( 'root' ) . database ( 'test' ) . collection ( 'inventory' ) . query ( ' {\"size.h\": {\"$gt\": 16}} ' ) Every time a dataitem arrives at this node, from outside, it will query mongodb for documents that have a nested h field inside a size object, that has a value greater than 16. The node will query mongodb every 7 seconds, triggered by data coming in from the value_emitter node. Parameters Parameter Description Default host ( string ) host name or ip address port ( integer ) 27017 user ( string ) username pass ( string ) the users password database ( string ) mongo database name collection ( string ) mongo collection name in database database query ( string ) a valid mongo selector as a json string {} every ( duration ) read interval 5s align ( is_set ) whether to align the every parameter false (not set) as set the root path for outputs undefined","title":"Mongo query"},{"location":"nodes/database/mongo_query.html#the-mongo_query-node","text":"Experimental . Since 0.15.2 Read data from MongoDB. Database , collection and selector ( query ) can be specified by the user. The mongo_query node will always output data_batch items at the moment, even if there is only 1 result document. Note: Faxe is not optimized for heavy batch processing, it is rather designed for massive concurrent stream processing. If you want to process more than a few thousand rows at a time, maybe faxe is not the right tool for your job.","title":"The mongo_query node"},{"location":"nodes/database/mongo_query.html#example","text":"def host = 'localhost' | mongo_query () . host ( host ) . user ( 'root' ) . pass ( 'root' ) . database ( 'test' ) . collection ( 'inventory' ) . query ( ' {\"item\": \"canvas\"} ' ) . every ( 5s ) . as ( 'data' ) Every 5 seconds query mongo db on database test and collection inventory for documents, that have \"canvas\" as their values for the \"item\" field. Sets root field to 'data'. def host = 'localhost' | value_emitter() . every ( 7s ) | mongo_query () . host ( host ) . user ( 'root' ) . pass ( 'root' ) . database ( 'test' ) . collection ( 'inventory' ) . query ( ' {\"size.h\": {\"$gt\": 16}} ' ) Every time a dataitem arrives at this node, from outside, it will query mongodb for documents that have a nested h field inside a size object, that has a value greater than 16. The node will query mongodb every 7 seconds, triggered by data coming in from the value_emitter node.","title":"Example"},{"location":"nodes/database/mongo_query.html#parameters","text":"Parameter Description Default host ( string ) host name or ip address port ( integer ) 27017 user ( string ) username pass ( string ) the users password database ( string ) mongo database name collection ( string ) mongo collection name in database database query ( string ) a valid mongo selector as a json string {} every ( duration ) read interval 5s align ( is_set ) whether to align the every parameter false (not set) as set the root path for outputs undefined","title":"Parameters"},{"location":"nodes/database/oracle_query.html","text":"The oracle_query node Read data from OracleDB. Example def host = 'my.oracle.host' def port = 1521 def user = 'MY_ORACLE_USER' def password = 'MY_ORACLE_PASS' def service_name = 'MY.service' def query = ' select * from room order by room_number ' |or acle_query () . host ( host ) . port ( port ) . user ( user ) . pass ( password ) . service_name ( service_name ) . query ( query ) . every ( 10s ) . align () Parameters Parameter Description Default host ( string ) host name or ip address port ( integer ) 1521 user ( string ) username pass ( string ) the users password service_name ( string ) query ( string ) a valid sql select statement result_type ( string ) eighter 'batch' or 'point' 'batch' time_field ( string ) name of the time field every ( duration ) read interval 5s align ( is_set ) whether to align every false (not set)","title":"Oracle query"},{"location":"nodes/database/oracle_query.html#the-oracle_query-node","text":"Read data from OracleDB.","title":"The oracle_query node"},{"location":"nodes/database/oracle_query.html#example","text":"def host = 'my.oracle.host' def port = 1521 def user = 'MY_ORACLE_USER' def password = 'MY_ORACLE_PASS' def service_name = 'MY.service' def query = ' select * from room order by room_number ' |or acle_query () . host ( host ) . port ( port ) . user ( user ) . pass ( password ) . service_name ( service_name ) . query ( query ) . every ( 10s ) . align ()","title":"Example"},{"location":"nodes/database/oracle_query.html#parameters","text":"Parameter Description Default host ( string ) host name or ip address port ( integer ) 1521 user ( string ) username pass ( string ) the users password service_name ( string ) query ( string ) a valid sql select statement result_type ( string ) eighter 'batch' or 'point' 'batch' time_field ( string ) name of the time field every ( duration ) read interval 5s align ( is_set ) whether to align every false (not set)","title":"Parameters"},{"location":"nodes/debug/conn_status.html","text":"The conn_status node Subscribe to internal connection status events of running tasks. For more information on these events make yourself familiar with faxe's metrics . Example %% get connection status events for task \"flow1\" and node \"amqp_publish13\" | conn_status () . flow ( 'flow1' ) . node ( 'amqp_publish13' ) %% get connection status events for every node in task \"flow1\" | conn_status () . flow ( 'flow1' ) Parameters Parameter Description Default flow( string ) Id of task node( string ) Id of node undefined","title":"Conn status"},{"location":"nodes/debug/conn_status.html#the-conn_status-node","text":"Subscribe to internal connection status events of running tasks. For more information on these events make yourself familiar with faxe's metrics .","title":"The conn_status node"},{"location":"nodes/debug/conn_status.html#example","text":"%% get connection status events for task \"flow1\" and node \"amqp_publish13\" | conn_status () . flow ( 'flow1' ) . node ( 'amqp_publish13' ) %% get connection status events for every node in task \"flow1\" | conn_status () . flow ( 'flow1' )","title":"Example"},{"location":"nodes/debug/conn_status.html#parameters","text":"Parameter Description Default flow( string ) Id of task node( string ) Id of node undefined","title":"Parameters"},{"location":"nodes/debug/debug.html","text":"The debug node The debug node logs all incoming data with erlang's lager framework and emits it, without touching it. Where the logs will be written, depends on the lager config. The debug message will include the current data-item converted to a string. See rest api for how to read the produced logs. Example | debug() | debug( 'error' ) %% [since 0.19.13] | debug( 'warning' ) .where( lambda : empty ( \"data.topic\" ) OR empty ( \"data.stream_id\" )) . message ( 'Topic or StreamId is empty!' ) Parameters Parameter Description Default [node] level ( string ) log level (see below) 'notice' message ( string ) [since 0.19.13] custom message that is written to the log '' where ( lambda ) [since 0.19.13] lambda expression, if evaluates as true, then logging will be performed undefined The level parameter must have one of the following values: log_level debug info notice warning error critical alert","title":"Debug"},{"location":"nodes/debug/debug.html#the-debug-node","text":"The debug node logs all incoming data with erlang's lager framework and emits it, without touching it. Where the logs will be written, depends on the lager config. The debug message will include the current data-item converted to a string. See rest api for how to read the produced logs.","title":"The debug node"},{"location":"nodes/debug/debug.html#example","text":"| debug() | debug( 'error' ) %% [since 0.19.13] | debug( 'warning' ) .where( lambda : empty ( \"data.topic\" ) OR empty ( \"data.stream_id\" )) . message ( 'Topic or StreamId is empty!' )","title":"Example"},{"location":"nodes/debug/debug.html#parameters","text":"Parameter Description Default [node] level ( string ) log level (see below) 'notice' message ( string ) [since 0.19.13] custom message that is written to the log '' where ( lambda ) [since 0.19.13] lambda expression, if evaluates as true, then logging will be performed undefined The level parameter must have one of the following values: log_level debug info notice warning error critical alert","title":"Parameters"},{"location":"nodes/debug/json_emitter.html","text":"The json_emitter node This node is for debugging and testing/mocking purposes. It periodically emits one of the json strings given with the json parameter. This can be done randomly or in sequence. experimental since 0.19.9 : With the modify and modify_with parameters, certain fields in the json string(s) can be modified with every interval. Examples | json_emitter() . every ( 3s ) . json ( ' {\"condition\": {\"id\": 0, \"name\": \"OK\"}, \"condition_reason\": \"\", \"predicted_maintenance_time\": 1584246411783, \"vac_on_without_contact\": [1.2, 2.5, 4.33], \"vac_on_with_contact\": [5.6, 45.98, 7.012]} ' , ' {\"condition\": {\"id\": 1, \"name\": \"Warning\"}, \"condition_reason\": \"bad succer\", \"predicted_maintenance_time\": 1583246411783, \"vac_on_without_contact\": [0.2, 2.5, 8.01], \"vac_on_with_contact\": [6.001, 4.798, 7.012]} ' , ' {\"condition\": {\"id\": 2, \"name\": \"Error\"}, \"condition_reason\": \"something went really wrong!\", \"predicted_maintenance_time\": 1582246411783, \"vac_on_without_contact\": [0.5, 2.5, 0.44], \"vac_on_with_contact\": [2.06, 4.98, 2.901]} ' ) . select ( 'rand' ) | debug() Emit one of the three given json strings(selected randomly) every 3 seconds. | json_emitter() . every ( 3s ) . json ( ' {\"condition\": {\"id\": 0, \"name\": \"OK\"}, \"condition_reason\": \"\", \"predicted_maintenance_time\": 1584246411783, \"vac_on_without_contact\": [1.2, 2.5, 4.33], \"vac_on_with_contact\": [5.6, 45.98, 7.012]} ' , ' {\"condition\": {\"id\": 1, \"name\": \"Warning\"}, \"condition_reason\": \"bad succer\", \"predicted_maintenance_time\": 1583246411783, \"vac_on_without_contact\": [0.2, 2.5, 8.01], \"vac_on_with_contact\": [6.001, 4.798, 7.012]} ' , ' {\"condition\": {\"id\": 2, \"name\": \"Error\"}, \"condition_reason\": \"something went really wrong!\", \"predicted_maintenance_time\": 1582246411783, \"vac_on_without_contact\": [0.5, 2.5, 0.44], \"vac_on_with_contact\": [2.06, 4.98, 2.901]} ' ) . select ( 'seq' ) | debug() Select one of the json strings in sequence every 3 seconds starting with the first one. After the last json string has been selectect and outputted, start over with the first one again. Parameters Parameter Description Default every( duration ) emit interval 5s jitter( duration ) add time jitter to the values produced 0ms json( string_list ) list of json strings align( is_set ) align the time to the every param false (not set) select( string ) entry select mode, possible values: rand or seq , batch 'rand' modify( string_list ) list of fields, that should be modified undefined modify_with( lambda_list ) list of lambda expressions, that perform the modification on the fields given with modify undefined Note : modify and modify_with must have the same number of parameters, if given. select value Description Resulting data-item rand randomly selects one of the json structures data-point seq selects the json entries in sequence starting from top data-point batch selects all of the entries resulting in a batch of data-points data-batch","title":"Json emitter"},{"location":"nodes/debug/json_emitter.html#the-json_emitter-node","text":"This node is for debugging and testing/mocking purposes. It periodically emits one of the json strings given with the json parameter. This can be done randomly or in sequence. experimental since 0.19.9 : With the modify and modify_with parameters, certain fields in the json string(s) can be modified with every interval.","title":"The json_emitter node"},{"location":"nodes/debug/json_emitter.html#examples","text":"| json_emitter() . every ( 3s ) . json ( ' {\"condition\": {\"id\": 0, \"name\": \"OK\"}, \"condition_reason\": \"\", \"predicted_maintenance_time\": 1584246411783, \"vac_on_without_contact\": [1.2, 2.5, 4.33], \"vac_on_with_contact\": [5.6, 45.98, 7.012]} ' , ' {\"condition\": {\"id\": 1, \"name\": \"Warning\"}, \"condition_reason\": \"bad succer\", \"predicted_maintenance_time\": 1583246411783, \"vac_on_without_contact\": [0.2, 2.5, 8.01], \"vac_on_with_contact\": [6.001, 4.798, 7.012]} ' , ' {\"condition\": {\"id\": 2, \"name\": \"Error\"}, \"condition_reason\": \"something went really wrong!\", \"predicted_maintenance_time\": 1582246411783, \"vac_on_without_contact\": [0.5, 2.5, 0.44], \"vac_on_with_contact\": [2.06, 4.98, 2.901]} ' ) . select ( 'rand' ) | debug() Emit one of the three given json strings(selected randomly) every 3 seconds. | json_emitter() . every ( 3s ) . json ( ' {\"condition\": {\"id\": 0, \"name\": \"OK\"}, \"condition_reason\": \"\", \"predicted_maintenance_time\": 1584246411783, \"vac_on_without_contact\": [1.2, 2.5, 4.33], \"vac_on_with_contact\": [5.6, 45.98, 7.012]} ' , ' {\"condition\": {\"id\": 1, \"name\": \"Warning\"}, \"condition_reason\": \"bad succer\", \"predicted_maintenance_time\": 1583246411783, \"vac_on_without_contact\": [0.2, 2.5, 8.01], \"vac_on_with_contact\": [6.001, 4.798, 7.012]} ' , ' {\"condition\": {\"id\": 2, \"name\": \"Error\"}, \"condition_reason\": \"something went really wrong!\", \"predicted_maintenance_time\": 1582246411783, \"vac_on_without_contact\": [0.5, 2.5, 0.44], \"vac_on_with_contact\": [2.06, 4.98, 2.901]} ' ) . select ( 'seq' ) | debug() Select one of the json strings in sequence every 3 seconds starting with the first one. After the last json string has been selectect and outputted, start over with the first one again.","title":"Examples"},{"location":"nodes/debug/json_emitter.html#parameters","text":"Parameter Description Default every( duration ) emit interval 5s jitter( duration ) add time jitter to the values produced 0ms json( string_list ) list of json strings align( is_set ) align the time to the every param false (not set) select( string ) entry select mode, possible values: rand or seq , batch 'rand' modify( string_list ) list of fields, that should be modified undefined modify_with( lambda_list ) list of lambda expressions, that perform the modification on the fields given with modify undefined Note : modify and modify_with must have the same number of parameters, if given.","title":"Parameters"},{"location":"nodes/debug/json_emitter.html#select","text":"value Description Resulting data-item rand randomly selects one of the json structures data-point seq selects the json entries in sequence starting from top data-point batch selects all of the entries resulting in a batch of data-points data-batch","title":"select"},{"location":"nodes/debug/log.html","text":"The log node Log incoming data to a file in json format (line by line) Example | log( 'topics.txt' ) Parameters Parameter Description Default [node] file( string ) valid writeable filepath","title":"Log"},{"location":"nodes/debug/log.html#the-log-node","text":"Log incoming data to a file in json format (line by line)","title":"The log node"},{"location":"nodes/debug/log.html#example","text":"| log( 'topics.txt' )","title":"Example"},{"location":"nodes/debug/log.html#parameters","text":"Parameter Description Default [node] file( string ) valid writeable filepath","title":"Parameters"},{"location":"nodes/debug/metrics.html","text":"The metrics node Subscribe to internal metric events of running tasks. For more information on these events make yourself familiar with faxe's metrics . Note: It is not possible to subscribe to metrics for the task the metrics node belongs to. Example %% get all metrics for task \"flow1\" and node \"amqp_publish13\" | metrics () . flow ( 'flow1' ) . node ( 'amqp_publish13' ) %% get total number of bytes read and written for task \"flow32\" | metrics () . flow ( 'flow32' ) . metrics ( 'bytes_read' , 'bytes_sent' ) Parameters Parameter Description Default flow( string ) Id of task node( string ) Id of node undefined metrics( string_list ) List of metric_names undefined","title":"Metrics"},{"location":"nodes/debug/metrics.html#the-metrics-node","text":"Subscribe to internal metric events of running tasks. For more information on these events make yourself familiar with faxe's metrics . Note: It is not possible to subscribe to metrics for the task the metrics node belongs to.","title":"The metrics node"},{"location":"nodes/debug/metrics.html#example","text":"%% get all metrics for task \"flow1\" and node \"amqp_publish13\" | metrics () . flow ( 'flow1' ) . node ( 'amqp_publish13' ) %% get total number of bytes read and written for task \"flow32\" | metrics () . flow ( 'flow32' ) . metrics ( 'bytes_read' , 'bytes_sent' )","title":"Example"},{"location":"nodes/debug/metrics.html#parameters","text":"Parameter Description Default flow( string ) Id of task node( string ) Id of node undefined metrics( string_list ) List of metric_names undefined","title":"Parameters"},{"location":"nodes/debug/value_emitter.html","text":"The value_emitter node This node is for debugging purposes. It periodically emits random values. Example | value_emitter() . every ( 1s ) . type ( point ) Emit a data_point with a random value in field val every second. Parameters Parameter Description Default every( duration ) emit interval 5s jitter( duration ) add time jitter to the values produced 0ms type( atom ) emit point or batch batch fields( string_list ) what fields to emit ['val'] format( atom ) the format of the fields emitted flat/ejson flat align( is_set ) align the time to the every param false (not set)","title":"Value emitter"},{"location":"nodes/debug/value_emitter.html#the-value_emitter-node","text":"This node is for debugging purposes. It periodically emits random values.","title":"The value_emitter node"},{"location":"nodes/debug/value_emitter.html#example","text":"| value_emitter() . every ( 1s ) . type ( point ) Emit a data_point with a random value in field val every second.","title":"Example"},{"location":"nodes/debug/value_emitter.html#parameters","text":"Parameter Description Default every( duration ) emit interval 5s jitter( duration ) add time jitter to the values produced 0ms type( atom ) emit point or batch batch fields( string_list ) what fields to emit ['val'] format( atom ) the format of the fields emitted flat/ejson flat align( is_set ) align the time to the every param false (not set)","title":"Parameters"},{"location":"nodes/flow/collect_unique.html","text":"The collect_unique node Experimental . With this node we can collect a unique set of values from data_points based on a given field's value. For every different value of the key-field , the node will cache the last data_point with that value. This node is useful, if you have multiple data-streams - all share a field called the key-field - that you want to condense into one data_point, according to that key-field's value; One can think of it as sort of an \"un-group\" function. Note: This node produces a completely new data_point. Given the name of a key-field , this node collects data_points using the value of this field to group and cache every data_point. Once the min_vals count of unique values is reached in internal buffer, it starts emitting every change within this set of values. New data_points, which have the same value for the key-field as seen before, will overwrite old values. Data_points that do not have the key-field present, will be ignored. On output, the node will condense the collected data_points into one, where all the data_points' fields are grouped by the value of the key-field. Note: The number of uniquely collected values will grow, but never shrink (at the moment). Also note: Produced data may become very large, if the value of the key-field is ever-changing, so that the node will cache a lot of data and therefore may use a lot of memory, be aware of that ! Examples | collect_unique ( 'data.key_field' ) . min_vals ( 5 ) .keep( 'data.values.node_id' , 'data.values.request_ref' ) . as ( 'node_id' , 'request_ref' ) In the above example the node will collect the fields: \"data.values.node_id\" and \"data.values.request_ref\" from every data_point that has a field called \"data.key_field\". As soon as it has collected 5 different values for \"data.key_field\" it will emit those collected values as a new data_point with the two fields called \"node_id\" and \"request_ref\". Parameters Parameter Description Default [node] field( string ) path to the key-field min_vals( integer ) number of different items collected before first output starts 1 keep( string_list ) values to keep from every data_point as( string_list ) output names for the keep values [] max_age ( duration ) max age for every collected entry undefined","title":"Collect unique"},{"location":"nodes/flow/collect_unique.html#the-collect_unique-node","text":"Experimental . With this node we can collect a unique set of values from data_points based on a given field's value. For every different value of the key-field , the node will cache the last data_point with that value. This node is useful, if you have multiple data-streams - all share a field called the key-field - that you want to condense into one data_point, according to that key-field's value; One can think of it as sort of an \"un-group\" function. Note: This node produces a completely new data_point. Given the name of a key-field , this node collects data_points using the value of this field to group and cache every data_point. Once the min_vals count of unique values is reached in internal buffer, it starts emitting every change within this set of values. New data_points, which have the same value for the key-field as seen before, will overwrite old values. Data_points that do not have the key-field present, will be ignored. On output, the node will condense the collected data_points into one, where all the data_points' fields are grouped by the value of the key-field. Note: The number of uniquely collected values will grow, but never shrink (at the moment). Also note: Produced data may become very large, if the value of the key-field is ever-changing, so that the node will cache a lot of data and therefore may use a lot of memory, be aware of that !","title":"The collect_unique node"},{"location":"nodes/flow/collect_unique.html#examples","text":"| collect_unique ( 'data.key_field' ) . min_vals ( 5 ) .keep( 'data.values.node_id' , 'data.values.request_ref' ) . as ( 'node_id' , 'request_ref' ) In the above example the node will collect the fields: \"data.values.node_id\" and \"data.values.request_ref\" from every data_point that has a field called \"data.key_field\". As soon as it has collected 5 different values for \"data.key_field\" it will emit those collected values as a new data_point with the two fields called \"node_id\" and \"request_ref\".","title":"Examples"},{"location":"nodes/flow/collect_unique.html#parameters","text":"Parameter Description Default [node] field( string ) path to the key-field min_vals( integer ) number of different items collected before first output starts 1 keep( string_list ) values to keep from every data_point as( string_list ) output names for the keep values [] max_age ( duration ) max age for every collected entry undefined","title":"Parameters"},{"location":"nodes/flow/combine.html","text":"The combine node Combine the values from 2 nodes, used to enrich a stream of data with data from another stream, that usually has lower frequency. Port 1 is the trigger port and its the port where data to be enriched comes into the node. Port 2 is the one where enrichment data come in. Every time a value is received on the trigger port, the node will emit a value, combined with whatever current value on port 2. The node will never emit on port 2 values. No output is given, as long as there has not arrived a value on port 2 to combine with. fields The fields parameter defines the fields to inject into the combination for the stream on port 2. To rename these fields, parameter prefix or aliases can be used. With prefix_delimiter a delimiter can be given, defaults to: '_' merge When merge_field is given, the node will merge the values from the input port 2 with the values from port 1. Objects and lists and lists of objects will be merged. If a path exists in both streams, the value in the first stream is superseded by the value in the second stream (in2). Except for lists, which will be combined. Either fields(optionally with prefix or aliases ) or merge_field must be given. If you want to join 2 or more streams consider using the join node . Examples def in1 = | value_emitter() . every ( 500ms ) . type ( point ) . fields ( 'val' ) def in2 = | value_emitter() . every ( 4s ) . type ( point ) . fields ( 'val2' , 'val3' ) in1 | combine( in2 ) . fields ( 'val2' , 'val3' ) . prefix ( 'comb' ) . prefix_delimiter ( '_' ) In this example values from the stream called in1 will be enriched with values from in2 . Outputfields will be called: val , comb_val2 and comb_val3 . The flow will emit every 500 milliseconds after 4 seconds have past initially. def in1 = | value_emitter() . every ( 500ms ) . type ( point ) . fields ( 'data.val' ) def in2 = | value_emitter() . every ( 4s ) . type ( point ) . fields ( 'data.val2' , 'data.val3' ) in1 | combine( in2 ) . merge_field ( 'data' ) This example will merge data from in2 into in1 , such that the resulting data-point will have the fields: data.val , data.val2 , data.val3 def v1 = | json_emitter() . every ( 1s ) . json ( ' {\"condition\": {\"id\": 0, \"name\": \"OK\", \"sub_cond\": [{\"value\": 33}]}, \"condition_reason\": \"\", \"predicted_maintenance_time\": 1584246411783, \"vac_on_without_contact\": [1.2, 2.5, 4.33]} ' ) def v2 = | json_emitter() . every ( 5s ) . json ( ' {\"condition\": {\"id1\": 0, \"name1\": \"OK\", \"sub_cond\": [{\"number\": 44}]}, \"condition_reason\": \"\", \"predicted_maintenance_time\": 1584246411785, \"vac_on_without_contact\": [2.2, 2.5, 4.33], \"vac_on_with_contact\": [5.6, 45.98, 7.012]} ' ) v1 | combine( v2 ) . merge_field ( 'data' ) The output from the above example will be: { \"data\" : { \"condition\" : { \"id1\" : 0 , \"id\" : 0 , \"name1\" : \"OK\" , \"name\" : \"OK\" , \"sub_cond\" :[{ \"number\" : 44 },{ \"value\" : 33 }]}, \"condition_reason\" : \"\" , \"predicted_maintenance_time\" : 1584246411785 , \"vac_on_without_contact\" :[ 1.2 , 2.2 , 2.5 , 2.5 , 4.33 , 4.33 ], \"vac_on_with_contact\" :[ 5.6 , 45.98 , 7.012 ] } Parameters Parameter Description Default [node] ( port ) input node for port 2 merge_field( string ) Base field for the merge operation [] fields( string_list ) List of fields to include [] tags( string_list ) List of tags to include [] aliases( string_list ) List of field aliases to use instead of the original field names [] prefix( string ) Prefix for the injected fields from stream 2 undefined prefix_delimiter( string ) Used to separate prefix and the original field name from stream 2 '_' nofill ( isset ) if set, dataoutput will happen regardless of a initial input on port 2 false (not set) When merge_field is given the params fields, prefix and prefix_delimiter have no effect. Otherwise either prefix or aliases must be given these are mutually exclusive parameters. If both are given, then prefix will win.","title":"Combine"},{"location":"nodes/flow/combine.html#the-combine-node","text":"Combine the values from 2 nodes, used to enrich a stream of data with data from another stream, that usually has lower frequency. Port 1 is the trigger port and its the port where data to be enriched comes into the node. Port 2 is the one where enrichment data come in. Every time a value is received on the trigger port, the node will emit a value, combined with whatever current value on port 2. The node will never emit on port 2 values. No output is given, as long as there has not arrived a value on port 2 to combine with.","title":"The combine node"},{"location":"nodes/flow/combine.html#fields","text":"The fields parameter defines the fields to inject into the combination for the stream on port 2. To rename these fields, parameter prefix or aliases can be used. With prefix_delimiter a delimiter can be given, defaults to: '_'","title":"fields"},{"location":"nodes/flow/combine.html#merge","text":"When merge_field is given, the node will merge the values from the input port 2 with the values from port 1. Objects and lists and lists of objects will be merged. If a path exists in both streams, the value in the first stream is superseded by the value in the second stream (in2). Except for lists, which will be combined. Either fields(optionally with prefix or aliases ) or merge_field must be given. If you want to join 2 or more streams consider using the join node .","title":"merge"},{"location":"nodes/flow/combine.html#examples","text":"def in1 = | value_emitter() . every ( 500ms ) . type ( point ) . fields ( 'val' ) def in2 = | value_emitter() . every ( 4s ) . type ( point ) . fields ( 'val2' , 'val3' ) in1 | combine( in2 ) . fields ( 'val2' , 'val3' ) . prefix ( 'comb' ) . prefix_delimiter ( '_' ) In this example values from the stream called in1 will be enriched with values from in2 . Outputfields will be called: val , comb_val2 and comb_val3 . The flow will emit every 500 milliseconds after 4 seconds have past initially. def in1 = | value_emitter() . every ( 500ms ) . type ( point ) . fields ( 'data.val' ) def in2 = | value_emitter() . every ( 4s ) . type ( point ) . fields ( 'data.val2' , 'data.val3' ) in1 | combine( in2 ) . merge_field ( 'data' ) This example will merge data from in2 into in1 , such that the resulting data-point will have the fields: data.val , data.val2 , data.val3 def v1 = | json_emitter() . every ( 1s ) . json ( ' {\"condition\": {\"id\": 0, \"name\": \"OK\", \"sub_cond\": [{\"value\": 33}]}, \"condition_reason\": \"\", \"predicted_maintenance_time\": 1584246411783, \"vac_on_without_contact\": [1.2, 2.5, 4.33]} ' ) def v2 = | json_emitter() . every ( 5s ) . json ( ' {\"condition\": {\"id1\": 0, \"name1\": \"OK\", \"sub_cond\": [{\"number\": 44}]}, \"condition_reason\": \"\", \"predicted_maintenance_time\": 1584246411785, \"vac_on_without_contact\": [2.2, 2.5, 4.33], \"vac_on_with_contact\": [5.6, 45.98, 7.012]} ' ) v1 | combine( v2 ) . merge_field ( 'data' ) The output from the above example will be: { \"data\" : { \"condition\" : { \"id1\" : 0 , \"id\" : 0 , \"name1\" : \"OK\" , \"name\" : \"OK\" , \"sub_cond\" :[{ \"number\" : 44 },{ \"value\" : 33 }]}, \"condition_reason\" : \"\" , \"predicted_maintenance_time\" : 1584246411785 , \"vac_on_without_contact\" :[ 1.2 , 2.2 , 2.5 , 2.5 , 4.33 , 4.33 ], \"vac_on_with_contact\" :[ 5.6 , 45.98 , 7.012 ] }","title":"Examples"},{"location":"nodes/flow/combine.html#parameters","text":"Parameter Description Default [node] ( port ) input node for port 2 merge_field( string ) Base field for the merge operation [] fields( string_list ) List of fields to include [] tags( string_list ) List of tags to include [] aliases( string_list ) List of field aliases to use instead of the original field names [] prefix( string ) Prefix for the injected fields from stream 2 undefined prefix_delimiter( string ) Used to separate prefix and the original field name from stream 2 '_' nofill ( isset ) if set, dataoutput will happen regardless of a initial input on port 2 false (not set) When merge_field is given the params fields, prefix and prefix_delimiter have no effect. Otherwise either prefix or aliases must be given these are mutually exclusive parameters. If both are given, then prefix will win.","title":"Parameters"},{"location":"nodes/flow/group_by.html","text":"The group_by node The group_by node is used to group a stream of data by the values of one or more fields. Each group is then processed independently and concurrently to the other groups, for the rest of the chain (subgraph). Note: The behaviour of using more than 1 group_by node within a flow is not defined. See group_union for how to 'un-group' a dataflow. Be aware of high group cardinality, as for every group, a number of processes (depends on the size of the grouped sub-flow) will be started in the dataflow engine. In other words, if you have a grouping that has high cardinality (many different values), more resources will be consumed. Examples | group_by ( 'fieldname1' , 'fieldname2' ) Groups data along two dimensions: fieldname1 and fieldname2 . def group_field = 'data.code' | json_emitter() . every ( 700ms ) . json ( '{\"code\" : 224, \"message\": \"this is a test\", \"mode\": 1}' , '{\"code\" : 334, \"message\": \"this is another test\", \"mode\": 1}' , '{\"code\" : 114, \"message\": \"this is another test\", \"mode\": 2}' , '{\"code\" : 443, \"message\": \"this is another test\", \"mode\": 1}' , '{\"code\" : 224, \"message\": \"this is another test\", \"mode\": 2}' , '{\"code\" : 111, \"message\": \"this is another test\", \"mode\": 1}' , '{\"code\" : 551, \"message\": \"this is another test\", \"mode\": 2}' ) . as ( 'data' ) | group_by ( group_field ) | eval( lambda : str_replace ( \"data.message\" , 'test' , string ( \"data.code\" )) ) . as ( 'data.message' ) | debug() In the above example data is grouped by the code field, so eventually the group_by node will start 6 computing sub-graphs to handle all groups. A computing sub-graph in this example will contain an eval node connected to a debug node. Parameters Parameter Description Default [node] fields( string_list ) fieldnames to group by lambda( lambda ) use a function to group the data-items (experimental) undefined","title":"Group by"},{"location":"nodes/flow/group_by.html#the-group_by-node","text":"The group_by node is used to group a stream of data by the values of one or more fields. Each group is then processed independently and concurrently to the other groups, for the rest of the chain (subgraph). Note: The behaviour of using more than 1 group_by node within a flow is not defined. See group_union for how to 'un-group' a dataflow. Be aware of high group cardinality, as for every group, a number of processes (depends on the size of the grouped sub-flow) will be started in the dataflow engine. In other words, if you have a grouping that has high cardinality (many different values), more resources will be consumed.","title":"The group_by node"},{"location":"nodes/flow/group_by.html#examples","text":"| group_by ( 'fieldname1' , 'fieldname2' ) Groups data along two dimensions: fieldname1 and fieldname2 . def group_field = 'data.code' | json_emitter() . every ( 700ms ) . json ( '{\"code\" : 224, \"message\": \"this is a test\", \"mode\": 1}' , '{\"code\" : 334, \"message\": \"this is another test\", \"mode\": 1}' , '{\"code\" : 114, \"message\": \"this is another test\", \"mode\": 2}' , '{\"code\" : 443, \"message\": \"this is another test\", \"mode\": 1}' , '{\"code\" : 224, \"message\": \"this is another test\", \"mode\": 2}' , '{\"code\" : 111, \"message\": \"this is another test\", \"mode\": 1}' , '{\"code\" : 551, \"message\": \"this is another test\", \"mode\": 2}' ) . as ( 'data' ) | group_by ( group_field ) | eval( lambda : str_replace ( \"data.message\" , 'test' , string ( \"data.code\" )) ) . as ( 'data.message' ) | debug() In the above example data is grouped by the code field, so eventually the group_by node will start 6 computing sub-graphs to handle all groups. A computing sub-graph in this example will contain an eval node connected to a debug node.","title":"Examples"},{"location":"nodes/flow/group_by.html#parameters","text":"Parameter Description Default [node] fields( string_list ) fieldnames to group by lambda( lambda ) use a function to group the data-items (experimental) undefined","title":"Parameters"},{"location":"nodes/flow/group_union.html","text":"The group_union node This node is used to terminate and 'un-group' a grouped dataflow. It must be placed after a group_by node. The group_union node acts as a union for grouped dataflows. If this node is used without a group_by node, it will have no effect at all on the data-flow. Note: The behaviour of using more than 1 group_union node within a flow is not defined. See group_by Examples | group_by ( 'fieldname1' ) %% 1 debug-node per group | debug() | group_union () %% end of grouping | debug() An instance of the first debug node wil be started for every group, the second one will exist only once.","title":"Group union"},{"location":"nodes/flow/group_union.html#the-group_union-node","text":"This node is used to terminate and 'un-group' a grouped dataflow. It must be placed after a group_by node. The group_union node acts as a union for grouped dataflows. If this node is used without a group_by node, it will have no effect at all on the data-flow. Note: The behaviour of using more than 1 group_union node within a flow is not defined. See group_by","title":"The group_union node"},{"location":"nodes/flow/group_union.html#examples","text":"| group_by ( 'fieldname1' ) %% 1 debug-node per group | debug() | group_union () %% end of grouping | debug() An instance of the first debug node wil be started for every group, the second one will exist only once.","title":"Examples"},{"location":"nodes/flow/join.html","text":"The join node Join data from two or more nodes. If the merge_field parameter is given, the node will merge the fields given from every in-node, instead of joining with prefixes. (See merge example below). If you want to enrich a stream of data with a second stream consider using the combine node . Example def v1 = | value_emitter() . every ( 3s ) . align () def v2 = | value_emitter() . every ( 5s ) . align () v1 | join( v2 ) . prefix ( 'v1.joined' , 'v2.joined' ) . tolerance ( 3s ) . missing_timeout ( 3s ) Joins the fields of v1 and v2 and produces a stream, that has the fields v1.joined.val and v2.joined.val Node Parameters Parameter Description Default [node] nodes( node_list ) list of node (chains) to merge [] Parameters Parameter Description Default prefix( string_list ) list of prefixes (used in join mode) ['', ''] (no prefixes) merge_field( string ) when given, the join node will do a field merge operation undefined missing_timeout( duration ) values that do not arrive within this timeout will be treated as missing 20s tolerance( duration ) timestamp tolerance. Determines the maximum difference a data-item's timestamp can have to the current timeslot, for the item to be included in the join operation. 2s ~fill( 'none' 'null' any ) deprecated, use full instead, the nodes default behaviour stayed the same 'none' full( boolean ) whether to output full joins (no value missing) only, this would drop joins that are not complete true fill value - join behaviour (deprecated) 'none' - (default) skip rows, where a point is missing, inner join. 'null' - fill missing points with null, full outer join. Any value - fill fields with given value, full outer join. Merge example Let's look at an example where the streams coming out of two nodes are not joined with prefixes, but a merge operation is performed. def v1 = | json_emitter() . every ( 3s ) . json ( ' {\"condition\": {\"id\": 0, \"name\": \"OK\", \"sub_cond\": [{\"value\": 33}]}, \"condition_reason\": \"\", \"predicted_maintenance_time\": 1584246411783, \"vac_on_without_contact\": [1.2, 2.5, 4.33]} ' ) . as ( 'data' ) def v2 = | json_emitter() . every ( 3s ) . json ( ' {\"condition\": {\"id1\": 0, \"name1\": \"OK\", \"sub_cond\": [{\"number\": 44}]}, \"condition_reason\": \"\", \"predicted_maintenance_time\": 1584246411783, \"vac_on_without_contact\": [2.2, 2.5, 4.33], \"vac_on_with_contact\": [5.6, 45.98, 7.012]} ' ) . as ( 'data' ) v1 | join( v2 ) . merge_field ( 'data' ) . tolerance ( 20ms ) . missing_timeout ( 30ms ) . full ( false ) | debug() v1 node data-field (in json format for readability): { \"condition\" : { \"id\" : 0 , \"name\" : \"OK\" , \"sub_cond\" : [{ \"value\" : 33 }]}, \"condition_reason\" : \"Reason\" , \"predicted_maintenance_time\" : 1584246411783 , \"vac_on_without_contact\" : [ 1.2 , 2.5 , 4.33 ] } v2 node data-field: { \"condition\" : { \"id1\" : 0 , \"name1\" : \"OK\" , \"sub_cond\" : [{ \"number\" : 44 }]}, \"condition_reason\" : \"\" , \"predicted_maintenance_time\" : 1584246411785 , \"vac_on_without_contact\" : [ 2.2 , 2.5 , 4.33 ], \"vac_on_with_contact\" : [ 5.6 , 45.98 , 7.012 ] } The result data-field after merge (json format here): { \"condition\" : { \"name1\" : \"OK\" , \"name\" : \"OK\" , \"id1\" : 0 , \"id\" : 0 , \"sub_cond\" :[{ \"number\" : 44 }, { \"value\" : 33 }] }, \"predicted_maintenance_time\" : 1584246411785 , \"vac_on_without_contact\" :[ 1.2 , 2.2 , 2.5 , 2.5 , 4.33 , 4.33 ], \"vac_on_with_contact\" :[ 5.6 , 45.98 , 7.012 ], \"condition_reason\" : \"\" } Objects and lists and lists of objects will be merged. If a path exists in several streams, the value in the first stream is superseded by the value in a following stream (\"condition_reason\" and \"predicted_maintenance_time\" in this example). Except for lists, which will be merged (\"vac_on_without_contact\").","title":"Join"},{"location":"nodes/flow/join.html#the-join-node","text":"Join data from two or more nodes. If the merge_field parameter is given, the node will merge the fields given from every in-node, instead of joining with prefixes. (See merge example below). If you want to enrich a stream of data with a second stream consider using the combine node .","title":"The join node"},{"location":"nodes/flow/join.html#example","text":"def v1 = | value_emitter() . every ( 3s ) . align () def v2 = | value_emitter() . every ( 5s ) . align () v1 | join( v2 ) . prefix ( 'v1.joined' , 'v2.joined' ) . tolerance ( 3s ) . missing_timeout ( 3s ) Joins the fields of v1 and v2 and produces a stream, that has the fields v1.joined.val and v2.joined.val","title":"Example"},{"location":"nodes/flow/join.html#node-parameters","text":"Parameter Description Default [node] nodes( node_list ) list of node (chains) to merge []","title":"Node Parameters"},{"location":"nodes/flow/join.html#parameters","text":"Parameter Description Default prefix( string_list ) list of prefixes (used in join mode) ['', ''] (no prefixes) merge_field( string ) when given, the join node will do a field merge operation undefined missing_timeout( duration ) values that do not arrive within this timeout will be treated as missing 20s tolerance( duration ) timestamp tolerance. Determines the maximum difference a data-item's timestamp can have to the current timeslot, for the item to be included in the join operation. 2s ~fill( 'none' 'null' any ) deprecated, use full instead, the nodes default behaviour stayed the same 'none' full( boolean ) whether to output full joins (no value missing) only, this would drop joins that are not complete true","title":"Parameters"},{"location":"nodes/flow/join.html#fill-value-join-behaviour-deprecated","text":"'none' - (default) skip rows, where a point is missing, inner join. 'null' - fill missing points with null, full outer join. Any value - fill fields with given value, full outer join.","title":"fill value - join behaviour (deprecated)"},{"location":"nodes/flow/join.html#merge-example","text":"Let's look at an example where the streams coming out of two nodes are not joined with prefixes, but a merge operation is performed. def v1 = | json_emitter() . every ( 3s ) . json ( ' {\"condition\": {\"id\": 0, \"name\": \"OK\", \"sub_cond\": [{\"value\": 33}]}, \"condition_reason\": \"\", \"predicted_maintenance_time\": 1584246411783, \"vac_on_without_contact\": [1.2, 2.5, 4.33]} ' ) . as ( 'data' ) def v2 = | json_emitter() . every ( 3s ) . json ( ' {\"condition\": {\"id1\": 0, \"name1\": \"OK\", \"sub_cond\": [{\"number\": 44}]}, \"condition_reason\": \"\", \"predicted_maintenance_time\": 1584246411783, \"vac_on_without_contact\": [2.2, 2.5, 4.33], \"vac_on_with_contact\": [5.6, 45.98, 7.012]} ' ) . as ( 'data' ) v1 | join( v2 ) . merge_field ( 'data' ) . tolerance ( 20ms ) . missing_timeout ( 30ms ) . full ( false ) | debug()","title":"Merge example"},{"location":"nodes/flow/join.html#v1-node-data-field-in-json-format-for-readability","text":"{ \"condition\" : { \"id\" : 0 , \"name\" : \"OK\" , \"sub_cond\" : [{ \"value\" : 33 }]}, \"condition_reason\" : \"Reason\" , \"predicted_maintenance_time\" : 1584246411783 , \"vac_on_without_contact\" : [ 1.2 , 2.5 , 4.33 ] }","title":"v1 node data-field (in json format for readability):"},{"location":"nodes/flow/join.html#v2-node-data-field","text":"{ \"condition\" : { \"id1\" : 0 , \"name1\" : \"OK\" , \"sub_cond\" : [{ \"number\" : 44 }]}, \"condition_reason\" : \"\" , \"predicted_maintenance_time\" : 1584246411785 , \"vac_on_without_contact\" : [ 2.2 , 2.5 , 4.33 ], \"vac_on_with_contact\" : [ 5.6 , 45.98 , 7.012 ] } The result data-field after merge (json format here): { \"condition\" : { \"name1\" : \"OK\" , \"name\" : \"OK\" , \"id1\" : 0 , \"id\" : 0 , \"sub_cond\" :[{ \"number\" : 44 }, { \"value\" : 33 }] }, \"predicted_maintenance_time\" : 1584246411785 , \"vac_on_without_contact\" :[ 1.2 , 2.2 , 2.5 , 2.5 , 4.33 , 4.33 ], \"vac_on_with_contact\" :[ 5.6 , 45.98 , 7.012 ], \"condition_reason\" : \"\" } Objects and lists and lists of objects will be merged. If a path exists in several streams, the value in the first stream is superseded by the value in a following stream (\"condition_reason\" and \"predicted_maintenance_time\" in this example). Except for lists, which will be merged (\"vac_on_without_contact\").","title":"v2 node data-field:"},{"location":"nodes/flow/union.html","text":"The union node Union of multiple streams. The union node takes the union of all of its parents as a simple pass through. Data items received from each parent are passed onto child nodes without modification. Example in1 | union( in2 , in3 ) The union of 3 nodes (chain expressions) def in1 = | mqtt_subscribe() . host ... def in2 = | amqp_consume() . host .... def in3 = ... in1 | union( in2 , in3 ) with chain expressions Parameters Parameter Description Default [node] nodes_in( node_list ) optional","title":"Union"},{"location":"nodes/flow/union.html#the-union-node","text":"Union of multiple streams. The union node takes the union of all of its parents as a simple pass through. Data items received from each parent are passed onto child nodes without modification.","title":"The union node"},{"location":"nodes/flow/union.html#example","text":"in1 | union( in2 , in3 ) The union of 3 nodes (chain expressions) def in1 = | mqtt_subscribe() . host ... def in2 = | amqp_consume() . host .... def in3 = ... in1 | union( in2 , in3 ) with chain expressions","title":"Example"},{"location":"nodes/flow/union.html#parameters","text":"Parameter Description Default [node] nodes_in( node_list ) optional","title":"Parameters"},{"location":"nodes/flowdata/batch.html","text":"The batch node Used to batch a number of points. As soon as the node has collected size number of points it will emit them in a data_batch. A timeout can be set, after which all points currently in the buffer will be emitted, regardless of the number of collected points. The timeout is started on the first datapoint coming in to an empty buffer. Example | batch( 12 ) | batch( 5 ) . timeout ( 3s ) The second example will output a batch with a maximum of 5 points. A data-batch message will be emitted, if either 5 points have been collected or 3 seconds have past since the first data-point came in. Parameters Parameter Description Default [node] size( integer ) Number of points to batch timeout( duration ) 1h","title":"Batch"},{"location":"nodes/flowdata/batch.html#the-batch-node","text":"Used to batch a number of points. As soon as the node has collected size number of points it will emit them in a data_batch. A timeout can be set, after which all points currently in the buffer will be emitted, regardless of the number of collected points. The timeout is started on the first datapoint coming in to an empty buffer.","title":"The batch node"},{"location":"nodes/flowdata/batch.html#example","text":"| batch( 12 ) | batch( 5 ) . timeout ( 3s ) The second example will output a batch with a maximum of 5 points. A data-batch message will be emitted, if either 5 points have been collected or 3 seconds have past since the first data-point came in.","title":"Example"},{"location":"nodes/flowdata/batch.html#parameters","text":"Parameter Description Default [node] size( integer ) Number of points to batch timeout( duration ) 1h","title":"Parameters"},{"location":"nodes/flowdata/default.html","text":"The default node Add fields and/or tags to a data_point or batch if they do not already exist. Does not overwrite or update any fields or tags. Note: This nodes checks for existence of fields before writing them. Consider using the set node , if you just want some fields set. It is more performant especially with high frequency data streams. Examples | default() . fields ( 'id' , 'vs' , 'df' ) . field_values ( 'some_id' , 1 , '05.043' ) The above example will set the field id to the value 'some_id' , if a field with the name id does not already exist. Accordingly vs will be set to 1, df will be set to '05.043'. | default() . fields ( 'id' , 'vs' , 'df' ) . field_values ( 25.44 ) Since v0.19.41: If exactly 1 value is given for field_values , it is used for every field name given. Parameters Parameter Description Default fields( string_list ) list of fieldnames [] field_values( list ) list of values for the given fields (exactly one entry or same length as fieldnames) [] tags( string_list ) list of tagnames [] tag_values( list ) list of values for the given tags (exactly one or same length as tagnames) []","title":"Default"},{"location":"nodes/flowdata/default.html#the-default-node","text":"Add fields and/or tags to a data_point or batch if they do not already exist. Does not overwrite or update any fields or tags. Note: This nodes checks for existence of fields before writing them. Consider using the set node , if you just want some fields set. It is more performant especially with high frequency data streams.","title":"The default node"},{"location":"nodes/flowdata/default.html#examples","text":"| default() . fields ( 'id' , 'vs' , 'df' ) . field_values ( 'some_id' , 1 , '05.043' ) The above example will set the field id to the value 'some_id' , if a field with the name id does not already exist. Accordingly vs will be set to 1, df will be set to '05.043'. | default() . fields ( 'id' , 'vs' , 'df' ) . field_values ( 25.44 ) Since v0.19.41: If exactly 1 value is given for field_values , it is used for every field name given.","title":"Examples"},{"location":"nodes/flowdata/default.html#parameters","text":"Parameter Description Default fields( string_list ) list of fieldnames [] field_values( list ) list of values for the given fields (exactly one entry or same length as fieldnames) [] tags( string_list ) list of tagnames [] tag_values( list ) list of values for the given tags (exactly one or same length as tagnames) []","title":"Parameters"},{"location":"nodes/flowdata/delete.html","text":"The delete node Delete fields and/or tags from a data_point or from all data_points in a data_batch. Optionally a lambda function can be given to perform deletion only on those data_points for which the function returns true . Example | delete() . fields ( 'temp' , 'data.meta[3]' ) The above example will delete the field named temp and the third array entry of the field data.meta . Conditional | delete() . fields ( 'temp' , 'data.meta[3]' ) .where( lambda : \"data.condition.id\" == 2 OR \"data.condition.name\" == 'warning' ) The above example will delete the field named temp and the third array entry of the field data.meta only if the current point has a field \"data.condition.id\" with value 2, or it has the field \"data.condition.name\" with value 'warning'. Parameters Parameter Description Default fields( string_list ) list of fieldnames to delete [] tags( string_list ) list of tagnames to delete [] where( lambda ) lambda function for conditional deleting undefined","title":"Delete"},{"location":"nodes/flowdata/delete.html#the-delete-node","text":"Delete fields and/or tags from a data_point or from all data_points in a data_batch. Optionally a lambda function can be given to perform deletion only on those data_points for which the function returns true .","title":"The delete node"},{"location":"nodes/flowdata/delete.html#example","text":"| delete() . fields ( 'temp' , 'data.meta[3]' ) The above example will delete the field named temp and the third array entry of the field data.meta .","title":"Example"},{"location":"nodes/flowdata/delete.html#conditional","text":"| delete() . fields ( 'temp' , 'data.meta[3]' ) .where( lambda : \"data.condition.id\" == 2 OR \"data.condition.name\" == 'warning' ) The above example will delete the field named temp and the third array entry of the field data.meta only if the current point has a field \"data.condition.id\" with value 2, or it has the field \"data.condition.name\" with value 'warning'.","title":"Conditional"},{"location":"nodes/flowdata/delete.html#parameters","text":"Parameter Description Default fields( string_list ) list of fieldnames to delete [] tags( string_list ) list of tagnames to delete [] where( lambda ) lambda function for conditional deleting undefined","title":"Parameters"},{"location":"nodes/flowdata/keep.html","text":"The keep node Keep only those fields and tags specified by the parameters. Example | keep( 'data.topic' , 'data.temperature' ) . as ( 'topic' , 'temperature' ) Parameters Parameter Description Default [node] fields( string_list ) list of fieldnames to keep from the incoming data tags( string_list ) list of tagnames to keep from the incoming data [] as( string_list ) list of new field names for the kept fields, if given, must have the same count of names as fields []","title":"Keep"},{"location":"nodes/flowdata/keep.html#the-keep-node","text":"Keep only those fields and tags specified by the parameters.","title":"The keep node"},{"location":"nodes/flowdata/keep.html#example","text":"| keep( 'data.topic' , 'data.temperature' ) . as ( 'topic' , 'temperature' )","title":"Example"},{"location":"nodes/flowdata/keep.html#parameters","text":"Parameter Description Default [node] fields( string_list ) list of fieldnames to keep from the incoming data tags( string_list ) list of tagnames to keep from the incoming data [] as( string_list ) list of new field names for the kept fields, if given, must have the same count of names as fields []","title":"Parameters"},{"location":"nodes/flowdata/rename.html","text":"The rename node Rename existing fields and/or tags. If a field or tag given does not exist, it will be ignored. Examples | rename() . fields ( 'topic' , 'temperature' ) . as_fields ( 'cipot' , 'mean_temp' ) A list of strings for the new field names is given. Since 0.19.4 : We can now also use lambda expressions for the as_fields parameter. | rename() . fields ( 'topic' , 'temperature' ) . as_fields ( 'cipot' , lambda : str_concat ( \"data.prefix\" , '_temp' )) Here we use a mixed list, strings and a lambda expression, for the as_fields parameter. Parameters Parameter Description Default fields( string_list ) list of fieldnames to rename [] as_fields( list ) list of strings or lambda expressions (can be mixed) for the new fieldnames [] tags( string_list ) list of tagnames to rename [] as_tags( string_list ) list of new tagnames for renaming []","title":"Rename"},{"location":"nodes/flowdata/rename.html#the-rename-node","text":"Rename existing fields and/or tags. If a field or tag given does not exist, it will be ignored.","title":"The rename node"},{"location":"nodes/flowdata/rename.html#examples","text":"| rename() . fields ( 'topic' , 'temperature' ) . as_fields ( 'cipot' , 'mean_temp' ) A list of strings for the new field names is given. Since 0.19.4 : We can now also use lambda expressions for the as_fields parameter. | rename() . fields ( 'topic' , 'temperature' ) . as_fields ( 'cipot' , lambda : str_concat ( \"data.prefix\" , '_temp' )) Here we use a mixed list, strings and a lambda expression, for the as_fields parameter.","title":"Examples"},{"location":"nodes/flowdata/rename.html#parameters","text":"Parameter Description Default fields( string_list ) list of fieldnames to rename [] as_fields( list ) list of strings or lambda expressions (can be mixed) for the new fieldnames [] tags( string_list ) list of tagnames to rename [] as_tags( string_list ) list of new tagnames for renaming []","title":"Parameters"},{"location":"nodes/flowdata/set.html","text":"The set node Set fields and/or tags to a data_point or batch. Overwrites any existing fields or tags. If fields or tags should be written only if they do not already exist, use the default node . Example | set() . fields ( 'id' , 'vs' , 'df' ) . field_values ( 'some_id' , 1 , '05.043' ) The above example will set the field id to the value 'some_id'. Accordingly vs will be set to 1, df will be set to '05.043'. Parameters Parameter Description Default fields( string_list ) list of fieldnames [] field_values( list ) list of values for the given fields (must have the same length as fieldnames) [] tags( string_list ) list of tagnames [] tag_values( list ) list of values for the given tags (must have the same length as tagnames) []","title":"Set"},{"location":"nodes/flowdata/set.html#the-set-node","text":"Set fields and/or tags to a data_point or batch. Overwrites any existing fields or tags. If fields or tags should be written only if they do not already exist, use the default node .","title":"The set node"},{"location":"nodes/flowdata/set.html#example","text":"| set() . fields ( 'id' , 'vs' , 'df' ) . field_values ( 'some_id' , 1 , '05.043' ) The above example will set the field id to the value 'some_id'. Accordingly vs will be set to 1, df will be set to '05.043'.","title":"Example"},{"location":"nodes/flowdata/set.html#parameters","text":"Parameter Description Default fields( string_list ) list of fieldnames [] field_values( list ) list of values for the given fields (must have the same length as fieldnames) [] tags( string_list ) list of tagnames [] tag_values( list ) list of values for the given tags (must have the same length as tagnames) []","title":"Parameters"},{"location":"nodes/http/http_get.html","text":"The http_get node Since 0.16.0 Requests data from a specified HTTP(s) endpoint via the GET method. If any errors occur during the request, the node will attempt to retry sending. Request are made periodically, if every is given and/or triggered via incoming data-items. Example | http_get () . host ( '127.0.0.1' ) . port ( 8081 ) . path ( '/v1/stats/faxe' ) . every ( 4s ) . align () . as ( 'get_response' ) Sends a GET request every 4 seconds to the specified host with the URI path /v1/stats/faxe . Response data will be interpreted as JSON and 'get_response' will be the root object for the resulting data-point. Example in json format Http response body: { \"id\" : 2233 , \"name\" : \"takahashu\" , \"mode\" : 123 } Resulting data-point { \"ts\" : 1629812164152 , \"get_response\" : { \"id\" : 2233 , \"name\" : \"takahashu\" , \"mode\" : 123 } } Without the as parameter, the resulting data-point would be: { \"ts\" : 1629812164152 , \"id\" : 2233 , \"name\" : \"takahashu\" , \"mode\" : 123 } Parameters Parameter Description Default host( string ) hostname or ip address of endpoint port( integer ) port number 80 tls( is_set ) whether to use tls ie. https false (not set) user( string ) username for Basic Authentication undefined pass( string ) password for Basic Authentication undefined path( string ) URI path of the http endpoint '/' every( duration ) interval at which requests are made undefined align( is_set ) align read intervals according to every false (not set) payload_type ( string ) how to interpert the response body, 'json' or 'plain' 'json' retries( integer ) number of retries, if request failed 2 as( string ) Root-path for the resulting data-point. If not given and payload-type is plain , defaults to 'data'. undefined","title":"Http get"},{"location":"nodes/http/http_get.html#the-http_get-node","text":"Since 0.16.0 Requests data from a specified HTTP(s) endpoint via the GET method. If any errors occur during the request, the node will attempt to retry sending. Request are made periodically, if every is given and/or triggered via incoming data-items.","title":"The http_get node"},{"location":"nodes/http/http_get.html#example","text":"| http_get () . host ( '127.0.0.1' ) . port ( 8081 ) . path ( '/v1/stats/faxe' ) . every ( 4s ) . align () . as ( 'get_response' ) Sends a GET request every 4 seconds to the specified host with the URI path /v1/stats/faxe . Response data will be interpreted as JSON and 'get_response' will be the root object for the resulting data-point. Example in json format Http response body: { \"id\" : 2233 , \"name\" : \"takahashu\" , \"mode\" : 123 } Resulting data-point { \"ts\" : 1629812164152 , \"get_response\" : { \"id\" : 2233 , \"name\" : \"takahashu\" , \"mode\" : 123 } } Without the as parameter, the resulting data-point would be: { \"ts\" : 1629812164152 , \"id\" : 2233 , \"name\" : \"takahashu\" , \"mode\" : 123 }","title":"Example"},{"location":"nodes/http/http_get.html#parameters","text":"Parameter Description Default host( string ) hostname or ip address of endpoint port( integer ) port number 80 tls( is_set ) whether to use tls ie. https false (not set) user( string ) username for Basic Authentication undefined pass( string ) password for Basic Authentication undefined path( string ) URI path of the http endpoint '/' every( duration ) interval at which requests are made undefined align( is_set ) align read intervals according to every false (not set) payload_type ( string ) how to interpert the response body, 'json' or 'plain' 'json' retries( integer ) number of retries, if request failed 2 as( string ) Root-path for the resulting data-point. If not given and payload-type is plain , defaults to 'data'. undefined","title":"Parameters"},{"location":"nodes/http/http_listen.html","text":"The http_listen node Since 0.16.0 The http_listen node provides an http endpoint, that is listening for incoming data via POST or PUT . Response Status Description Body 200 OK OK json : {\"success\": \"true\"} 401 Unauthorized BasicAuth was defined with user and pass options, but Authorization header not present or has wrong value. 405 Method not allowed Http method other than OPTIONS , POST or PUT used. 415 Unsupported Media Type Body is missing or wrong content-type is used. Example 1 | http_listen () Will set up an http endpoint, waiting for data coming in on port 8899 and path '/' and as application/json data. The body will be interpreted as json and inserted with the root-object 'data' in the resulting data-point. Example 2 | http_listen () . path ( '/SInterface/MaintenanceInterface_SaveMaintenanceAlert' ) . as ( 'data.http_res' ) . content_type ( 'application/x-www-form-urlencoded' ) . payload_type ( 'json' ) . tls () Will set up an https endpoint, waiting for data coming in on port 8899 and with path '/SInterface/MaintenanceInterface_SaveMaintenanceAlert' and as application/x-www-form-urlencoded data. Every field in this urlencoded body will be interpreted as a json string. For example: if there is a field called alert_type in the incoming body with a value of: {\"id\": 0, \"name\": \"notice\"} , there will be a field data.http_res.alert_type in the resulting data-item: { \"ts\" : 1629812164152 , \"data\" : { \"http_res\" : { \"alert_type\" : { \"id\" : 0 , \"name\" : \"notice\" }} } } Example 3 | http_listen () . port ( 8898 ) . content_type ( 'text/plain' ) . as ( 'data.http_res' ) . payload_type ( 'json' ) Will set up an http (no tls) endpoint, listening on port 8898 and path '/'. The body of the message will be interpreted as a json string. The resulting data-structure will be set under data.http_res in the resulting data-item. Example: Raw body value: {\"id\": 2233, \"name\": \"takahashu\", \"mode\": 123} . Resulting data-point in json format: { \"ts\" : 1629812164152 , \"data\" : { \"http_res\" : { \"id\" : 2233 , \"name\" : \"takahashu\" , \"mode\" : 123 } } } Parameters Parameter Description Default port( integer ) port to listen on 8899 tls( is_set ) whether to use tls (https), For ssl options used (tls version, ciphers suites, etc.) see faxe's http API config false (not set) path( string ) URI path of the http endpoint '/' payload_type( string ) how to interpret incoming data, valid values: 'plain' or 'json' 'plain' content_type( string ) how the message-body will be interpreted, 'application/x-www-form-urlencoded' , 'text/plain' , 'application/json' 'application/json' as( string ) base path for resulting data-items, if not given and content_type is text/plain or application/json , 'data' is the default value undefined / 'data' user( string ) If given, BasicAuth Authorization is to be used in requests to this endpoint. undefined pass( string ) If user is given, pass must be given also. undefined","title":"Http listen"},{"location":"nodes/http/http_listen.html#the-http_listen-node","text":"Since 0.16.0 The http_listen node provides an http endpoint, that is listening for incoming data via POST or PUT .","title":"The http_listen node"},{"location":"nodes/http/http_listen.html#response","text":"Status Description Body 200 OK OK json : {\"success\": \"true\"} 401 Unauthorized BasicAuth was defined with user and pass options, but Authorization header not present or has wrong value. 405 Method not allowed Http method other than OPTIONS , POST or PUT used. 415 Unsupported Media Type Body is missing or wrong content-type is used.","title":"Response"},{"location":"nodes/http/http_listen.html#example-1","text":"| http_listen () Will set up an http endpoint, waiting for data coming in on port 8899 and path '/' and as application/json data. The body will be interpreted as json and inserted with the root-object 'data' in the resulting data-point.","title":"Example 1"},{"location":"nodes/http/http_listen.html#example-2","text":"| http_listen () . path ( '/SInterface/MaintenanceInterface_SaveMaintenanceAlert' ) . as ( 'data.http_res' ) . content_type ( 'application/x-www-form-urlencoded' ) . payload_type ( 'json' ) . tls () Will set up an https endpoint, waiting for data coming in on port 8899 and with path '/SInterface/MaintenanceInterface_SaveMaintenanceAlert' and as application/x-www-form-urlencoded data. Every field in this urlencoded body will be interpreted as a json string. For example: if there is a field called alert_type in the incoming body with a value of: {\"id\": 0, \"name\": \"notice\"} , there will be a field data.http_res.alert_type in the resulting data-item: { \"ts\" : 1629812164152 , \"data\" : { \"http_res\" : { \"alert_type\" : { \"id\" : 0 , \"name\" : \"notice\" }} } }","title":"Example 2"},{"location":"nodes/http/http_listen.html#example-3","text":"| http_listen () . port ( 8898 ) . content_type ( 'text/plain' ) . as ( 'data.http_res' ) . payload_type ( 'json' ) Will set up an http (no tls) endpoint, listening on port 8898 and path '/'. The body of the message will be interpreted as a json string. The resulting data-structure will be set under data.http_res in the resulting data-item. Example: Raw body value: {\"id\": 2233, \"name\": \"takahashu\", \"mode\": 123} . Resulting data-point in json format: { \"ts\" : 1629812164152 , \"data\" : { \"http_res\" : { \"id\" : 2233 , \"name\" : \"takahashu\" , \"mode\" : 123 } } }","title":"Example 3"},{"location":"nodes/http/http_listen.html#parameters","text":"Parameter Description Default port( integer ) port to listen on 8899 tls( is_set ) whether to use tls (https), For ssl options used (tls version, ciphers suites, etc.) see faxe's http API config false (not set) path( string ) URI path of the http endpoint '/' payload_type( string ) how to interpret incoming data, valid values: 'plain' or 'json' 'plain' content_type( string ) how the message-body will be interpreted, 'application/x-www-form-urlencoded' , 'text/plain' , 'application/json' 'application/json' as( string ) base path for resulting data-items, if not given and content_type is text/plain or application/json , 'data' is the default value undefined / 'data' user( string ) If given, BasicAuth Authorization is to be used in requests to this endpoint. undefined pass( string ) If user is given, pass must be given also. undefined","title":"Parameters"},{"location":"nodes/http/http_post.html","text":"The http_post node Sends incoming data to a specified HTTP endpoint via the POST method as a JSON message. If any errors occur during the request, the node will attempt to retry sending. Content-type header application/json will be used. Note: This node is a sink node and does not output any flow-data, therefore any node connected to this node will not get any data. Example | http_post() . host ( 'remote.com' ) . port ( 8088 ) . path ( '/receive/json' ) Sends all incoming data to http://remote.com:8088/receive/json in JSON format. Parameters Parameter Description Default host( string ) hostname or ip address of endpoint port( integer ) port number tls( is_set ) whether to use tls ie. https false (not set) user( string ) username for Basic Authentication undefined pass( string ) password for Basic Authentication undefined path( string ) URI path of the http endpoint ''","title":"Http post"},{"location":"nodes/http/http_post.html#the-http_post-node","text":"Sends incoming data to a specified HTTP endpoint via the POST method as a JSON message. If any errors occur during the request, the node will attempt to retry sending. Content-type header application/json will be used. Note: This node is a sink node and does not output any flow-data, therefore any node connected to this node will not get any data.","title":"The http_post node"},{"location":"nodes/http/http_post.html#example","text":"| http_post() . host ( 'remote.com' ) . port ( 8088 ) . path ( '/receive/json' ) Sends all incoming data to http://remote.com:8088/receive/json in JSON format.","title":"Example"},{"location":"nodes/http/http_post.html#parameters","text":"Parameter Description Default host( string ) hostname or ip address of endpoint port( integer ) port number tls( is_set ) whether to use tls ie. https false (not set) user( string ) username for Basic Authentication undefined pass( string ) password for Basic Authentication undefined path( string ) URI path of the http endpoint ''","title":"Parameters"},{"location":"nodes/logic/case.html","text":"The case node Evaluates a series of lambda expressions in a top down manner. The node will output / add the corresponding value of the first lambda expression that evaluates as true. If none of the lambda expressions evaluate as true, a default value will be used The case node works in a similar way CASE expressions in SQL work. Example | case( lambda : \"data.condition.name\" == 'OK' , lambda : \"data.condition.name\" == 'Warning' , lambda : \"data.condition.name\" == 'Error' ) . values ( '{\"cond\": \"Everything OK!\"}' , '{\"cond\": \"Oh, oh, a Warning!\"}' , '{\"cond\": \"Damn, Error!\"}' ) . json () . as ( 'data' ) .default( '{\"cond\": \"Nothing matched!!!\"}' ) Parameters Parameter Description Default [node] lambdas( lambda_list ) list of lambda expressions values( string_list\\|text_list ) corresponding values json( is_set ) if set, will treat the values and default parameters as json strings false, not set as ( string ) field-path for the output value default( any ) default value to use, if no case clause matches","title":"Case"},{"location":"nodes/logic/case.html#the-case-node","text":"Evaluates a series of lambda expressions in a top down manner. The node will output / add the corresponding value of the first lambda expression that evaluates as true. If none of the lambda expressions evaluate as true, a default value will be used The case node works in a similar way CASE expressions in SQL work.","title":"The case node"},{"location":"nodes/logic/case.html#example","text":"| case( lambda : \"data.condition.name\" == 'OK' , lambda : \"data.condition.name\" == 'Warning' , lambda : \"data.condition.name\" == 'Error' ) . values ( '{\"cond\": \"Everything OK!\"}' , '{\"cond\": \"Oh, oh, a Warning!\"}' , '{\"cond\": \"Damn, Error!\"}' ) . json () . as ( 'data' ) .default( '{\"cond\": \"Nothing matched!!!\"}' )","title":"Example"},{"location":"nodes/logic/case.html#parameters","text":"Parameter Description Default [node] lambdas( lambda_list ) list of lambda expressions values( string_list\\|text_list ) corresponding values json( is_set ) if set, will treat the values and default parameters as json strings false, not set as ( string ) field-path for the output value default( any ) default value to use, if no case clause matches","title":"Parameters"},{"location":"nodes/logic/change_detect.html","text":"The change_detect node Emits new point-values only if different from the previous point. Multiple fields can be monitored for change by this node. If no fields are given, the complete data-item is compared to the last one. If a reset_timeout is given, all previous values will be reset when no value is received within this amount of time. So that after the timeout the first data_item will be emitted. For value comparison erlang's strict equals (=:=) is used, so 1.0 is not equal to 1. Example %% detects any changes in data_items | change_detect() %% detect changes in one field, with timeout %% outputs at least the first data_item coming in after a 3 second timeout | change_detect( 'val' ) . reset_timeout ( 3s ) %% detect changes in two fields, with timeout | change_detect( 'data.State.Err' , 'data.State.Msg' ) . reset_timeout ( 60s ) % in-example json notation: % {\"data\": {\"x\": {\"temp\": 32.4564}, \"y\" : {\"temp\" : 31.15155}} } | change_detect( 'data.x.temp' , 'data.y.temp' ) Parameters Parameter Description Default [node] fields( string_list ) List of fields to monitor optional reset_timeout( duration ) Previous values TTL 3h","title":"Change detect"},{"location":"nodes/logic/change_detect.html#the-change_detect-node","text":"Emits new point-values only if different from the previous point. Multiple fields can be monitored for change by this node. If no fields are given, the complete data-item is compared to the last one. If a reset_timeout is given, all previous values will be reset when no value is received within this amount of time. So that after the timeout the first data_item will be emitted. For value comparison erlang's strict equals (=:=) is used, so 1.0 is not equal to 1.","title":"The change_detect node"},{"location":"nodes/logic/change_detect.html#example","text":"%% detects any changes in data_items | change_detect() %% detect changes in one field, with timeout %% outputs at least the first data_item coming in after a 3 second timeout | change_detect( 'val' ) . reset_timeout ( 3s ) %% detect changes in two fields, with timeout | change_detect( 'data.State.Err' , 'data.State.Msg' ) . reset_timeout ( 60s ) % in-example json notation: % {\"data\": {\"x\": {\"temp\": 32.4564}, \"y\" : {\"temp\" : 31.15155}} } | change_detect( 'data.x.temp' , 'data.y.temp' )","title":"Example"},{"location":"nodes/logic/change_detect.html#parameters","text":"Parameter Description Default [node] fields( string_list ) List of fields to monitor optional reset_timeout( duration ) Previous values TTL 3h","title":"Parameters"},{"location":"nodes/logic/deadman.html","text":"The deadman node Emits a point, if there is no item coming in for the given amount of time. For output there are two options: If repeat_last param is set, the node will output the last message it saw incoming as the dead-message, if there is no last message yet, an empty message will be emitted With fields and field_values a list of values can be provided to be included in the output. If no fields (and field_values) parameter and is given, an empty datapoint will be emitted. The repeat_last parameter will always override the fields and field_values parameter The node will forward every message it gets by default, this can be changed by using the no_forward flag Examples | deadman( 15s ) Outputs an empty data-point item, if the node does not see any items coming in withing 15 seconds, while simply forwarding any item it gets. The first timeout starts, when the node is starting. def interval = 15s | deadman( interval ) . trigger_on_value () . repeat_last () . repeat_interval ( interval ) Outputs every item as a passthrough and, if no item is seen within 15 seconds, outputs the last item with a new timestamp, that is derived from the last iteration and increased by interval (15s). Parameters Parameter Description Default [node] timeout( duration ) timeout value for the node fields( string_list ) undefined field_values ( string_list ) undefined repeat_last( is_set) whether to output the last value seen false, not set no_forward( is_set) whether to output every message that comes in (pass through) false, not set repeat_with_new_ts ( bool ) when repeating an item, set the current timestamp to that item before emitting true repeat_interval ( duration ) when repeating an item, adds this amount of time to a buffered timestamp (from the last item seen, or from the last addition of this interval) to genereate a new timestamp for the repeated item, this setting is independent of repeat_with_new_ts and takes precedence over it undefined","title":"Deadman"},{"location":"nodes/logic/deadman.html#the-deadman-node","text":"Emits a point, if there is no item coming in for the given amount of time. For output there are two options: If repeat_last param is set, the node will output the last message it saw incoming as the dead-message, if there is no last message yet, an empty message will be emitted With fields and field_values a list of values can be provided to be included in the output. If no fields (and field_values) parameter and is given, an empty datapoint will be emitted. The repeat_last parameter will always override the fields and field_values parameter The node will forward every message it gets by default, this can be changed by using the no_forward flag","title":"The deadman node"},{"location":"nodes/logic/deadman.html#examples","text":"| deadman( 15s ) Outputs an empty data-point item, if the node does not see any items coming in withing 15 seconds, while simply forwarding any item it gets. The first timeout starts, when the node is starting. def interval = 15s | deadman( interval ) . trigger_on_value () . repeat_last () . repeat_interval ( interval ) Outputs every item as a passthrough and, if no item is seen within 15 seconds, outputs the last item with a new timestamp, that is derived from the last iteration and increased by interval (15s).","title":"Examples"},{"location":"nodes/logic/deadman.html#parameters","text":"Parameter Description Default [node] timeout( duration ) timeout value for the node fields( string_list ) undefined field_values ( string_list ) undefined repeat_last( is_set) whether to output the last value seen false, not set no_forward( is_set) whether to output every message that comes in (pass through) false, not set repeat_with_new_ts ( bool ) when repeating an item, set the current timestamp to that item before emitting true repeat_interval ( duration ) when repeating an item, adds this amount of time to a buffered timestamp (from the last item seen, or from the last addition of this interval) to genereate a new timestamp for the repeated item, this setting is independent of repeat_with_new_ts and takes precedence over it undefined","title":"Parameters"},{"location":"nodes/logic/eval.html","text":"The eval node Evaluate one or more lambda expressions. For an explanation of lambdas, see lambda . The list of lambda expressions given, will be evaluated top down. This means that a lambda can use the result of a previous expression. The resulting fields named with the as parameter will be added to the current data-point. Examples | eval( lambda : \"val\" * 2 , lambda : \"double\" / 2 ) %% 'double' is also used in the second expression above . as ( 'double' , 'val' ) This example demonstrates the 'serial' behaviour of the eval node. The second expression uses the field double , which the first expression just created. | eval( lambda : int ( str_concat ( string ( int ( \"val\" )), string ( int ( \"val\" )))) ) . as ( 'concat_string.int' ) The string value of the field 'val' is concatenated to itself, this is then casted to an int value and added to the current data-point as the field 'concat_string.int'. For more lambda examples see lambda Parameters Parameter Description Default [node] lambdas( lambda_list ) list of lambda expressions as( string_list ) list of output fieldnames (must have the same length as lambdas ) tags ( string_list ) list of output tagnames []","title":"Eval"},{"location":"nodes/logic/eval.html#the-eval-node","text":"Evaluate one or more lambda expressions. For an explanation of lambdas, see lambda . The list of lambda expressions given, will be evaluated top down. This means that a lambda can use the result of a previous expression. The resulting fields named with the as parameter will be added to the current data-point.","title":"The eval node"},{"location":"nodes/logic/eval.html#examples","text":"| eval( lambda : \"val\" * 2 , lambda : \"double\" / 2 ) %% 'double' is also used in the second expression above . as ( 'double' , 'val' ) This example demonstrates the 'serial' behaviour of the eval node. The second expression uses the field double , which the first expression just created. | eval( lambda : int ( str_concat ( string ( int ( \"val\" )), string ( int ( \"val\" )))) ) . as ( 'concat_string.int' ) The string value of the field 'val' is concatenated to itself, this is then casted to an int value and added to the current data-point as the field 'concat_string.int'. For more lambda examples see lambda","title":"Examples"},{"location":"nodes/logic/eval.html#parameters","text":"Parameter Description Default [node] lambdas( lambda_list ) list of lambda expressions as( string_list ) list of output fieldnames (must have the same length as lambdas ) tags ( string_list ) list of output tagnames []","title":"Parameters"},{"location":"nodes/logic/sample.html","text":"The sample node Samples the incoming points or batches. One point will be emitted every count or duration specified. When a duration is given, this node will emit the first data-item arriving after the timeout, then the timeout starts again. Example | sample( 5 ) Keep every 5th data_point or data_batch. | sample( 10s ) Keep the first point or batch after a 10 second interval. Parameters Parameter Description Default [node] rate ( integer duration ) sample rate","title":"Sample"},{"location":"nodes/logic/sample.html#the-sample-node","text":"Samples the incoming points or batches. One point will be emitted every count or duration specified. When a duration is given, this node will emit the first data-item arriving after the timeout, then the timeout starts again.","title":"The sample node"},{"location":"nodes/logic/sample.html#example","text":"| sample( 5 ) Keep every 5th data_point or data_batch. | sample( 10s ) Keep the first point or batch after a 10 second interval.","title":"Example"},{"location":"nodes/logic/sample.html#parameters","text":"Parameter Description Default [node] rate ( integer duration ) sample rate","title":"Parameters"},{"location":"nodes/logic/shift.html","text":"The shift node The shift node shifts points and batches in time. This is useful for comparing batches or points from different times. Example | shift( 5m ) Shift all data points 5 minutes forward in time. | shift( - 10s ) Shift all data points 10 seconds backwards in time. Parameters Parameter Description Default [node] offset ( duration ) time offset","title":"Shift"},{"location":"nodes/logic/shift.html#the-shift-node","text":"The shift node shifts points and batches in time. This is useful for comparing batches or points from different times.","title":"The shift node"},{"location":"nodes/logic/shift.html#example","text":"| shift( 5m ) Shift all data points 5 minutes forward in time. | shift( - 10s ) Shift all data points 10 seconds backwards in time.","title":"Example"},{"location":"nodes/logic/shift.html#parameters","text":"Parameter Description Default [node] offset ( duration ) time offset","title":"Parameters"},{"location":"nodes/logic/state_change.html","text":"The state_change node Computes the duration and count of points of a given state. The state is defined via a lambda expression. For each consecutive point for which the expression evaluates as true, state count and duration will be incremented. This node can be used to track state in data and produce new data based on the state-events. It can produce new data-points every time the state is entered and/or left . The enter data-point If the .enter() option is set, a new data-point will be emitted on state-enter. The new data-point will have a field, named with the .enter_as() option, set to 1 . This fieldname defaults to state_entered . new since 0.19.51: state_id , uuid v4 is created on every state entry The leave data-point If the .leave() option is set, a new data-point will be emitted on state-leave. Fields for this data-point: Name Description state_left stateflag, set to 1 state_start_ts timestamp at which the state has been entered state_end_ts timestamp at which the state-expression has been satisfied the last time state_duration duration of the state in milliseconds state_count number of points, the number of consecutive data-points for which the state-expression returned true state_id new since 0.19.51 , uuid v4 is created on every state-enter action When the lambda expression generates an error during evaluation, the current point is discarded and does not affect any calculations. Note that while state-count is 1, state-duration will be 0, if there is exactly 1 data-point within the state-window. Example %% the lambda defines our state | state_change ( lambda : \"val\" < 7 AND \"err\" != 0 ) %% the node will emit a data-point on state-leave only . leave () %% we keep these fields for the new state-leave data-point . leave_keep ( 'err' , 'err_code' ) Example output in json: { \"ts\" : 1232154654655 , \"err\" : 1 , \"err_code\" : 1492 , \"state_left\" : 1 , \"state_id\" : \"07aae050-9d30-4570-989d-74f5f21d52bf\" , \"state_start_ts\" : 1232154644655 , \"state_end_ts\" : 1232154654655 , \"state_duration\" : 10000 , \"state_count\" : 22 } %% the lambda defines our state | state_change ( lambda : \"val\" > 2 OR \"err\" == 1 ) %% the node will emit a data-point on state-enter . enter () %% the node will emit a data-point on state-leave . leave () %% we keep these fields for the new state-enter data-point . enter_keep ( 'err' , 'err_code' ) %% we keep these fields for the new state-leave data-point . leave_keep ( 'err' , 'err_code' ) %% prefix all fields written by this node with 'my_' . prefix ( 'my_' ) Example output in json for the enter data-point: { \"ts\" : 1232154654655 , \"state_id\" : \"07aae050-9d30-4570-989d-74f5f21d52bf\" , \"err\" : 1 , \"err_code\" : 1492 , \"my_state_entered\" : 1 } Parameters Parameter Description Default [node] lambda( lambda ) state lambda expression enter( is_set ) emit a datapoint on state-enter undefined leave( is_set ) emit a datapoint on state-leave undefined enter_as( string ) name for the \"enter\" field, it will be set to true 'state_entered' leave_as( string ) name for the \"leave\" field, it will be set to true 'state_left' state_id_as( string ) since 0.19.51: name for the \"state_id\" field, 'state_id' enter_keep( string_list ) a list of fieldnames that should be kept for the enter data-point [] leave_keep( string_list ) a list of fieldnames that should be kept for the leave data-point [] prefix( string ) prefix fields added by this node with a string ( keep -fields stay untouched) '' (empty string) At least one of the enter | leave options must be given.","title":"State change"},{"location":"nodes/logic/state_change.html#the-state_change-node","text":"Computes the duration and count of points of a given state. The state is defined via a lambda expression. For each consecutive point for which the expression evaluates as true, state count and duration will be incremented. This node can be used to track state in data and produce new data based on the state-events. It can produce new data-points every time the state is entered and/or left .","title":"The state_change node"},{"location":"nodes/logic/state_change.html#the-enter-data-point","text":"If the .enter() option is set, a new data-point will be emitted on state-enter. The new data-point will have a field, named with the .enter_as() option, set to 1 . This fieldname defaults to state_entered . new since 0.19.51: state_id , uuid v4 is created on every state entry","title":"The enter data-point"},{"location":"nodes/logic/state_change.html#the-leave-data-point","text":"If the .leave() option is set, a new data-point will be emitted on state-leave. Fields for this data-point: Name Description state_left stateflag, set to 1 state_start_ts timestamp at which the state has been entered state_end_ts timestamp at which the state-expression has been satisfied the last time state_duration duration of the state in milliseconds state_count number of points, the number of consecutive data-points for which the state-expression returned true state_id new since 0.19.51 , uuid v4 is created on every state-enter action When the lambda expression generates an error during evaluation, the current point is discarded and does not affect any calculations. Note that while state-count is 1, state-duration will be 0, if there is exactly 1 data-point within the state-window.","title":"The leave data-point"},{"location":"nodes/logic/state_change.html#example","text":"%% the lambda defines our state | state_change ( lambda : \"val\" < 7 AND \"err\" != 0 ) %% the node will emit a data-point on state-leave only . leave () %% we keep these fields for the new state-leave data-point . leave_keep ( 'err' , 'err_code' ) Example output in json: { \"ts\" : 1232154654655 , \"err\" : 1 , \"err_code\" : 1492 , \"state_left\" : 1 , \"state_id\" : \"07aae050-9d30-4570-989d-74f5f21d52bf\" , \"state_start_ts\" : 1232154644655 , \"state_end_ts\" : 1232154654655 , \"state_duration\" : 10000 , \"state_count\" : 22 } %% the lambda defines our state | state_change ( lambda : \"val\" > 2 OR \"err\" == 1 ) %% the node will emit a data-point on state-enter . enter () %% the node will emit a data-point on state-leave . leave () %% we keep these fields for the new state-enter data-point . enter_keep ( 'err' , 'err_code' ) %% we keep these fields for the new state-leave data-point . leave_keep ( 'err' , 'err_code' ) %% prefix all fields written by this node with 'my_' . prefix ( 'my_' ) Example output in json for the enter data-point: { \"ts\" : 1232154654655 , \"state_id\" : \"07aae050-9d30-4570-989d-74f5f21d52bf\" , \"err\" : 1 , \"err_code\" : 1492 , \"my_state_entered\" : 1 }","title":"Example"},{"location":"nodes/logic/state_change.html#parameters","text":"Parameter Description Default [node] lambda( lambda ) state lambda expression enter( is_set ) emit a datapoint on state-enter undefined leave( is_set ) emit a datapoint on state-leave undefined enter_as( string ) name for the \"enter\" field, it will be set to true 'state_entered' leave_as( string ) name for the \"leave\" field, it will be set to true 'state_left' state_id_as( string ) since 0.19.51: name for the \"state_id\" field, 'state_id' enter_keep( string_list ) a list of fieldnames that should be kept for the enter data-point [] leave_keep( string_list ) a list of fieldnames that should be kept for the leave data-point [] prefix( string ) prefix fields added by this node with a string ( keep -fields stay untouched) '' (empty string) At least one of the enter | leave options must be given.","title":"Parameters"},{"location":"nodes/logic/state_count.html","text":"The state_count node Computes the number of consecutive points in a given state. The state is defined via a lambda expression. For each consecutive point for which the expression evaluates as true, the state count will be incremented. When a point evaluates to false, the state count is reset. The state count will be added as an additional int field to each point. If the expression evaluates to false, the value will be -1. If the expression generates an error during evaluation, the point is discarded and does not affect the state count. Example | state_count( lambda : \"val\" < 7 ) . as ( 'val_below_7' ) Counts the number of consecutive points which have the value of the val field below 7 . Parameters Parameter Description Default [node] lambda( lambda ) state lambda expression as( string ) name for the added count field 'state_count'","title":"State count"},{"location":"nodes/logic/state_count.html#the-state_count-node","text":"Computes the number of consecutive points in a given state. The state is defined via a lambda expression. For each consecutive point for which the expression evaluates as true, the state count will be incremented. When a point evaluates to false, the state count is reset. The state count will be added as an additional int field to each point. If the expression evaluates to false, the value will be -1. If the expression generates an error during evaluation, the point is discarded and does not affect the state count.","title":"The state_count node"},{"location":"nodes/logic/state_count.html#example","text":"| state_count( lambda : \"val\" < 7 ) . as ( 'val_below_7' ) Counts the number of consecutive points which have the value of the val field below 7 .","title":"Example"},{"location":"nodes/logic/state_count.html#parameters","text":"Parameter Description Default [node] lambda( lambda ) state lambda expression as( string ) name for the added count field 'state_count'","title":"Parameters"},{"location":"nodes/logic/state_duration.html","text":"The state_duration node Computes the duration of a given state. The state is defined via a lambda expression. For each consecutive point for which the lambda expression evaluates as true, the state duration will be incremented by the duration between points. When a point evaluates as false, the state duration is reset. The state duration will be added as an additional field to each point and it's unit is milliseconds . If the expression evaluates to false, the value will be -1. When the lambda expression generates an error during evaluation, the point is discarded and does not affect the state duration.. Example | state_duration( lambda : \"val\" < 7 ) Parameters Parameter Description Default [node] lambda( lambda ) state lambda expression as( string ) name for the added duration field 'state_duration'","title":"State duration"},{"location":"nodes/logic/state_duration.html#the-state_duration-node","text":"Computes the duration of a given state. The state is defined via a lambda expression. For each consecutive point for which the lambda expression evaluates as true, the state duration will be incremented by the duration between points. When a point evaluates as false, the state duration is reset. The state duration will be added as an additional field to each point and it's unit is milliseconds . If the expression evaluates to false, the value will be -1. When the lambda expression generates an error during evaluation, the point is discarded and does not affect the state duration..","title":"The state_duration node"},{"location":"nodes/logic/state_duration.html#example","text":"| state_duration( lambda : \"val\" < 7 )","title":"Example"},{"location":"nodes/logic/state_duration.html#parameters","text":"Parameter Description Default [node] lambda( lambda ) state lambda expression as( string ) name for the added duration field 'state_duration'","title":"Parameters"},{"location":"nodes/logic/state_sequence.html","text":"The state_sequence node This node takes a list of lambda expressions representing different states. It will emit values only after each state has evaluated as true in the given order and, for each step in the sequence within the corresponding timeout. A transition timeout must be defined for every state transition with the within parameter. If a timeout occurs at any point the sequence will be reset and started from the first expression again. Note that the sequence timeouts start after the first data_point has satisfied the first lambda expression. Therefore, if 3 lambda states are given, only 2 durations for the within parameter can be defined. With the strict parameter the sequence of states must be met exactly without any intermediary data_points coming in, that do not satisfy the current state expression. Normally this would not reset the sequence of evaluation, in this mode, it will. On a successful evaluation of the whole sequence, the node will simply output the last value, that completed the sequence. The state_sequence node can be used with one or many input nodes. Example in1 | state_sequence( in2 , in3 ) %% can use any number of nodes . states ( lambda : \"data.topic\" == 'in1' , %% state 1 lambda : \"data.topic\" == 'in2' , %% state 2 lambda : \"data.topic\" == 'in3' %% state 3 ) . within ( 25s , %% transition-time from state 1 to state 2 20s %% transition-time from state 2 to state 3 ) Parameters Parameter Description Default [node] nodes_in( node_list ) a list of node(chains) optional states ( lambda_list ) the states within( duration_list ) one timeout for every state-transition strict( is_set ) whether the state sequence must be transition exactly false (not set)","title":"State sequence"},{"location":"nodes/logic/state_sequence.html#the-state_sequence-node","text":"This node takes a list of lambda expressions representing different states. It will emit values only after each state has evaluated as true in the given order and, for each step in the sequence within the corresponding timeout. A transition timeout must be defined for every state transition with the within parameter. If a timeout occurs at any point the sequence will be reset and started from the first expression again. Note that the sequence timeouts start after the first data_point has satisfied the first lambda expression. Therefore, if 3 lambda states are given, only 2 durations for the within parameter can be defined. With the strict parameter the sequence of states must be met exactly without any intermediary data_points coming in, that do not satisfy the current state expression. Normally this would not reset the sequence of evaluation, in this mode, it will. On a successful evaluation of the whole sequence, the node will simply output the last value, that completed the sequence. The state_sequence node can be used with one or many input nodes.","title":"The state_sequence node"},{"location":"nodes/logic/state_sequence.html#example","text":"in1 | state_sequence( in2 , in3 ) %% can use any number of nodes . states ( lambda : \"data.topic\" == 'in1' , %% state 1 lambda : \"data.topic\" == 'in2' , %% state 2 lambda : \"data.topic\" == 'in3' %% state 3 ) . within ( 25s , %% transition-time from state 1 to state 2 20s %% transition-time from state 2 to state 3 )","title":"Example"},{"location":"nodes/logic/state_sequence.html#parameters","text":"Parameter Description Default [node] nodes_in( node_list ) a list of node(chains) optional states ( lambda_list ) the states within( duration_list ) one timeout for every state-transition strict( is_set ) whether the state sequence must be transition exactly false (not set)","title":"Parameters"},{"location":"nodes/logic/time_diff.html","text":"The time_diff node The time_diff node adds a field to the current data-item containing the difference between the timestamps of the consecutive items. Note that the difference in time will be calculated from the data-points timestamp fields and does not reflect the difference in time points coming into the node. For the other behaviour see time_elapsed . The unit for output values is milliseconds. Example | time_diff() . as ( 'time_diff' ) Parameters Parameter Description Default as( string ) name of the field for parsed data 'timediff' default( any ) default value to use, when no previous time exists 0","title":"Time diff"},{"location":"nodes/logic/time_diff.html#the-time_diff-node","text":"The time_diff node adds a field to the current data-item containing the difference between the timestamps of the consecutive items. Note that the difference in time will be calculated from the data-points timestamp fields and does not reflect the difference in time points coming into the node. For the other behaviour see time_elapsed . The unit for output values is milliseconds.","title":"The time_diff node"},{"location":"nodes/logic/time_diff.html#example","text":"| time_diff() . as ( 'time_diff' )","title":"Example"},{"location":"nodes/logic/time_diff.html#parameters","text":"Parameter Description Default as( string ) name of the field for parsed data 'timediff' default( any ) default value to use, when no previous time exists 0","title":"Parameters"},{"location":"nodes/logic/time_elapsed.html","text":"The time_elapsed node The time_elapsed node adds a field to the current data-item containing the difference in arrival time of consecutive items. See the time_diff node . The unit for output values is milliseconds. Example | time_elapsed() . as ( 'time_dur' ) Parameters Parameter Description Default as( string ) name of the field for parsed data 'elapsed' default( any ) default value to use, when no previous time exists 0","title":"Time elapsed"},{"location":"nodes/logic/time_elapsed.html#the-time_elapsed-node","text":"The time_elapsed node adds a field to the current data-item containing the difference in arrival time of consecutive items. See the time_diff node . The unit for output values is milliseconds.","title":"The time_elapsed node"},{"location":"nodes/logic/time_elapsed.html#example","text":"| time_elapsed() . as ( 'time_dur' )","title":"Example"},{"location":"nodes/logic/time_elapsed.html#parameters","text":"Parameter Description Default as( string ) name of the field for parsed data 'elapsed' default( any ) default value to use, when no previous time exists 0","title":"Parameters"},{"location":"nodes/logic/triggered_timeout.html","text":"The triggered_timeout node Emits a point, if there is no message coming in for the given amount of time. A timeout will be started on an explicit trigger: * When a lambda expression is given for parameter timeout_trigger , this expression must evaluate as true to start (and after a timeout has occurred to restart) a timeout. If no lambda expression is given for the timeout_trigger , the trigger is any data_point coming in on port 1, the so called trigger_port . A new trigger does not restart a running timeout. After a timeout occurred, the node waits for a new trigger to come in before it starts a new timeout. After a timeout is started the node waits for data coming in, that either does not satisfy the trigger expression(when a lambda expression is given for the timeout_trigger parameter) or is coming in on any port except the trigger_port (port 1). Data for the outgoing data-point can be defined with the fields and field_values parameters. This node can have any number of input-nodes. Example def timeout = 30s % ... in1 | triggered_timeout( in2 ) . timeout ( timeout ) . timeout_trigger ( lambda : \"data.topic\" == 'in1' ) def condition_reason = 'oh no !!' robot_state | triggered_timeout( or derlog ) . timeout ( timeout ) . fields ( 'combined.condition.name' , 'combined.condition_reason' , 'combined.condition.id' ) . field_values ( 'ERROR' , condition_reason , 2 ) %.cancel_fields('combined.condition.name', 'combined.condition_reason', 'combined.condition.id') %.cancel_field_values('OK', '', 0) Parameters Parameter Description Default timeout( duration ) timeout_trigger( lambda ) lambda expression which triggers the timeout optional fields ( string_list ) paths for the output fields optional field_values( list ) values for the output fields optional","title":"Triggered timeout"},{"location":"nodes/logic/triggered_timeout.html#the-triggered_timeout-node","text":"Emits a point, if there is no message coming in for the given amount of time. A timeout will be started on an explicit trigger: * When a lambda expression is given for parameter timeout_trigger , this expression must evaluate as true to start (and after a timeout has occurred to restart) a timeout. If no lambda expression is given for the timeout_trigger , the trigger is any data_point coming in on port 1, the so called trigger_port . A new trigger does not restart a running timeout. After a timeout occurred, the node waits for a new trigger to come in before it starts a new timeout. After a timeout is started the node waits for data coming in, that either does not satisfy the trigger expression(when a lambda expression is given for the timeout_trigger parameter) or is coming in on any port except the trigger_port (port 1). Data for the outgoing data-point can be defined with the fields and field_values parameters. This node can have any number of input-nodes.","title":"The triggered_timeout node"},{"location":"nodes/logic/triggered_timeout.html#example","text":"def timeout = 30s % ... in1 | triggered_timeout( in2 ) . timeout ( timeout ) . timeout_trigger ( lambda : \"data.topic\" == 'in1' ) def condition_reason = 'oh no !!' robot_state | triggered_timeout( or derlog ) . timeout ( timeout ) . fields ( 'combined.condition.name' , 'combined.condition_reason' , 'combined.condition.id' ) . field_values ( 'ERROR' , condition_reason , 2 ) %.cancel_fields('combined.condition.name', 'combined.condition_reason', 'combined.condition.id') %.cancel_field_values('OK', '', 0)","title":"Example"},{"location":"nodes/logic/triggered_timeout.html#parameters","text":"Parameter Description Default timeout( duration ) timeout_trigger( lambda ) lambda expression which triggers the timeout optional fields ( string_list ) paths for the output fields optional field_values( list ) values for the output fields optional","title":"Parameters"},{"location":"nodes/logic/value_diff.html","text":"The value_diff node Outputs the difference to a previous value for multiple fields. If no previous value is found for a specific field, a configurable default value is used. This node can handle numeric values only. Example | value_diff () . fields ( 'value' ) . as ( 'value_diff' ) Parameters Parameter Description Default fields( string_list ) as( string_list ) names of the ouptput fields, if not specified all fields will be overwritten with diff values defaults to fields default( number ) Value to use a default current value of field mode( string ) diff modes: abs (absolute difference between previous and current), c-p (current minus previous value), p-c (previous minus current) 'abs'","title":"Value diff"},{"location":"nodes/logic/value_diff.html#the-value_diff-node","text":"Outputs the difference to a previous value for multiple fields. If no previous value is found for a specific field, a configurable default value is used. This node can handle numeric values only.","title":"The value_diff node"},{"location":"nodes/logic/value_diff.html#example","text":"| value_diff () . fields ( 'value' ) . as ( 'value_diff' )","title":"Example"},{"location":"nodes/logic/value_diff.html#parameters","text":"Parameter Description Default fields( string_list ) as( string_list ) names of the ouptput fields, if not specified all fields will be overwritten with diff values defaults to fields default( number ) Value to use a default current value of field mode( string ) diff modes: abs (absolute difference between previous and current), c-p (current minus previous value), p-c (previous minus current) 'abs'","title":"Parameters"},{"location":"nodes/logic/where.html","text":"The where node Filter points and batches with a lambda expression, which returns a boolean value. Data-items for which the lambda expression evaluates as false will be discarded. Example | where( lambda : hour ( \"ts\" ) < 18 AND hour ( \"ts\" ) > 8 ) Discards every point who's timestamp is not between 09:00 and 18:00 UTC. Parameters Parameter Description Default [node] lambda( lambda ) The lambda filter expression","title":"Where"},{"location":"nodes/logic/where.html#the-where-node","text":"Filter points and batches with a lambda expression, which returns a boolean value. Data-items for which the lambda expression evaluates as false will be discarded.","title":"The where node"},{"location":"nodes/logic/where.html#example","text":"| where( lambda : hour ( \"ts\" ) < 18 AND hour ( \"ts\" ) > 8 ) Discards every point who's timestamp is not between 09:00 and 18:00 UTC.","title":"Example"},{"location":"nodes/logic/where.html#parameters","text":"Parameter Description Default [node] lambda( lambda ) The lambda filter expression","title":"Parameters"},{"location":"nodes/messaging/amqp_consume.html","text":"The amqp_consume node Consume data from an amqp-broker like RabbitMQ . This node accepts regular amqp routing keys as well as MQTT style topic strings for bindings / routing_key . In safe mode Once a data-item is received by the node, it will be immediately stored in an on-disk queue for data-safety. Only after this will the item be acknowledged to the amqp broker. Message deduplication If the amqp correlation-id property is set (to a unique value per message), this node can perform efficient message deduplication. See amqp_publish for details on this. Prefetch count, ack_every and dedup_size For a description of these settings, see table below. As they relate to one another in some kind, here is a rule of thumb for how to set ack_every and dedup_size when prefetch is changed: set ack_every to one third of prefetch set dedup_size to 3 times the prefetch value Example: prefetch = 100, ack_every = 35, dedup_size = 300 At the moment this node can only set up and work with topic exchanges. Example | amqp_consume() . host ( 'deves-amqp-cluster1.internal' ) . bindings ( 'my.routing.key' ) . exchange ( 'x_xchange' ) . queue ( 'faxe_test' ) . dt_field ( 'UTC-Time' ) . dt_format ( 'float_micro' ) Parameters Parameter Description Default host( string ) Ip address or hostname of the broker from config port( integer ) The broker's port 5672 / from config user( string ) AMQP user from config pass( string ) AMQP password from config vhost( string ) vhost to connect to on the broker '/' routing_key( string ) deprecated routing key to use for queue binding undefined bindings( string_list ) list of queue bindings keys [] queue( string ) name of the queue to bind to the exchange FlowName + '_' + NodeName queue_prefix( string ) prefix for the queue-name that will be ensured to exist for queue from config exchange( string ) name of the exchange to bind to the source exchange FlowName + '_' + NodeName exchange_prefix( string ) prefix for the exchange-name that will be ensured to exist for exchange from config prefetch( integer ) prefetch count to use 70 consumer_tag( string ) Identifier for the queue consumer 'c_' + FlowName + '_' + NodeName ack_every( integer ) number of messages to consume before acknowledging them to the broker 20 ack_after( duration ) timeout after which all currently not acknowledged messages will be acknowledged, regardless of the ack_every setting 5s dedup_size( integer ) number of correlation-ids to hold in memory for message deduplication 200 dt_field( string ) name of the timestamp field that is expected 'ts' dt_format( string ) timestamp or datetime format that is expected (see datetime-parsing ) 'millisecond' include_topic ( bool ) whether to include the routingkey in the resulting datapoints true topic_as ( string ) if include_topic is true, this will be the fieldname for the routingkey value 'topic' as ( string ) base object for the output data-point undefined ssl( is_set ) whether to use ssl, if true, ssl options from faxe's config for amqp connections will be used false (not set) confirm ( boolean ) whether to acknowledge consumed messages to the amqp broker, when set to false , throughput can be increased with the danger of data-loss true safe ( boolean ) whether to use faxe's internal queue. If true , messages consumed from the amqp broker will be stored in an internal ondisc queue before they get sent to downstream nodes, to avoid losing data. false Exactly one of these must be provided: routing_key , bindings .","title":"Amqp consume"},{"location":"nodes/messaging/amqp_consume.html#the-amqp_consume-node","text":"Consume data from an amqp-broker like RabbitMQ . This node accepts regular amqp routing keys as well as MQTT style topic strings for bindings / routing_key .","title":"The amqp_consume node"},{"location":"nodes/messaging/amqp_consume.html#in-safe-mode","text":"Once a data-item is received by the node, it will be immediately stored in an on-disk queue for data-safety. Only after this will the item be acknowledged to the amqp broker.","title":"In safe mode"},{"location":"nodes/messaging/amqp_consume.html#message-deduplication","text":"If the amqp correlation-id property is set (to a unique value per message), this node can perform efficient message deduplication. See amqp_publish for details on this.","title":"Message deduplication"},{"location":"nodes/messaging/amqp_consume.html#prefetch-count-ack_every-and-dedup_size","text":"For a description of these settings, see table below. As they relate to one another in some kind, here is a rule of thumb for how to set ack_every and dedup_size when prefetch is changed: set ack_every to one third of prefetch set dedup_size to 3 times the prefetch value Example: prefetch = 100, ack_every = 35, dedup_size = 300 At the moment this node can only set up and work with topic exchanges.","title":"Prefetch count, ack_every and dedup_size"},{"location":"nodes/messaging/amqp_consume.html#example","text":"| amqp_consume() . host ( 'deves-amqp-cluster1.internal' ) . bindings ( 'my.routing.key' ) . exchange ( 'x_xchange' ) . queue ( 'faxe_test' ) . dt_field ( 'UTC-Time' ) . dt_format ( 'float_micro' )","title":"Example"},{"location":"nodes/messaging/amqp_consume.html#parameters","text":"Parameter Description Default host( string ) Ip address or hostname of the broker from config port( integer ) The broker's port 5672 / from config user( string ) AMQP user from config pass( string ) AMQP password from config vhost( string ) vhost to connect to on the broker '/' routing_key( string ) deprecated routing key to use for queue binding undefined bindings( string_list ) list of queue bindings keys [] queue( string ) name of the queue to bind to the exchange FlowName + '_' + NodeName queue_prefix( string ) prefix for the queue-name that will be ensured to exist for queue from config exchange( string ) name of the exchange to bind to the source exchange FlowName + '_' + NodeName exchange_prefix( string ) prefix for the exchange-name that will be ensured to exist for exchange from config prefetch( integer ) prefetch count to use 70 consumer_tag( string ) Identifier for the queue consumer 'c_' + FlowName + '_' + NodeName ack_every( integer ) number of messages to consume before acknowledging them to the broker 20 ack_after( duration ) timeout after which all currently not acknowledged messages will be acknowledged, regardless of the ack_every setting 5s dedup_size( integer ) number of correlation-ids to hold in memory for message deduplication 200 dt_field( string ) name of the timestamp field that is expected 'ts' dt_format( string ) timestamp or datetime format that is expected (see datetime-parsing ) 'millisecond' include_topic ( bool ) whether to include the routingkey in the resulting datapoints true topic_as ( string ) if include_topic is true, this will be the fieldname for the routingkey value 'topic' as ( string ) base object for the output data-point undefined ssl( is_set ) whether to use ssl, if true, ssl options from faxe's config for amqp connections will be used false (not set) confirm ( boolean ) whether to acknowledge consumed messages to the amqp broker, when set to false , throughput can be increased with the danger of data-loss true safe ( boolean ) whether to use faxe's internal queue. If true , messages consumed from the amqp broker will be stored in an internal ondisc queue before they get sent to downstream nodes, to avoid losing data. false Exactly one of these must be provided: routing_key , bindings .","title":"Parameters"},{"location":"nodes/messaging/amqp_publish.html","text":"The amqp_publish node Publish data to an amqp-broker exchange. The most popular amqp-broker is RabbitMQ . Incoming data is converted to JSON before sending. This node accepts regular amqp routing keys as well as MQTT style topic strings for each of the routing_key(...) params. The amqp correlation-id property will be set to phash2(routing_key + payload) using erlang's phash2 function on every published message: The erlang documentation on phash2: Portable hash function that gives the same hash for the same Erlang term regardless of machine architecture and ERTS version. (phash2 outputs an integer which gets casted to a string to be used as a correlation-id) The amqp_consume node will use this values to perform deduplication on message receiving. Note: This node is a sink node and does not output any flow-data, therefore any node connected to it will not receive any data from this node. Example | amqp_publish() . host ( '127.0.0.1' ) . routing_key ( 'my.routing.key' ) . exchange ( 'x_xchange' ) Parameters Parameter Description Default host( string ) Ip address or hostname of the broker from config port( integer ) The broker's port 5672 / from config file user( string ) AMQP user from config file pass( string ) AMQP password from config file vhost( string ) vhost to connect to on the broker '/' routing_key( string ) routing key for the published messages undefined routing_key_lambda( lambda ) lambda expression producing a routing key for the published messages undefined routing_key_field( string ) path to a field in the current data-item, who's value should be used as the routing-key undefined exchange( string ) name of the exchange to publish to qos( integer ) publish quality, see table below for details 1 persistent( bool ) whether to send the amqp messages with delivery-mode 2 (persistent) false (delivery_mode = 1) ssl( is_set ) whether to use ssl false (not set) One of routing_key , routing_key_lambda , routing_key_field is required. Qos Qos description consequences 0 In memory queuing of messages, in case of network issues. Does not use publisher confirm on the channel. Highest throuput. 1 On disc queuing of messages, in case of network issues. Does not use publisher confirm on the channel. Not yet published messages will survive a flow crash. At least once. 2 On disc queue + acknowledgment according to acknowledgement from the amqp broker (publisher confirm). Most safe data delivery. Aims at exactly once semantics.","title":"Amqp publish"},{"location":"nodes/messaging/amqp_publish.html#the-amqp_publish-node","text":"Publish data to an amqp-broker exchange. The most popular amqp-broker is RabbitMQ . Incoming data is converted to JSON before sending. This node accepts regular amqp routing keys as well as MQTT style topic strings for each of the routing_key(...) params. The amqp correlation-id property will be set to phash2(routing_key + payload) using erlang's phash2 function on every published message: The erlang documentation on phash2: Portable hash function that gives the same hash for the same Erlang term regardless of machine architecture and ERTS version. (phash2 outputs an integer which gets casted to a string to be used as a correlation-id) The amqp_consume node will use this values to perform deduplication on message receiving. Note: This node is a sink node and does not output any flow-data, therefore any node connected to it will not receive any data from this node.","title":"The amqp_publish node"},{"location":"nodes/messaging/amqp_publish.html#example","text":"| amqp_publish() . host ( '127.0.0.1' ) . routing_key ( 'my.routing.key' ) . exchange ( 'x_xchange' )","title":"Example"},{"location":"nodes/messaging/amqp_publish.html#parameters","text":"Parameter Description Default host( string ) Ip address or hostname of the broker from config port( integer ) The broker's port 5672 / from config file user( string ) AMQP user from config file pass( string ) AMQP password from config file vhost( string ) vhost to connect to on the broker '/' routing_key( string ) routing key for the published messages undefined routing_key_lambda( lambda ) lambda expression producing a routing key for the published messages undefined routing_key_field( string ) path to a field in the current data-item, who's value should be used as the routing-key undefined exchange( string ) name of the exchange to publish to qos( integer ) publish quality, see table below for details 1 persistent( bool ) whether to send the amqp messages with delivery-mode 2 (persistent) false (delivery_mode = 1) ssl( is_set ) whether to use ssl false (not set) One of routing_key , routing_key_lambda , routing_key_field is required.","title":"Parameters"},{"location":"nodes/messaging/amqp_publish.html#qos","text":"Qos description consequences 0 In memory queuing of messages, in case of network issues. Does not use publisher confirm on the channel. Highest throuput. 1 On disc queuing of messages, in case of network issues. Does not use publisher confirm on the channel. Not yet published messages will survive a flow crash. At least once. 2 On disc queue + acknowledgment according to acknowledgement from the amqp broker (publisher confirm). Most safe data delivery. Aims at exactly once semantics.","title":"Qos"},{"location":"nodes/messaging/mqtt_amqp_bridge.html","text":"The mqtt_amqp_bridge node The mqtt_amqp-bridge node provides a message-order preserving and fail-safe mqtt-to-amqp bridge. It is designed for minimized overhead, high throughput and fault-tolerant message delivery. Receives data from an mqtt-broker and writes each indiviual topic with an amqp-publisher via an internal on-disk queue. This node starts 1 mqtt-subscriber and up to max_publishers number of amqp-publishers. The node does only work standalone at the moment, meaning you cannot connect it to other nodes. The mqtt_amqp_bridge is completely unaware of the message content. For performance reasons the node does not parse incoming data or use data_items as every other node in faxe does, instead internally it will work with the raw binaries received from the mqtt broker and pass them through to the amqp publishers. Example def topic = 'my/topic/#' | mqtt_amqp_bridge () . topics ( topic ) %% amqp params . amqp_host ( '10.11.12.13' ) . amqp_user ( 'user' ) . amqp_pass ( 'pass' ) . amqp_exchange ( 'x_exchange' ) . amqp_ssl () . max_publishers ( 5 ) Parameters Parameter Description Default host( string ) Ip address or hostname of the mqtt broker from config port( integer ) The mqtt broker's port 1883 from config user( string ) username for the mqtt connection from config pass( string ) password for the mqtt connection from config ssl( is_set ) whether to use ssl for the mqtt connection false (not set) topics( string_list ) mqtt topic to use qos( integer ) Quality of service for mqtt, one of 0, 1 or 2 1 amqp_host( string ) Ip address or hostname of the amqp broker from config file amqp_port( integer ) The amqp broker's port 1883 from config file amqp_user( string ) username for amqp connections from config file amqp_pass( string ) password for amqp connections from config file amqp_ssl( is_set ) whether to use ssl for the amqp connection false (not set) amqp_vhost( string ) VHost for the amqp broker '/' amqp_exchange( string ) name of the amqp exchange to publish to max_publishers( integer ) max number of amqp publishers that will be started 3 safe( is_set) whether to use queue acknowledgement for the internal on-disc queue false (not set) persistent( bool ) whether to send the amqp messages with delivery-mode 2 (persistent) false (delivery_mode = 1) reset_timeout( duration ) when the bridge does not see any new message for a topic for this amount of time, it will try to stop the corresponding queue and amqp-publisher process, if appropiate 5m","title":"Mqtt amqp bridge"},{"location":"nodes/messaging/mqtt_amqp_bridge.html#the-mqtt_amqp_bridge-node","text":"The mqtt_amqp-bridge node provides a message-order preserving and fail-safe mqtt-to-amqp bridge. It is designed for minimized overhead, high throughput and fault-tolerant message delivery. Receives data from an mqtt-broker and writes each indiviual topic with an amqp-publisher via an internal on-disk queue. This node starts 1 mqtt-subscriber and up to max_publishers number of amqp-publishers. The node does only work standalone at the moment, meaning you cannot connect it to other nodes. The mqtt_amqp_bridge is completely unaware of the message content. For performance reasons the node does not parse incoming data or use data_items as every other node in faxe does, instead internally it will work with the raw binaries received from the mqtt broker and pass them through to the amqp publishers.","title":"The mqtt_amqp_bridge node"},{"location":"nodes/messaging/mqtt_amqp_bridge.html#example","text":"def topic = 'my/topic/#' | mqtt_amqp_bridge () . topics ( topic ) %% amqp params . amqp_host ( '10.11.12.13' ) . amqp_user ( 'user' ) . amqp_pass ( 'pass' ) . amqp_exchange ( 'x_exchange' ) . amqp_ssl () . max_publishers ( 5 )","title":"Example"},{"location":"nodes/messaging/mqtt_amqp_bridge.html#parameters","text":"Parameter Description Default host( string ) Ip address or hostname of the mqtt broker from config port( integer ) The mqtt broker's port 1883 from config user( string ) username for the mqtt connection from config pass( string ) password for the mqtt connection from config ssl( is_set ) whether to use ssl for the mqtt connection false (not set) topics( string_list ) mqtt topic to use qos( integer ) Quality of service for mqtt, one of 0, 1 or 2 1 amqp_host( string ) Ip address or hostname of the amqp broker from config file amqp_port( integer ) The amqp broker's port 1883 from config file amqp_user( string ) username for amqp connections from config file amqp_pass( string ) password for amqp connections from config file amqp_ssl( is_set ) whether to use ssl for the amqp connection false (not set) amqp_vhost( string ) VHost for the amqp broker '/' amqp_exchange( string ) name of the amqp exchange to publish to max_publishers( integer ) max number of amqp publishers that will be started 3 safe( is_set) whether to use queue acknowledgement for the internal on-disc queue false (not set) persistent( bool ) whether to send the amqp messages with delivery-mode 2 (persistent) false (delivery_mode = 1) reset_timeout( duration ) when the bridge does not see any new message for a topic for this amount of time, it will try to stop the corresponding queue and amqp-publisher process, if appropiate 5m","title":"Parameters"},{"location":"nodes/messaging/mqtt_publish.html","text":"The mqtt_publish node Publish data to a mqtt-broker. Incoming data is converted to JSON before sending. If the save() parameter is given, every message first gets stored to an on-disk queue before sending, this way we can make sure no message gets lost when disconnected from the broker. Note: This node is a sink node and does not output any flow-data, therefore any node connected to this node will not get any data. Example def topic = 'top/track/pressure' | mqtt_publish() . topic ( topic ) . retained () Using a lambda expression for the topic: def topic_base = 'top/' | mqtt_publish() . topic_lambda ( lambda : str_concat ([ topic_base , \"type\" , '/' , \"measurement\" ]) Here the topic string is built with a lambda expression using the topic_base declaration, the string '/' and two fields from the current data_point. The topic string may be a different one with every data_point that gets published. Parameters Parameter Description Default host( string ) Ip address or hostname of the broker from config port( integer ) The broker's port 1883 from config user( string ) username from config pass( string ) password from config client_id( string ) mqtt client id, defaults to a combination of flow-id and node-id undefined topic( string ) mqtt topic to use undefined topic_lambda( lambda ) mqtt topic to use evaluated via a lambda expression undefined topic_field( string ) [since 0.19.9] path to a field in the current data-item, who's value should be used as the topic undefined qos( integer ) Quality of service, one of 0, 1 or 2 1 retained( is_set ) whether the message should be retained on the broker false (not set) save( is_set ) send save (on-disk queuing) false (not set) ssl( is_set ) whether to use ssl false (not set) topic or topic_lambda or topic_field must be provided.","title":"Mqtt publish"},{"location":"nodes/messaging/mqtt_publish.html#the-mqtt_publish-node","text":"Publish data to a mqtt-broker. Incoming data is converted to JSON before sending. If the save() parameter is given, every message first gets stored to an on-disk queue before sending, this way we can make sure no message gets lost when disconnected from the broker. Note: This node is a sink node and does not output any flow-data, therefore any node connected to this node will not get any data.","title":"The mqtt_publish node"},{"location":"nodes/messaging/mqtt_publish.html#example","text":"def topic = 'top/track/pressure' | mqtt_publish() . topic ( topic ) . retained () Using a lambda expression for the topic: def topic_base = 'top/' | mqtt_publish() . topic_lambda ( lambda : str_concat ([ topic_base , \"type\" , '/' , \"measurement\" ]) Here the topic string is built with a lambda expression using the topic_base declaration, the string '/' and two fields from the current data_point. The topic string may be a different one with every data_point that gets published.","title":"Example"},{"location":"nodes/messaging/mqtt_publish.html#parameters","text":"Parameter Description Default host( string ) Ip address or hostname of the broker from config port( integer ) The broker's port 1883 from config user( string ) username from config pass( string ) password from config client_id( string ) mqtt client id, defaults to a combination of flow-id and node-id undefined topic( string ) mqtt topic to use undefined topic_lambda( lambda ) mqtt topic to use evaluated via a lambda expression undefined topic_field( string ) [since 0.19.9] path to a field in the current data-item, who's value should be used as the topic undefined qos( integer ) Quality of service, one of 0, 1 or 2 1 retained( is_set ) whether the message should be retained on the broker false (not set) save( is_set ) send save (on-disk queuing) false (not set) ssl( is_set ) whether to use ssl false (not set) topic or topic_lambda or topic_field must be provided.","title":"Parameters"},{"location":"nodes/messaging/mqtt_subscribe.html","text":"The mqtt_subscribe node Subscribe to an mqtt-broker and get data from one or more topics. Example | mqtt_subscribe() . topics ( 'top/grips/#' ) . dt_field ( 'UTC-Stamp' ) . dt_format ( 'float_micro' ) Parameters Parameter Description Default host( string ) Ip address or hostname of the broker from config port( integer ) The broker's port 1883 from config user( string ) username from config pass( string ) password from config client_id( string ) mqtt client id, defaults to a combination of flow-id and node-id undefined topics( string_list ) mqtt topic(s) to use undefined topic( string ) mqtt topic to use undefined qos( integer ) Quality of service, one of 0, 1 or 2 1 dt_field( string ) name of the timestamp field that is expected 'ts' dt_format( string ) timestamp or datetime format that is expected (see datetime-parsing ) 'millisecond' include_topic ( bool ) whether to include the mqtt-topic in the resulting datapoints true topic_as ( string ) if include_topic is true, this will be the fieldname for the mqtt-topic value 'topic' as ( string ) base object for the output data-point undefined ssl( is_set ) whether to use ssl false (not set) One of topic , topics must be specified.","title":"Mqtt subscribe"},{"location":"nodes/messaging/mqtt_subscribe.html#the-mqtt_subscribe-node","text":"Subscribe to an mqtt-broker and get data from one or more topics.","title":"The mqtt_subscribe node"},{"location":"nodes/messaging/mqtt_subscribe.html#example","text":"| mqtt_subscribe() . topics ( 'top/grips/#' ) . dt_field ( 'UTC-Stamp' ) . dt_format ( 'float_micro' )","title":"Example"},{"location":"nodes/messaging/mqtt_subscribe.html#parameters","text":"Parameter Description Default host( string ) Ip address or hostname of the broker from config port( integer ) The broker's port 1883 from config user( string ) username from config pass( string ) password from config client_id( string ) mqtt client id, defaults to a combination of flow-id and node-id undefined topics( string_list ) mqtt topic(s) to use undefined topic( string ) mqtt topic to use undefined qos( integer ) Quality of service, one of 0, 1 or 2 1 dt_field( string ) name of the timestamp field that is expected 'ts' dt_format( string ) timestamp or datetime format that is expected (see datetime-parsing ) 'millisecond' include_topic ( bool ) whether to include the mqtt-topic in the resulting datapoints true topic_as ( string ) if include_topic is true, this will be the fieldname for the mqtt-topic value 'topic' as ( string ) base object for the output data-point undefined ssl( is_set ) whether to use ssl false (not set) One of topic , topics must be specified.","title":"Parameters"},{"location":"nodes/statistics/avg.html","text":"The avg node Compute the average. See the stats node Example | avg () . field ( 'temperature' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Avg"},{"location":"nodes/statistics/avg.html#the-avg-node","text":"Compute the average. See the stats node","title":"The avg node"},{"location":"nodes/statistics/avg.html#example","text":"| avg () . field ( 'temperature' )","title":"Example"},{"location":"nodes/statistics/avg.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/bottom.html","text":"The sum node Select the bottom num points for field. See the stats node Example | bottom () . field ( 'temperature' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node num( integer ) number of points to select 1","title":"Bottom"},{"location":"nodes/statistics/bottom.html#the-sum-node","text":"Select the bottom num points for field. See the stats node","title":"The sum node"},{"location":"nodes/statistics/bottom.html#example","text":"| bottom () . field ( 'temperature' )","title":"Example"},{"location":"nodes/statistics/bottom.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node num( integer ) number of points to select 1","title":"Parameters"},{"location":"nodes/statistics/count.html","text":"The count node Count the number of points. See the stats node Example | count () . field ( 'over_ts' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Count"},{"location":"nodes/statistics/count.html#the-count-node","text":"Count the number of points. See the stats node","title":"The count node"},{"location":"nodes/statistics/count.html#example","text":"| count () . field ( 'over_ts' )","title":"Example"},{"location":"nodes/statistics/count.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/count_change.html","text":"The count_change node Count the number value changes. See the stats node Example | count_change () . field ( 'value' ) . as ( 'changed' ) Parameters all statistics nodes have (at least) the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Count change"},{"location":"nodes/statistics/count_change.html#the-count_change-node","text":"Count the number value changes. See the stats node","title":"The count_change node"},{"location":"nodes/statistics/count_change.html#example","text":"| count_change () . field ( 'value' ) . as ( 'changed' )","title":"Example"},{"location":"nodes/statistics/count_change.html#parameters","text":"all statistics nodes have (at least) the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/count_distinct.html","text":"The count node Count the number of distinct values. See the stats node Example | count () . field ( 'product' ) . as ( 'distinct_products' ) Parameters all statistics nodes have (at least) the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Count distinct"},{"location":"nodes/statistics/count_distinct.html#the-count-node","text":"Count the number of distinct values. See the stats node","title":"The count node"},{"location":"nodes/statistics/count_distinct.html#example","text":"| count () . field ( 'product' ) . as ( 'distinct_products' )","title":"Example"},{"location":"nodes/statistics/count_distinct.html#parameters","text":"all statistics nodes have (at least) the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/distinct.html","text":"The distinct node Select unique values. See the stats node Example | distinct () . field ( 'status' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Distinct"},{"location":"nodes/statistics/distinct.html#the-distinct-node","text":"Select unique values. See the stats node","title":"The distinct node"},{"location":"nodes/statistics/distinct.html#example","text":"| distinct () . field ( 'status' )","title":"Example"},{"location":"nodes/statistics/distinct.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/elapsed.html","text":"The elapsed node Compute the elapsed time between points. See the stats node Example | elapsed () . field ( 'trigger' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Elapsed"},{"location":"nodes/statistics/elapsed.html#the-elapsed-node","text":"Compute the elapsed time between points. See the stats node","title":"The elapsed node"},{"location":"nodes/statistics/elapsed.html#example","text":"| elapsed () . field ( 'trigger' )","title":"Example"},{"location":"nodes/statistics/elapsed.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/first.html","text":"The first node Select the first that means the oldest point. See the stats node Example | first () . field ( 'temperature' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"First"},{"location":"nodes/statistics/first.html#the-first-node","text":"Select the first that means the oldest point. See the stats node","title":"The first node"},{"location":"nodes/statistics/first.html#example","text":"| first () . field ( 'temperature' )","title":"Example"},{"location":"nodes/statistics/first.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/geometric_mean.html","text":"The geometric_mean node Compute the geometric_mean. See the stats node Example | geometric_mean () . field ( 'pressure' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Geometric mean"},{"location":"nodes/statistics/geometric_mean.html#the-geometric_mean-node","text":"Compute the geometric_mean. See the stats node","title":"The geometric_mean node"},{"location":"nodes/statistics/geometric_mean.html#example","text":"| geometric_mean () . field ( 'pressure' )","title":"Example"},{"location":"nodes/statistics/geometric_mean.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/kurtosis.html","text":"The kurtosis node Compute the kurtosis of data. See the stats node Example | kurtosis () . field ( 'temperature' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Kurtosis"},{"location":"nodes/statistics/kurtosis.html#the-kurtosis-node","text":"Compute the kurtosis of data. See the stats node","title":"The kurtosis node"},{"location":"nodes/statistics/kurtosis.html#example","text":"| kurtosis () . field ( 'temperature' )","title":"Example"},{"location":"nodes/statistics/kurtosis.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/last.html","text":"The last node Select the last, that means the newest point. See the stats node Example | last () . field ( 'chair' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Last"},{"location":"nodes/statistics/last.html#the-last-node","text":"Select the last, that means the newest point. See the stats node","title":"The last node"},{"location":"nodes/statistics/last.html#example","text":"| last () . field ( 'chair' )","title":"Example"},{"location":"nodes/statistics/last.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/max.html","text":"The max node Compute the maximum value. See the stats node Example | max () . field ( 'temperature' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Max"},{"location":"nodes/statistics/max.html#the-max-node","text":"Compute the maximum value. See the stats node","title":"The max node"},{"location":"nodes/statistics/max.html#example","text":"| max () . field ( 'temperature' )","title":"Example"},{"location":"nodes/statistics/max.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/mean.html","text":"The mean node Compute the mean of data. See the stats node Example | mean () . field ( 'current' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Mean"},{"location":"nodes/statistics/mean.html#the-mean-node","text":"Compute the mean of data. See the stats node","title":"The mean node"},{"location":"nodes/statistics/mean.html#example","text":"| mean () . field ( 'current' )","title":"Example"},{"location":"nodes/statistics/mean.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/median.html","text":"The median node Compute the median of data. See the stats node Example | median () . field ( 'temperature' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Median"},{"location":"nodes/statistics/median.html#the-median-node","text":"Compute the median of data. See the stats node","title":"The median node"},{"location":"nodes/statistics/median.html#example","text":"| median () . field ( 'temperature' )","title":"Example"},{"location":"nodes/statistics/median.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/min.html","text":"The min node Compute the minimum of data. See the stats node Example | min () . field ( 'temperature' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Min"},{"location":"nodes/statistics/min.html#the-min-node","text":"Compute the minimum of data. See the stats node","title":"The min node"},{"location":"nodes/statistics/min.html#example","text":"| min () . field ( 'temperature' )","title":"Example"},{"location":"nodes/statistics/min.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/percentile.html","text":"The percentile node Select a point at the given percentile. This is a selector function, no interpolation between points is performed. See the stats node Example | percentile () . perc ( 95 ) . field ( 'temperature' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node perc ( integer ) select percentile 95","title":"Percentile"},{"location":"nodes/statistics/percentile.html#the-percentile-node","text":"Select a point at the given percentile. This is a selector function, no interpolation between points is performed. See the stats node","title":"The percentile node"},{"location":"nodes/statistics/percentile.html#example","text":"| percentile () . perc ( 95 ) . field ( 'temperature' )","title":"Example"},{"location":"nodes/statistics/percentile.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node perc ( integer ) select percentile 95","title":"Parameters"},{"location":"nodes/statistics/stddev.html","text":"The stddev node Compute the standard deviation of the data. See the stats node Example | stddev () . field ( 'temperature' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Stddev"},{"location":"nodes/statistics/stddev.html#the-stddev-node","text":"Compute the standard deviation of the data. See the stats node","title":"The stddev node"},{"location":"nodes/statistics/stddev.html#example","text":"| stddev () . field ( 'temperature' )","title":"Example"},{"location":"nodes/statistics/stddev.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/sum.html","text":"The sum node Compute the sum of data. See the stats node Example | sum () . field ( 'temperature' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Sum"},{"location":"nodes/statistics/sum.html#the-sum-node","text":"Compute the sum of data. See the stats node","title":"The sum node"},{"location":"nodes/statistics/sum.html#example","text":"| sum () . field ( 'temperature' )","title":"Example"},{"location":"nodes/statistics/sum.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/statistics/top.html","text":"The top node Select the top num points. See the stats node Example | top () . field ( 'temperature' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node num( integer ) number of points to select 1","title":"Top"},{"location":"nodes/statistics/top.html#the-top-node","text":"Select the top num points. See the stats node","title":"The top node"},{"location":"nodes/statistics/top.html#example","text":"| top () . field ( 'temperature' )","title":"Example"},{"location":"nodes/statistics/top.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node num( integer ) number of points to select 1","title":"Parameters"},{"location":"nodes/statistics/variance.html","text":"The variance node Compute the data's variance. See the stats node Example | variance () . field ( 'temperature' ) Parameters all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Variance"},{"location":"nodes/statistics/variance.html#the-variance-node","text":"Compute the data's variance. See the stats node","title":"The variance node"},{"location":"nodes/statistics/variance.html#example","text":"| variance () . field ( 'temperature' )","title":"Example"},{"location":"nodes/statistics/variance.html#parameters","text":"all statistics nodes have the following parameters Parameter Description Default field( string ) name of the field used for computation as( string ) name for the field for output values defaults to the name of the stats-node","title":"Parameters"},{"location":"nodes/window/win_clock.html","text":"The win_clock node A window node is for batching data_points, therefore all window nodes will output data_batch items. This window-type has wall-clock timing, timestamps contained in incoming events are not relevant here. When the align option is true, window boundaries are aligned according to the every option, this means when every is 5s and an event comes into the window at time 15:03:27, this event will be member of the window that starts at 15:03:25, otherwise the window would start at 15:03:27. By default, the boundaries are defined relative to the first data point the window node receives. With fill_period given, the window will not emit before \"period\" time has elapsed (for the first time). This only applies if the period is greater than the every value. Example | win_clock() . every ( 5s ) . period ( 15s ) . fill_period () . align () The window will emit every 5 seconds, but only after initially 15 seconds have passed (due to fill_period ), it has its boundaries aligned to 5 second intervals. Parameters Parameter Description Default period( duration ) Window length defaults to every (giving us a tumbling window) every( duration ) Output window contents every align( is_set ) Align the window boundaries false (not set) fill_period( is_set ) Window output only when period time has elapsed false (not set)","title":"Win clock"},{"location":"nodes/window/win_clock.html#the-win_clock-node","text":"A window node is for batching data_points, therefore all window nodes will output data_batch items. This window-type has wall-clock timing, timestamps contained in incoming events are not relevant here. When the align option is true, window boundaries are aligned according to the every option, this means when every is 5s and an event comes into the window at time 15:03:27, this event will be member of the window that starts at 15:03:25, otherwise the window would start at 15:03:27. By default, the boundaries are defined relative to the first data point the window node receives. With fill_period given, the window will not emit before \"period\" time has elapsed (for the first time). This only applies if the period is greater than the every value.","title":"The win_clock node"},{"location":"nodes/window/win_clock.html#example","text":"| win_clock() . every ( 5s ) . period ( 15s ) . fill_period () . align () The window will emit every 5 seconds, but only after initially 15 seconds have passed (due to fill_period ), it has its boundaries aligned to 5 second intervals.","title":"Example"},{"location":"nodes/window/win_clock.html#parameters","text":"Parameter Description Default period( duration ) Window length defaults to every (giving us a tumbling window) every( duration ) Output window contents every align( is_set ) Align the window boundaries false (not set) fill_period( is_set ) Window output only when period time has elapsed false (not set)","title":"Parameters"},{"location":"nodes/window/win_event.html","text":"The win_event node A window node is for batching data_points, therefore all window nodes will output data_batch items. This window holds period number of data_points and emits every every incoming point. With fill_period given, the window will only emit when it is filled with period points. This only applies if the period is greater than the every value. Examples | win_event() . every ( 5 ) . period ( 15 ) . fill_period () The window will emit it's contents every 5 incoming points, but only after the window is filled with 15 points. | win_event() . every ( 5 ) . period ( 15 ) The window will emit it's contents every 5 incoming points. On first emit 5 points will be outputted, on the second emit 10 points will be emitted. From the third emit onwards, the window will output 15 points. Starting with the 4th emit, the window will output 15 data_points - with 10 old and 5 new points (Sliding window). Parameters Parameter Description Default period( integer ) Window length, number of points defaults to every (tumbling window) every( integer ) Output window contents every n incoming points fill_period( is_set ) Output only when window is filled false (not set)","title":"Win event"},{"location":"nodes/window/win_event.html#the-win_event-node","text":"A window node is for batching data_points, therefore all window nodes will output data_batch items. This window holds period number of data_points and emits every every incoming point. With fill_period given, the window will only emit when it is filled with period points. This only applies if the period is greater than the every value.","title":"The win_event node"},{"location":"nodes/window/win_event.html#examples","text":"| win_event() . every ( 5 ) . period ( 15 ) . fill_period () The window will emit it's contents every 5 incoming points, but only after the window is filled with 15 points. | win_event() . every ( 5 ) . period ( 15 ) The window will emit it's contents every 5 incoming points. On first emit 5 points will be outputted, on the second emit 10 points will be emitted. From the third emit onwards, the window will output 15 points. Starting with the 4th emit, the window will output 15 data_points - with 10 old and 5 new points (Sliding window).","title":"Examples"},{"location":"nodes/window/win_event.html#parameters","text":"Parameter Description Default period( integer ) Window length, number of points defaults to every (tumbling window) every( integer ) Output window contents every n incoming points fill_period( is_set ) Output only when window is filled false (not set)","title":"Parameters"},{"location":"nodes/window/win_session.html","text":"The win_session node Experimental . Since 0.17.2 This window refers it's timing to the timestamp contained in the incoming data-items. A session window aggregates records into a session, which represents a period of activity separated by a specified gap of inactivity. Any data_points with timestamps that occur within the inactivity gap of existing sessions will be added to this session. If a data_point's timestamp occurs outside the session gap, a new session is created. A new session window starts if the last record that arrived is further back in time than the specified inactivity gap. Example | value_emitter() . every ( 500ms ) . jitter ( 4600ms ) | win_session () . session_timeout ( 4500ms ) | debug( 'info' ) Every data_point, that has a timestamp < last-timestamp + session_timeout, will be member of the current window. Parameters Parameter Description Default session_timeout( duration ) also called inactivity gap 3m","title":"Win session"},{"location":"nodes/window/win_session.html#the-win_session-node","text":"Experimental . Since 0.17.2 This window refers it's timing to the timestamp contained in the incoming data-items. A session window aggregates records into a session, which represents a period of activity separated by a specified gap of inactivity. Any data_points with timestamps that occur within the inactivity gap of existing sessions will be added to this session. If a data_point's timestamp occurs outside the session gap, a new session is created. A new session window starts if the last record that arrived is further back in time than the specified inactivity gap.","title":"The win_session node"},{"location":"nodes/window/win_session.html#example","text":"| value_emitter() . every ( 500ms ) . jitter ( 4600ms ) | win_session () . session_timeout ( 4500ms ) | debug( 'info' ) Every data_point, that has a timestamp < last-timestamp + session_timeout, will be member of the current window.","title":"Example"},{"location":"nodes/window/win_session.html#parameters","text":"Parameter Description Default session_timeout( duration ) also called inactivity gap 3m","title":"Parameters"},{"location":"nodes/window/win_time.html","text":"The win_time node A window node is for batching data_points, therefore all window nodes will output data_batch items. This window refers it's timing to the timestamp contained in the incoming data-items. With fill_period given, the window will not emit before \"period\" time has elapsed (for the first time). Note that, since this window type does not rely on wall clock, but on the points timestamps, it is possible that no data is emitted, if there are no new points coming in. Example | win_time() . every ( 5s ) . period ( 15s ) The window will emit it's contents every 5 seconds. | win_time() . every ( 1m ) Period is 1 minute here (period defaults to every) Parameters Parameter Description Default period( duration ) Window length defaults to every (giving us a tumbling window) every( duration ) Output window contents every fill_period( is_set ) Window output only when period time has accumulated false (not set)","title":"Win time"},{"location":"nodes/window/win_time.html#the-win_time-node","text":"A window node is for batching data_points, therefore all window nodes will output data_batch items. This window refers it's timing to the timestamp contained in the incoming data-items. With fill_period given, the window will not emit before \"period\" time has elapsed (for the first time). Note that, since this window type does not rely on wall clock, but on the points timestamps, it is possible that no data is emitted, if there are no new points coming in.","title":"The win_time node"},{"location":"nodes/window/win_time.html#example","text":"| win_time() . every ( 5s ) . period ( 15s ) The window will emit it's contents every 5 seconds. | win_time() . every ( 1m ) Period is 1 minute here (period defaults to every)","title":"Example"},{"location":"nodes/window/win_time.html#parameters","text":"Parameter Description Default period( duration ) Window length defaults to every (giving us a tumbling window) every( duration ) Output window contents every fill_period( is_set ) Window output only when period time has accumulated false (not set)","title":"Parameters"}]}